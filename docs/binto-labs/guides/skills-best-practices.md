# Claude Skills System: Best Practices & Patterns

**Last Updated:** 2025-10-31 | **Version:** claude-flow v2.7.0-alpha.14

---

## Executive Summary

The Claude Skills system is a paradigm shift from explicit slash commands to **intent-based activation**. Skills discover themselves through YAML frontmatter scanning, activate automatically via natural language matching, and use progressive disclosure to reduce context pollution by 40%.

**Key Insight:** Don't think in commands - think in conversations. Claude reads your intent and activates the right skills.

---

## 1. Skills Discovery & Activation

### How Discovery Works

**Startup Process:**
1. Claude scans `.claude/skills/*/SKILL.md` files
2. Reads YAML frontmatter (name, description, tags, category)
3. Builds internal skill index without loading full content
4. Waits for user intent matching

**Example YAML Frontmatter:**
```yaml
---
name: sparc-methodology
description: SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) comprehensive development methodology with multi-agent orchestration
version: 2.7.0
category: development
tags:
  - sparc
  - tdd
  - architecture
  - orchestration
  - methodology
  - multi-agent
author: Claude Flow
---
```

### What Triggers Activation

Skills activate when your request matches:
- **Description keywords**: "SPARC", "vector search", "swarm orchestration"
- **Tags**: "tdd", "architecture", "github", "performance"
- **Category**: "development", "github", "memory", "automation"
- **Context**: Working with PRs â†’ GitHub skills activate
- **Problem domain**: "slow queries" â†’ performance skills activate

### Progressive Disclosure Mechanism

**Three-Tier Structure:**
```markdown
# Skill Name
## Overview (10-20 lines)
   â†“ Loaded first for intent matching

## Quick Start (50-100 lines)
   â†“ Loaded if skill matches intent

## Advanced Features (500+ lines)
   â†“ Loaded only if explicitly needed
```

**Benefits:**
- **40% less context pollution** (proven metric)
- Faster skill matching (only read metadata)
- Cleaner token usage (load what's needed)

### Manual vs Automatic Activation

**Automatic (Preferred 95% of time):**
```bash
# âœ… GOOD: Natural language
"Build user authentication with TDD"
"Find similar code to this authentication pattern"
"Review PR #42 for security vulnerabilities"

# â†’ Skills activate automatically based on intent
```

**Manual (Rare cases):**
```bash
# Use only when automatic activation fails or for explicit control
/skill-name
npx claude-flow sparc run <mode> "task"
```

**When to use manual activation:**
- Automatic activation missed the right skill
- Testing specific skill functionality
- Debugging skill behavior
- Educational/tutorial purposes

---

## 2. Natural Language Patterns

### Effective Prompts (What Works)

#### âœ… Pattern 1: Action + Domain + Method
```bash
# Good examples:
"Build user authentication using SPARC methodology with TDD"
"Find similar authentication code patterns in the codebase"
"Review this PR for security vulnerabilities using swarm coordination"
"Optimize database queries using performance analysis"
```

**Why it works:** Clearly states action (build, find, review, optimize), domain (auth, code, PR, queries), and preferred method (SPARC, patterns, swarm, analysis).

#### âœ… Pattern 2: Problem + Constraint + Goal
```bash
# Good examples:
"Our API is slow (800ms) - optimize to under 200ms"
"This CSRF bug keeps appearing - fix and remember the solution"
"We need multi-repo sync across 5 microservices"
```

**Why it works:** Describes problem state, constraints, and desired outcome. Claude infers needed skills.

#### âœ… Pattern 3: Context + Task + Quality
```bash
# Good examples:
"Working on checkout flow - implement payment integration with 90% test coverage"
"Refactoring legacy auth service - ensure no regressions with verification enabled"
"Building new API - use architecture patterns and store decisions in memory"
```

**Why it works:** Provides context, clear task, and quality expectations.

#### âœ… Pattern 4: Just Describe the Goal
```bash
# Good examples:
"Help me pair program on this React component"
"Let's build a REST API with authentication"
"Create a swarm to analyze this codebase from multiple angles"
```

**Why it works:** Simple, natural, conversational. Claude figures out what skills are needed.

### Anti-Patterns (What Doesn't Work)

#### âŒ Anti-Pattern 1: Over-Specifying
```bash
# BAD: Too explicit about mechanisms
"Use the agentdb-vector-search skill to create embeddings and search the vector database using HNSW indexing with cosine similarity"

# GOOD: Just state what you need
"Find similar authentication code"
# â†’ agentdb-vector-search activates automatically
```

**Why it fails:** You're doing Claude's job. Let the skill system choose tools.

#### âŒ Anti-Pattern 2: Under-Specifying
```bash
# BAD: Too vague
"Make it better"
"Fix the code"
"Improve performance"

# GOOD: Provide context
"Optimize API response time from 800ms to under 200ms"
"Fix CSRF token missing in checkout flow"
"Refactor authentication module for better testability"
```

**Why it fails:** No clear intent for skill matching.

#### âŒ Anti-Pattern 3: Command Thinking
```bash
# BAD: Treating skills like CLI commands
"/agentdb-vector-search query [0.1,0.2,0.3]"
"/sparc-methodology mode=tdd task='feature'"

# GOOD: Natural conversation
"Search for similar code patterns"
"Build this feature with TDD"
```

**Why it fails:** Skills aren't commands - they're capabilities that activate based on conversation.

#### âŒ Anti-Pattern 4: Mixing Abstraction Levels
```bash
# BAD: Mixing high-level goals with low-level implementation
"Use SPARC to build auth but call mcp__claude-flow__swarm_init with topology mesh"

# GOOD: Either high-level or low-level, not both
High-level: "Build auth feature systematically with tests"
Low-level: Use MCP tools directly when you need precise control
```

**Why it fails:** Confusing intent. Choose abstraction level and stick with it.

### Natural Language vs CLI Comparison

| Scenario | Natural Language (Preferred) | CLI (Fallback) |
|----------|------------------------------|----------------|
| **Feature Development** | "Build shopping cart with SPARC and TDD" | `npx claude-flow sparc tdd "shopping cart"` |
| **Code Search** | "Find similar authentication patterns" | `npx agentdb query ./db "[...]"` |
| **PR Review** | "Review PR #42 for security issues" | `npx claude-flow swarm "review PR" --strategy review` |
| **Performance** | "Why is this API slow? Find bottlenecks" | `npx claude-flow sparc run optimizer "API"` |
| **Memory Search** | "Remember our API design decisions" | `npx claude-flow memory store --key "api-design"` |

**Rule of Thumb:** Use natural language 95% of the time. Use CLI only when:
- Automation/scripting (CI/CD pipelines)
- Precise parameter control needed
- Debugging specific tools
- Natural language activation fails

---

## 3. Skill Composition

### How Skills Call Other Skills

**Internal Composition (Automatic):**
```typescript
// github-code-review skill internally calls:
// 1. agentdb-vector-search (find similar issues)
// 2. reasoningbank-agentdb (apply learned patterns)
// 3. verification-quality (validate review completeness)

// User just says: "Review PR #42"
// â†’ Claude activates github-code-review
// â†’ github-code-review activates sub-skills automatically
```

**Skills don't call each other explicitly** - they declare capabilities, and Claude's orchestration layer coordinates them.

### Combining Multiple Skills

#### Pattern 1: Sequential Composition
```bash
# User request: "Build authentication feature systematically"

# Skills activated in sequence:
# 1. sparc-methodology (structure development)
# 2. reasoningbank-agentdb (check for learned patterns)
# 3. agentdb-vector-search (find similar implementations)
# 4. verification-quality (ensure quality gates)
# 5. hooks-automation (auto-format and learn)
```

#### Pattern 2: Parallel Composition
```bash
# User request: "Create swarm to build: API, UI, tests, docs"

# Skills activated in parallel:
# 1. swarm-orchestration (coordinates agents)
# 2. sparc-methodology (guides each agent)
# 3. agentdb-memory-patterns (shares context)
# 4. performance-analysis (monitors execution)
```

#### Pattern 3: Hierarchical Composition
```bash
# User request: "Build e-commerce platform with microservices"

# Top-level: hive-mind-advanced (queen coordinator)
# â”œâ”€ Mid-level: swarm-orchestration (per service)
# â”‚  â”œâ”€ Low-level: sparc-methodology (feature dev)
# â”‚  â”œâ”€ Low-level: github-code-review (quality)
# â”‚  â””â”€ Low-level: verification-quality (gates)
# â””â”€ Support: agentdb-memory-patterns (shared state)
```

### Coordination Patterns

#### Memory-Based Coordination
```bash
# Skill A stores decision
sparc-methodology:
  â†’ stores architecture in memory["api-design"]

# Skill B reads decision
swarm-orchestration:
  â†’ reads memory["api-design"]
  â†’ coordinates agents based on architecture

# Skill C validates decision
verification-quality:
  â†’ reads memory["api-design"]
  â†’ ensures implementation matches
```

#### Hook-Based Coordination
```bash
# Pre-task hook (runs before any operation)
hooks-automation:
  â†’ pre-task: Prepare resources
  â†’ pre-search: Cache common queries

# Post-task hook (runs after operation)
hooks-automation:
  â†’ post-edit: Auto-format code
  â†’ post-task: Update memory with learnings

# Session hook (runs at session boundaries)
hooks-automation:
  â†’ session-end: Export metrics
  â†’ session-restore: Load previous context
```

#### Event-Based Coordination
```bash
# Skill emits event
swarm-orchestration:
  â†’ emits: "agent-failed"

# Multiple skills react
performance-analysis:
  â†’ reacts: Log failure metrics
verification-quality:
  â†’ reacts: Trigger rollback check
hooks-automation:
  â†’ reacts: Update failure patterns
```

### Memory Sharing Between Skills

**Namespaced Memory:**
```typescript
// Skill A: sparc-methodology
await memory.store({
  namespace: 'architecture',
  key: 'api-design-v1',
  value: apiDesign,
  ttl: 86400000 // 24 hours
});

// Skill B: swarm-orchestration reads same namespace
const design = await memory.retrieve({
  namespace: 'architecture',
  key: 'api-design-v1'
});

// Skill C: reasoningbank-agentdb learns from outcome
await memory.store({
  namespace: 'learnings',
  key: 'api-design-outcome',
  value: { success: true, metrics: {...} }
});
```

**Memory Hierarchy:**
```
Global Memory (all skills)
â”œâ”€ project/ (project-wide state)
â”‚  â”œâ”€ architecture/
â”‚  â”œâ”€ decisions/
â”‚  â””â”€ standards/
â”œâ”€ skills/ (skill-specific state)
â”‚  â”œâ”€ sparc/
â”‚  â”œâ”€ swarm/
â”‚  â””â”€ agentdb/
â””â”€ sessions/ (temporary session state)
   â”œâ”€ swarm-001/
   â””â”€ feature-auth-v1/
```

---

## 4. Performance Considerations

### Context Window Usage

**Skill Loading Strategy:**
```
Startup (0 tokens):
â””â”€ Scan 25 skills
   â””â”€ Read only YAML frontmatter (~250 tokens total)

User Request (~50 tokens):
â””â”€ Match 2-3 relevant skills
   â””â”€ Load Overview sections (~300 tokens)

Skill Activation (~200 tokens):
â””â”€ Load full skill content only if needed
   â””â”€ Progressive disclosure keeps it minimal
```

**Context Budget:**
- **Idle state:** ~250 tokens (just metadata)
- **Active state:** ~800 tokens (2-3 skills loaded)
- **Heavy usage:** ~2000 tokens (complex multi-skill orchestration)
- **Traditional commands:** ~3000+ tokens (everything loaded upfront)

**Savings:** **40% reduction** in context usage vs slash commands.

### Token Efficiency

**Proven Optimizations:**

1. **Progressive Disclosure:** Load only what's needed
   - Metadata scanning: 10 tokens/skill
   - Overview loading: 50-100 tokens/skill
   - Full content: 500+ tokens/skill
   - **Result:** 60-80% token savings

2. **ReasoningBank Integration:** Reduce repeated explanations
   - First time: Full explanation (500 tokens)
   - Second time: Reference learned pattern (50 tokens)
   - Fifth time: Near-instant application (10 tokens)
   - **Result:** 32.3% token reduction over time

3. **Memory Coordination:** Avoid re-computation
   - Without memory: Re-analyze every time (1000 tokens)
   - With memory: Read cached analysis (50 tokens)
   - **Result:** 95% token savings for repeated tasks

4. **Batch Operations:** Reduce coordination overhead
   - Sequential: 100 tokens Ã— 10 operations = 1000 tokens
   - Batched: 200 tokens for 10 operations = 200 tokens
   - **Result:** 80% coordination overhead reduction

### Execution Speed Comparisons

**Sequential vs Parallel (Swarm Orchestration):**
```
Sequential Development:
â”œâ”€ Research: 5 minutes
â”œâ”€ Design: 10 minutes
â”œâ”€ Code: 20 minutes
â”œâ”€ Test: 15 minutes
â””â”€ Review: 10 minutes
Total: 60 minutes

Parallel Development (Mesh Swarm):
â”œâ”€ Research â”€â”€â”€â”€â”
â”œâ”€ Design â”€â”€â”€â”€â”€â”€â”¤
â”œâ”€ Code â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â†’ Orchestration: 5 min
â”œâ”€ Test â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€ Review â”€â”€â”€â”€â”€â”€â”˜
Total: ~20 minutes (2.8-4.4x faster)
```

**ReasoningBank Learning Curve:**
```
First Bug Fix: 15 minutes (learn pattern)
Second Bug: 8 minutes (47% faster - recognize pattern)
Fifth Bug: 5 minutes (65% faster - expert-level application)
```

**AgentDB Vector Search:**
```
Legacy ChromaDB: 15ms/query
AgentDB HNSW: <100Âµs/query (150x faster)

Large-scale (1M vectors):
Legacy: 100 seconds
AgentDB: 8ms (12,500x faster)
```

### When to Use Skills vs Direct Commands

**Use Skills (Natural Language) When:**
- âœ… Exploratory development (figuring out what to build)
- âœ… Complex multi-step workflows (build, test, review, deploy)
- âœ… Learning from experience (ReasoningBank enabled)
- âœ… Quality matters over speed (verification gates)
- âœ… Collaborative work (swarms, hive-mind)

**Use Direct Commands (CLI/MCP) When:**
- âš¡ Automation/scripting (CI/CD pipelines)
- âš¡ Precise control needed (specific parameters)
- âš¡ Simple one-off operations (format file, run test)
- âš¡ Performance-critical paths (direct tool access)
- âš¡ Debugging (need to see exact tool behavior)

**Benchmark Data:**
| Operation | Natural Language (Skills) | Direct Command | Winner |
|-----------|---------------------------|----------------|--------|
| Feature development | 20 min (parallel swarm) | 60 min (sequential) | Skills 3x faster |
| Code search | Auto-optimized | Manual params | Skills (easier) |
| Simple file edit | +5 sec overhead | Instant | Commands faster |
| Complex workflow | Auto-coordinated | Manual setup | Skills (simpler) |
| CI/CD pipeline | Not applicable | Reliable | Commands only option |

---

## 5. Common Mistakes

### Over-Specifying (Being Too Explicit)

#### âŒ Mistake Example:
```bash
"Use the agentdb-vector-search skill with HNSW indexing and binary quantization to find code similar to this authentication function using cosine similarity with a threshold of 0.75 and return the top 10 results"
```

**Why it's bad:**
- Tells Claude HOW instead of WHAT
- Skill system is designed to choose optimal methods
- May miss better approaches Claude knows about
- Verbose and harder to read

#### âœ… Better Approach:
```bash
"Find similar authentication code"
```

**Why it's better:**
- States the goal clearly
- agentdb-vector-search activates automatically
- Skill chooses optimal settings (HNSW, quantization, etc.)
- Claude can optimize based on context (codebase size, performance needs)

#### ğŸ’¡ Pro Tip:
Think like you're talking to a senior developer, not programming a computer. Say "find similar code" not "execute vector search algorithm with parameters X, Y, Z".

---

### Under-Specifying (Being Too Vague)

#### âŒ Mistake Example:
```bash
"Make it better"
"Fix the code"
"Improve things"
"Optimize"
```

**Why it's bad:**
- No clear intent for skill matching
- Claude has to guess what you want
- May activate wrong skills
- Wastes time clarifying

#### âœ… Better Approach:
```bash
"Optimize API response time from 800ms to under 200ms"
"Fix CSRF token missing error in checkout flow"
"Refactor authentication for better testability"
"Improve code readability in UserService"
```

**Why it's better:**
- Clear problem statement
- Measurable goal (800ms â†’ 200ms)
- Specific domain (API, CSRF, auth, UserService)
- Skills can match intent precisely

#### ğŸ’¡ Pro Tip:
Include these three elements:
1. **Current state:** "API is slow at 800ms"
2. **Problem:** "Response time too high"
3. **Goal:** "Under 200ms"

---

### Skill Conflicts or Overlap

#### âŒ Mistake Example:
```bash
"Use SPARC methodology and also use swarm orchestration and also use hive-mind coordination"
```

**Why it's bad:**
- sparc-methodology already uses swarm orchestration internally
- hive-mind is for 5-20 independent tasks (different use case)
- Creates coordination conflicts
- Redundant activation

#### âœ… Better Approach:
```bash
# For systematic feature development:
"Build authentication feature with TDD"
â†’ Activates: sparc-methodology (includes orchestration)

# For large-scale parallel work:
"Refactor 5 microservices to TypeScript"
â†’ Activates: hive-mind-advanced (better for independent tasks)
```

**Why it's better:**
- One skill per abstraction level
- Skills internally compose what they need
- No coordination conflicts
- Cleaner execution

#### ğŸ’¡ Pro Tip:
**Skill hierarchy:**
```
High-level (orchestrators):
â”œâ”€ sparc-methodology (systematic development)
â”œâ”€ hive-mind-advanced (5-20 independent tasks)
â””â”€ swarm-orchestration (3-8 coordinated agents)

Mid-level (specialized):
â”œâ”€ github-code-review (PR reviews)
â”œâ”€ agentdb-vector-search (semantic search)
â””â”€ performance-analysis (bottlenecks)

Low-level (utilities):
â”œâ”€ hooks-automation (formatting, learning)
â”œâ”€ verification-quality (quality gates)
â””â”€ agentdb-memory-patterns (persistence)
```

**Rule:** Pick ONE from high-level, let it coordinate mid/low-level skills.

---

### Memory Namespace Pollution

#### âŒ Mistake Example:
```typescript
// Storing everything in global namespace
await memory.store({ key: 'data1', value: {...} });
await memory.store({ key: 'data2', value: {...} });
await memory.store({ key: 'temp', value: {...} });
// 50 more global keys...
```

**Why it's bad:**
- Global namespace gets cluttered (100+ keys)
- Hard to find what you need
- Conflicts between skills
- No automatic cleanup

#### âœ… Better Approach:
```typescript
// Use namespaced memory with TTL
await memory.store({
  namespace: 'architecture',
  key: 'api-design-v1',
  value: apiDesign,
  ttl: 86400000 // 24 hours
});

await memory.store({
  namespace: 'sessions',
  key: 'swarm-001',
  value: sessionState,
  ttl: 3600000 // 1 hour
});

await memory.store({
  namespace: 'learnings',
  key: 'csrf-fix-pattern',
  value: solution,
  ttl: null // permanent
});
```

**Why it's better:**
- Organized by purpose (architecture, sessions, learnings)
- Automatic cleanup with TTL
- Skills can own their namespace
- Easy to search/manage

#### ğŸ’¡ Pro Tip:
**Standard Namespaces:**
```
project/        - Project-wide decisions (TTL: 7 days)
  â”œâ”€ architecture/
  â”œâ”€ standards/
  â””â”€ decisions/

skills/         - Skill-specific state (TTL: 1 day)
  â”œâ”€ sparc/
  â”œâ”€ swarm/
  â””â”€ agentdb/

sessions/       - Temporary session data (TTL: 1 hour)
  â”œâ”€ swarm-001/
  â””â”€ feature-auth/

learnings/      - Permanent learned patterns (no TTL)
  â”œâ”€ patterns/
  â””â”€ solutions/
```

---

## 6. Practical Examples

### âœ… Excellent: Natural Feature Development
```bash
User: "Build user authentication with JWT tokens and refresh tokens.
       Use TDD, ensure 90% coverage, and store architecture decisions."

# What happens:
# 1. sparc-methodology activates (TDD, systematic approach)
# 2. agentdb-memory-patterns stores decisions automatically
# 3. verification-quality ensures 90% coverage gate
# 4. reasoningbank-agentdb learns patterns for next time
# 5. hooks-automation formats code and updates docs

# Result: Complete feature with:
# âœ… Tests written first (TDD)
# âœ… 90%+ test coverage
# âœ… Architecture documented in memory
# âœ… Pattern learned for future auth features
# âœ… Code formatted automatically
```

**Why it's excellent:**
- Clear goal (JWT auth with refresh)
- Quality expectation (90% coverage)
- Learning enabled (store decisions)
- Natural language (no tool names)
- Multiple skills activated seamlessly

---

### âœ… Excellent: Intelligent Bug Fix with Learning
```bash
User: "Fix the CSRF token missing error in checkout flow.
       Remember the solution for similar issues."

# What happens:
# 1. agentdb-vector-search checks for similar past fixes
# 2. reasoningbank-agentdb applies learned CSRF patterns (if available)
# 3. If first time: Investigates, fixes, stores solution
# 4. If seen before: Applies learned fix 47% faster
# 5. verification-quality ensures no regressions
# 6. hooks-automation updates tests automatically

# First time: 15 minutes (learn pattern)
# Second time: 8 minutes (47% faster - recognize pattern)
# Fifth time: 5 minutes (65% faster - expert-level application)
```

**Why it's excellent:**
- Problem + context (CSRF in checkout)
- Learning enabled (remember solution)
- Gets faster over time (ReasoningBank)
- Quality ensured (verification)

---

### âœ… Excellent: Large-Scale Parallel Refactor
```bash
User: "Refactor 3 microservices to TypeScript:
       auth-service, payment-service, notification-service"

# What happens:
# 1. hive-mind-advanced initializes queen-led coordination
# 2. 3 worker swarms created (one per service)
# 3. Each swarm uses sparc-methodology internally
# 4. agentdb-memory-patterns shares refactoring patterns
# 5. All 3 services refactor in parallel
# 6. verification-quality validates each independently

# Sequential: 3 Ã— 20 minutes = 60 minutes
# Parallel (hive-mind): ~20 minutes (3x faster)
```

**Why it's excellent:**
- Multiple independent tasks (perfect for hive-mind)
- Clear scope (3 services, TypeScript)
- Parallel execution (3x speedup)
- Automatic coordination

---

### âŒ Poor: Command Thinking
```bash
User: "/agentdb-vector-search --query 'auth code' --limit 10 --threshold 0.75"

# What happens:
# âŒ Treated like CLI command (not natural)
# âŒ Manual parameter specification
# âŒ No skill composition (isolated operation)
# âŒ Harder to optimize (fixed parameters)
```

**Why it's poor:**
- Command syntax (not conversation)
- Manual tuning (skill should choose)
- No learning (isolated operation)
- Brittle (breaks if parameters change)

**Better approach:**
```bash
User: "Find authentication code similar to this pattern"
# â†’ agentdb-vector-search activates automatically with optimal settings
```

---

### âŒ Poor: Over-Orchestration
```bash
User: "Initialize swarm with mesh topology, spawn researcher, coder, tester agents,
       orchestrate task with parallel strategy, enable HNSW indexing,
       use binary quantization, store in memory namespace 'project/arch'"

# What happens:
# âŒ Too many implementation details
# âŒ Tells Claude HOW instead of WHAT
# âŒ May conflict with skill's optimal strategy
# âŒ Verbose and error-prone
```

**Why it's poor:**
- Micromanaging skill behavior
- Missing the forest for the trees
- Skills designed to handle these details

**Better approach:**
```bash
User: "Build REST API with authentication - use parallel agents for speed"
# â†’ swarm-orchestration chooses optimal topology and settings
```

---

### âŒ Poor: Vague Request
```bash
User: "Make the code better"

# What happens:
# âŒ No clear intent (what aspect?)
# âŒ Can't match skills effectively
# âŒ Requires clarification round-trip
# âŒ Wastes time
```

**Why it's poor:**
- No measurable goal
- No problem statement
- Skills can't match intent

**Better approach:**
```bash
User: "Refactor UserService for better testability -
       reduce complexity and improve mocking"
# â†’ Clear goal, measurable outcome, specific domain
```

---

## 7. Quick Reference

### Natural Language Patterns Cheat Sheet

```bash
# Feature Development
"Build [feature] with [method] ensuring [quality]"
Example: "Build payment processing with SPARC ensuring 90% coverage"

# Code Search
"Find [what] similar to [reference]"
Example: "Find error handling patterns similar to this function"

# Bug Fixes
"Fix [error] in [location] and [remember/learn/document]"
Example: "Fix memory leak in DataProcessor and remember the solution"

# Performance
"Optimize [component] from [current] to [target]"
Example: "Optimize API from 800ms to under 200ms"

# Coordination
"Use [agents] to [task] in [mode]"
Example: "Use swarm to build microservices in parallel"

# Review
"Review [target] for [aspects]"
Example: "Review PR #42 for security and performance"
```

### Skill Activation Keywords

```bash
# Development
sparc, tdd, architecture, systematic, methodology
â†’ Activates: sparc-methodology

# Search & Memory
find, similar, search, semantic, patterns, remember
â†’ Activates: agentdb-vector-search, agentdb-memory-patterns

# Learning
learn, remember, improve, adapt, reasoning
â†’ Activates: reasoningbank-agentdb, reasoningbank-intelligence

# Coordination
swarm, agents, parallel, mesh, hierarchical, coordinate
â†’ Activates: swarm-orchestration, hive-mind-advanced

# GitHub
pr, review, pull request, repository, issue, release
â†’ Activates: github-* skills

# Quality
verify, quality, test, coverage, rollback, gates
â†’ Activates: verification-quality

# Performance
optimize, slow, bottleneck, performance, speed
â†’ Activates: performance-analysis, agentdb-optimization
```

### Decision Tree: Which Skill?

```
Q: Building a new feature?
â”œâ”€ Yes, systematically â†’ sparc-methodology
â””â”€ No â†’ Continue

Q: Need to find/search something?
â”œâ”€ Code patterns â†’ agentdb-vector-search
â”œâ”€ Previous solutions â†’ reasoningbank-agentdb
â””â”€ No â†’ Continue

Q: Multiple agents needed?
â”œâ”€ 3-8 coordinated agents â†’ swarm-orchestration
â”œâ”€ 5-20 independent tasks â†’ hive-mind-advanced
â””â”€ No â†’ Continue

Q: GitHub-related?
â”œâ”€ PR review â†’ github-code-review
â”œâ”€ CI/CD â†’ github-workflow-automation
â”œâ”€ Releases â†’ github-release-management
â””â”€ No â†’ Continue

Q: Performance issue?
â”œâ”€ Find bottlenecks â†’ performance-analysis
â”œâ”€ Optimize database â†’ agentdb-optimization
â””â”€ No â†’ Continue

Q: Quality assurance?
â””â”€ verification-quality (always recommended)
```

---

## 8. Performance Metrics Reference

### Proven Speed Improvements

```
Sequential Development: 60 minutes
Parallel (Swarm): ~20 minutes
Speedup: 2.8-4.4x

Token Usage Reduction: 32.3%
Context Pollution Reduction: 40%
SWE-Bench Solve Rate: 84.8%
```

### AgentDB Performance

```
Pattern Search:
â”œâ”€ Legacy ChromaDB: 15ms
â””â”€ AgentDB HNSW: <100Âµs (150x faster)

Batch Operations:
â”œâ”€ Legacy: 1 second (100 vectors)
â””â”€ AgentDB: 2ms (500x faster)

Large-scale (1M vectors):
â”œâ”€ Legacy: 100 seconds
â””â”€ AgentDB: 8ms (12,500x faster)

Memory Efficiency:
â”œâ”€ Binary quantization: 32x reduction (3GB â†’ 96MB)
â”œâ”€ Scalar quantization: 4x reduction (3GB â†’ 768MB)
â””â”€ Product quantization: 8-16x reduction
```

### ReasoningBank Learning Curve

```
First occurrence: 15 minutes (learn pattern)
â”œâ”€ Confidence: 50%
â””â”€ Token usage: 500 tokens

Second occurrence: 8 minutes (47% faster)
â”œâ”€ Confidence: 68%
â””â”€ Token usage: 200 tokens (60% reduction)

Fifth occurrence: 5 minutes (65% faster)
â”œâ”€ Confidence: 85%
â””â”€ Token usage: 50 tokens (90% reduction)
```

---

## 9. Conclusion

### The Paradigm Shift

**Old Way (Slash Commands):**
- Explicit invocation (`/command`)
- All commands loaded (context pollution)
- Flat namespace (hard to organize)
- Manual coordination required

**New Way (Skills System):**
- Intent-based activation (natural language)
- Progressive disclosure (40% less context)
- Organized categories (easy discovery)
- Automatic composition (skills work together)

### Golden Rules

1. **Think in conversations, not commands**
   - Say what you want, not how to do it
   - Let skills choose optimal methods

2. **Start high-level, go low-level only when needed**
   - Natural language first (95% of time)
   - CLI/MCP when automation or precision required (5%)

3. **One skill per abstraction level**
   - Don't mix sparc + swarm + hive-mind
   - Pick the right level, let it compose others

4. **Enable learning always**
   - Use `--reasoningbank-enabled` flag
   - Gets 47-65% faster over time
   - 32.3% token reduction

5. **Use memory for coordination**
   - Namespace your data (project/, skills/, sessions/, learnings/)
   - Set appropriate TTLs
   - Share state between skills

### Next Steps

1. **Explore:** Say "create a custom skill for our API patterns"
2. **Practice:** Say "let's pair program on this feature"
3. **Scale:** Say "create a swarm to build this microservice"
4. **Automate:** Say "setup hooks for automatic formatting"
5. **Learn:** Say "remember this solution for future issues"

---

## Resources

- **Complete Skills Tutorial:** [skills-tutorial.md](../../skills-tutorial.md)
- **Skills Quickstart:** [skills-quickstart.md](./skills-quickstart.md)
- **SPARC Reference:** [../../reference/SPARC.md](../../reference/SPARC.md)
- **Hive-Mind Reference:** [HIVE-MIND-REFERENCE.md](./HIVE-MIND-REFERENCE.md)
- **GitHub:** https://github.com/ruvnet/claude-flow
- **Issues:** https://github.com/ruvnet/claude-flow/issues

---

**Remember:** Skills are discovered automatically. Just describe what you want, and Claude activates the right capabilities at the right time!
