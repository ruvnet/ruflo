{
  "version": 3,
  "sources": ["../../../src/services/agentic-flow-hooks/llm-hooks.ts"],
  "sourcesContent": ["/**\n * LLM-specific hooks for agentic-flow integration\n * \n * Provides pre/post operation hooks for all LLM calls with\n * memory persistence and performance optimization.\n */\n\nimport { agenticHookManager } from './hook-manager.js';\nimport type {\n  AgenticHookContext,\n  HookHandlerResult,\n  LLMHookPayload,\n  LLMMetrics,\n  Pattern,\n  SideEffect,\n} from './types.js';\n\n// ===== Pre-LLM Call Hook =====\n\nexport const preLLMCallHook = {\n  id: 'agentic-pre-llm-call',\n  type: 'pre-llm-call' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, operation, request } = payload;\n    \n    // Check memory for similar requests\n    const cacheKey = generateCacheKey(provider, model, request);\n    const cached = await checkMemoryCache(cacheKey, context);\n    \n    if (cached) {\n      return {\n        continue: false, // Skip LLM call\n        modified: true,\n        payload: {\n          ...payload,\n          response: cached.response,\n          metrics: {\n            ...cached.metrics,\n            cacheHit: true,\n          },\n        },\n        sideEffects: [\n          {\n            type: 'metric',\n            action: 'increment',\n            data: { name: 'llm.cache.hits' },\n          },\n        ],\n      };\n    }\n    \n    // Load provider-specific optimizations\n    const optimizations = await loadProviderOptimizations(provider, context);\n    \n    // Apply request optimizations\n    const optimizedRequest = applyRequestOptimizations(\n      request,\n      optimizations,\n      context\n    );\n    \n    // Track pre-call metrics\n    const sideEffects: SideEffect[] = [\n      {\n        type: 'metric',\n        action: 'increment',\n        data: { name: `llm.calls.${provider}.${model}` },\n      },\n      {\n        type: 'memory',\n        action: 'store',\n        data: {\n          key: `llm:request:${context.correlationId}`,\n          value: {\n            provider,\n            model,\n            operation,\n            request: optimizedRequest,\n            timestamp: Date.now(),\n          },\n          ttl: 3600, // 1 hour\n        },\n      },\n    ];\n    \n    return {\n      continue: true,\n      modified: true,\n      payload: {\n        ...payload,\n        request: optimizedRequest,\n      },\n      sideEffects,\n    };\n  },\n};\n\n// ===== Post-LLM Call Hook =====\n\nexport const postLLMCallHook = {\n  id: 'agentic-post-llm-call',\n  type: 'post-llm-call' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, request, response, metrics } = payload;\n    \n    if (!response || !metrics) {\n      return { continue: true };\n    }\n    \n    const sideEffects: SideEffect[] = [];\n    \n    // Store response in memory for caching\n    const cacheKey = generateCacheKey(provider, model, request);\n    sideEffects.push({\n      type: 'memory',\n      action: 'store',\n      data: {\n        key: `llm:cache:${cacheKey}`,\n        value: {\n          response,\n          metrics,\n          timestamp: Date.now(),\n        },\n        ttl: determineCacheTTL(operation, response),\n      },\n    });\n    \n    // Extract patterns for neural training\n    const patterns = extractResponsePatterns(request, response, metrics);\n    if (patterns.length > 0) {\n      sideEffects.push({\n        type: 'neural',\n        action: 'train',\n        data: {\n          patterns,\n          modelId: `llm-optimizer-${provider}`,\n        },\n      });\n    }\n    \n    // Update performance metrics\n    sideEffects.push(\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.latency.${provider}.${model}`,\n          value: metrics.latency,\n        },\n      },\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.tokens.${provider}.${model}`,\n          value: response.usage.totalTokens,\n        },\n      },\n      {\n        type: 'metric',\n        action: 'update',\n        data: {\n          name: `llm.cost.${provider}.${model}`,\n          value: metrics.costEstimate,\n        },\n      }\n    );\n    \n    // Check for performance issues\n    if (metrics.latency > getLatencyThreshold(provider, model)) {\n      sideEffects.push({\n        type: 'notification',\n        action: 'send',\n        data: {\n          level: 'warning',\n          message: `High latency detected for ${provider}/${model}: ${metrics.latency}ms`,\n        },\n      });\n    }\n    \n    // Store provider health score\n    await updateProviderHealth(provider, metrics.providerHealth, context);\n    \n    return {\n      continue: true,\n      sideEffects,\n    };\n  },\n};\n\n// ===== LLM Error Hook =====\n\nexport const llmErrorHook = {\n  id: 'agentic-llm-error',\n  type: 'llm-error' as const,\n  priority: 100,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, error } = payload;\n    \n    if (!error) {\n      return { continue: true };\n    }\n    \n    const sideEffects: SideEffect[] = [];\n    \n    // Log error details\n    sideEffects.push({\n      type: 'log',\n      action: 'write',\n      data: {\n        level: 'error',\n        message: `LLM error from ${provider}/${model}`,\n        data: {\n          error: error.message,\n          stack: error.stack,\n          request: payload.request,\n        },\n      },\n    });\n    \n    // Update error metrics\n    sideEffects.push({\n      type: 'metric',\n      action: 'increment',\n      data: { name: `llm.errors.${provider}.${model}` },\n    });\n    \n    // Check if we should fallback\n    const fallbackProvider = await selectFallbackProvider(\n      provider,\n      model,\n      error,\n      context\n    );\n    \n    if (fallbackProvider) {\n      return {\n        continue: false, // Don't propagate error\n        modified: true,\n        payload: {\n          ...payload,\n          provider: fallbackProvider.provider,\n          model: fallbackProvider.model,\n          error: undefined, // Clear error for retry\n        },\n        sideEffects: [\n          ...sideEffects,\n          {\n            type: 'notification',\n            action: 'send',\n            data: {\n              level: 'info',\n              message: `Falling back from ${provider}/${model} to ${fallbackProvider.provider}/${fallbackProvider.model}`,\n            },\n          },\n        ],\n      };\n    }\n    \n    return {\n      continue: true,\n      sideEffects,\n    };\n  },\n};\n\n// ===== LLM Retry Hook =====\n\nexport const llmRetryHook = {\n  id: 'agentic-llm-retry',\n  type: 'llm-retry' as const,\n  priority: 90,\n  handler: async (\n    payload: LLMHookPayload,\n    context: AgenticHookContext\n  ): Promise<HookHandlerResult> => {\n    const { provider, model, metrics } = payload;\n    const retryCount = metrics?.retryCount || 0;\n    \n    // Adjust request parameters for retry\n    const adjustedRequest = adjustRequestForRetry(\n      payload.request,\n      retryCount\n    );\n    \n    const sideEffects: SideEffect[] = [\n      {\n        type: 'metric',\n        action: 'increment',\n        data: { name: `llm.retries.${provider}.${model}` },\n      },\n    ];\n    \n    // Apply exponential backoff\n    const backoffMs = Math.min(1000 * Math.pow(2, retryCount), 10000);\n    await new Promise(resolve => setTimeout(resolve, backoffMs));\n    \n    return {\n      continue: true,\n      modified: true,\n      payload: {\n        ...payload,\n        request: adjustedRequest,\n        metrics: {\n          ...metrics,\n          retryCount: retryCount + 1,\n        },\n      },\n      sideEffects,\n    };\n  },\n};\n\n// ===== Helper Functions =====\n\nfunction generateCacheKey(\n  provider: string,\n  model: string,\n  request: LLMHookPayload['request']\n): string {\n  const normalized = {\n    provider,\n    model,\n    messages: request.messages?.map(m => ({\n      role: m.role,\n      content: m.content.substring(0, 100), // First 100 chars\n    })),\n    temperature: request.temperature,\n    maxTokens: request.maxTokens,\n  };\n  \n  return Buffer.from(JSON.stringify(normalized)).toString('base64');\n}\n\nasync function checkMemoryCache(\n  cacheKey: string,\n  context: AgenticHookContext\n): Promise<any | null> {\n  // Implementation would integrate with memory service\n  // This is a placeholder\n  return null;\n}\n\nasync function loadProviderOptimizations(\n  provider: string,\n  context: AgenticHookContext\n): Promise<any> {\n  // Load provider-specific optimizations from memory\n  // This is a placeholder\n  return {\n    maxRetries: 3,\n    timeout: 30000,\n    rateLimit: 100,\n  };\n}\n\nfunction applyRequestOptimizations(\n  request: LLMHookPayload['request'],\n  optimizations: any,\n  context: AgenticHookContext\n): LLMHookPayload['request'] {\n  // Apply various optimizations\n  const optimized = { ...request };\n  \n  // Optimize token usage\n  if (optimized.maxTokens && optimized.maxTokens > 4000) {\n    optimized.maxTokens = 4000; // Cap at reasonable limit\n  }\n  \n  // Optimize temperature for consistency\n  if (optimized.temperature === undefined) {\n    optimized.temperature = 0.7;\n  }\n  \n  // Add stop sequences if missing\n  if (!optimized.stopSequences && optimized.messages) {\n    optimized.stopSequences = ['\\n\\nHuman:', '\\n\\nAssistant:'];\n  }\n  \n  return optimized;\n}\n\nfunction determineCacheTTL(\n  operation: string,\n  response: LLMHookPayload['response']\n): number {\n  // Determine cache TTL based on operation and response\n  switch (operation) {\n    case 'embedding':\n      return 86400; // 24 hours for embeddings\n    case 'completion':\n      // Shorter TTL for completions\n      return response?.usage?.totalTokens && response.usage.totalTokens > 1000\n        ? 1800 // 30 minutes for long responses\n        : 3600; // 1 hour for short responses\n    default:\n      return 3600; // 1 hour default\n  }\n}\n\nfunction extractResponsePatterns(\n  request: LLMHookPayload['request'],\n  response: LLMHookPayload['response'],\n  metrics: LLMMetrics\n): Pattern[] {\n  const patterns: Pattern[] = [];\n  \n  // Extract performance patterns\n  if (metrics.latency > 1000) {\n    patterns.push({\n      id: `perf_${Date.now()}`,\n      type: 'optimization',\n      confidence: 0.8,\n      occurrences: 1,\n      context: {\n        provider: metrics.providerHealth < 0.8 ? 'unhealthy' : 'healthy',\n        requestSize: JSON.stringify(request).length,\n        responseTokens: response?.usage?.totalTokens || 0,\n        latency: metrics.latency,\n      },\n    });\n  }\n  \n  // Extract success patterns\n  if (response?.choices?.[0]?.finishReason === 'stop') {\n    patterns.push({\n      id: `success_${Date.now()}`,\n      type: 'success',\n      confidence: 0.9,\n      occurrences: 1,\n      context: {\n        temperature: request.temperature,\n        maxTokens: request.maxTokens,\n        actualTokens: response.usage?.totalTokens || 0,\n      },\n    });\n  }\n  \n  return patterns;\n}\n\nfunction getLatencyThreshold(provider: string, model: string): number {\n  // Provider/model specific thresholds\n  const thresholds: Record<string, number> = {\n    'openai:gpt-4': 5000,\n    'openai:gpt-3.5-turbo': 2000,\n    'anthropic:claude-3': 4000,\n    'anthropic:claude-instant': 1500,\n  };\n  \n  return thresholds[`${provider}:${model}`] || 3000;\n}\n\nasync function updateProviderHealth(\n  provider: string,\n  health: number,\n  context: AgenticHookContext\n): Promise<void> {\n  // Update provider health in memory\n  const healthKey = `provider:health:${provider}`;\n  const currentHealth = await context.memory.cache.get(healthKey) || [];\n  \n  currentHealth.push({\n    timestamp: Date.now(),\n    health,\n  });\n  \n  // Keep last 100 health checks\n  if (currentHealth.length > 100) {\n    currentHealth.shift();\n  }\n  \n  await context.memory.cache.set(healthKey, currentHealth);\n}\n\nasync function selectFallbackProvider(\n  provider: string,\n  model: string,\n  error: Error,\n  context: AgenticHookContext\n): Promise<{ provider: string; model: string } | null> {\n  // Implement intelligent fallback selection\n  const fallbacks: Record<string, { provider: string; model: string }[]> = {\n    'openai': [\n      { provider: 'anthropic', model: 'claude-3' },\n      { provider: 'cohere', model: 'command' },\n    ],\n    'anthropic': [\n      { provider: 'openai', model: 'gpt-4' },\n      { provider: 'cohere', model: 'command' },\n    ],\n  };\n  \n  const candidates = fallbacks[provider] || [];\n  \n  // Select based on health scores\n  for (const candidate of candidates) {\n    const healthKey = `provider:health:${candidate.provider}`;\n    const healthData = await context.memory.cache.get(healthKey) || [];\n    \n    if (healthData.length > 0) {\n      const avgHealth = healthData.reduce((sum: number, h: any) => \n        sum + h.health, 0\n      ) / healthData.length;\n      \n      if (avgHealth > 0.7) {\n        return candidate;\n      }\n    }\n  }\n  \n  return null;\n}\n\nfunction adjustRequestForRetry(\n  request: LLMHookPayload['request'],\n  retryCount: number\n): LLMHookPayload['request'] {\n  const adjusted = { ...request };\n  \n  // Increase temperature slightly for variety\n  if (adjusted.temperature !== undefined) {\n    adjusted.temperature = Math.min(\n      adjusted.temperature + (0.1 * retryCount),\n      1.0\n    );\n  }\n  \n  // Reduce max tokens to improve success rate\n  if (adjusted.maxTokens !== undefined) {\n    adjusted.maxTokens = Math.floor(\n      adjusted.maxTokens * Math.pow(0.9, retryCount)\n    );\n  }\n  \n  return adjusted;\n}\n\n// ===== Register Hooks =====\n\nexport function registerLLMHooks(): void {\n  agenticHookManager.register(preLLMCallHook);\n  agenticHookManager.register(postLLMCallHook);\n  agenticHookManager.register(llmErrorHook);\n  agenticHookManager.register(llmRetryHook);\n}"],
  "mappings": ";;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAOA,0BAAmC;AAY5B,MAAM,iBAAiB;AAAA,EAC5B,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,UAAU;AAAA,EACV,SAAS,OACP,SACA,YAC+B;AAC/B,UAAM,EAAE,UAAU,OAAO,WAAAA,YAAW,QAAQ,IAAI;AAGhD,UAAM,WAAW,iBAAiB,UAAU,OAAO,OAAO;AAC1D,UAAM,SAAS,MAAM,iBAAiB,UAAU,OAAO;AAEvD,QAAI,QAAQ;AACV,aAAO;AAAA,QACL,UAAU;AAAA;AAAA,QACV,UAAU;AAAA,QACV,SAAS;AAAA,UACP,GAAG;AAAA,UACH,UAAU,OAAO;AAAA,UACjB,SAAS;AAAA,YACP,GAAG,OAAO;AAAA,YACV,UAAU;AAAA,UACZ;AAAA,QACF;AAAA,QACA,aAAa;AAAA,UACX;AAAA,YACE,MAAM;AAAA,YACN,QAAQ;AAAA,YACR,MAAM,EAAE,MAAM,iBAAiB;AAAA,UACjC;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAGA,UAAM,gBAAgB,MAAM,0BAA0B,UAAU,OAAO;AAGvE,UAAM,mBAAmB;AAAA,MACvB;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,UAAM,cAA4B;AAAA,MAChC;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM,EAAE,MAAM,aAAa,QAAQ,IAAI,KAAK,GAAG;AAAA,MACjD;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ,KAAK,eAAe,QAAQ,aAAa;AAAA,UACzC,OAAO;AAAA,YACL;AAAA,YACA;AAAA,YACA,WAAAA;AAAA,YACA,SAAS;AAAA,YACT,WAAW,KAAK,IAAI;AAAA,UACtB;AAAA,UACA,KAAK;AAAA;AAAA,QACP;AAAA,MACF;AAAA,IACF;AAEA,WAAO;AAAA,MACL,UAAU;AAAA,MACV,UAAU;AAAA,MACV,SAAS;AAAA,QACP,GAAG;AAAA,QACH,SAAS;AAAA,MACX;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;AAIO,MAAM,kBAAkB;AAAA,EAC7B,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,UAAU;AAAA,EACV,SAAS,OACP,SACA,YAC+B;AAC/B,UAAM,EAAE,UAAU,OAAO,SAAS,UAAU,QAAQ,IAAI;AAExD,QAAI,CAAC,YAAY,CAAC,SAAS;AACzB,aAAO,EAAE,UAAU,KAAK;AAAA,IAC1B;AAEA,UAAM,cAA4B,CAAC;AAGnC,UAAM,WAAW,iBAAiB,UAAU,OAAO,OAAO;AAC1D,gBAAY,KAAK;AAAA,MACf,MAAM;AAAA,MACN,QAAQ;AAAA,MACR,MAAM;AAAA,QACJ,KAAK,aAAa,QAAQ;AAAA,QAC1B,OAAO;AAAA,UACL;AAAA,UACA;AAAA,UACA,WAAW,KAAK,IAAI;AAAA,QACtB;AAAA,QACA,KAAK,kBAAkB,WAAW,QAAQ;AAAA,MAC5C;AAAA,IACF,CAAC;AAGD,UAAM,WAAW,wBAAwB,SAAS,UAAU,OAAO;AACnE,QAAI,SAAS,SAAS,GAAG;AACvB,kBAAY,KAAK;AAAA,QACf,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ;AAAA,UACA,SAAS,iBAAiB,QAAQ;AAAA,QACpC;AAAA,MACF,CAAC;AAAA,IACH;AAGA,gBAAY;AAAA,MACV;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ,MAAM,eAAe,QAAQ,IAAI,KAAK;AAAA,UACtC,OAAO,QAAQ;AAAA,QACjB;AAAA,MACF;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ,MAAM,cAAc,QAAQ,IAAI,KAAK;AAAA,UACrC,OAAO,SAAS,MAAM;AAAA,QACxB;AAAA,MACF;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ,MAAM,YAAY,QAAQ,IAAI,KAAK;AAAA,UACnC,OAAO,QAAQ;AAAA,QACjB;AAAA,MACF;AAAA,IACF;AAGA,QAAI,QAAQ,UAAU,oBAAoB,UAAU,KAAK,GAAG;AAC1D,kBAAY,KAAK;AAAA,QACf,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM;AAAA,UACJ,OAAO;AAAA,UACP,SAAS,6BAA6B,QAAQ,IAAI,KAAK,KAAK,QAAQ,OAAO;AAAA,QAC7E;AAAA,MACF,CAAC;AAAA,IACH;AAGA,UAAM,qBAAqB,UAAU,QAAQ,gBAAgB,OAAO;AAEpE,WAAO;AAAA,MACL,UAAU;AAAA,MACV;AAAA,IACF;AAAA,EACF;AACF;AAIO,MAAM,eAAe;AAAA,EAC1B,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,UAAU;AAAA,EACV,SAAS,OACP,SACA,YAC+B;AAC/B,UAAM,EAAE,UAAU,OAAO,MAAM,IAAI;AAEnC,QAAI,CAAC,OAAO;AACV,aAAO,EAAE,UAAU,KAAK;AAAA,IAC1B;AAEA,UAAM,cAA4B,CAAC;AAGnC,gBAAY,KAAK;AAAA,MACf,MAAM;AAAA,MACN,QAAQ;AAAA,MACR,MAAM;AAAA,QACJ,OAAO;AAAA,QACP,SAAS,kBAAkB,QAAQ,IAAI,KAAK;AAAA,QAC5C,MAAM;AAAA,UACJ,OAAO,MAAM;AAAA,UACb,OAAO,MAAM;AAAA,UACb,SAAS,QAAQ;AAAA,QACnB;AAAA,MACF;AAAA,IACF,CAAC;AAGD,gBAAY,KAAK;AAAA,MACf,MAAM;AAAA,MACN,QAAQ;AAAA,MACR,MAAM,EAAE,MAAM,cAAc,QAAQ,IAAI,KAAK,GAAG;AAAA,IAClD,CAAC;AAGD,UAAM,mBAAmB,MAAM;AAAA,MAC7B;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,QAAI,kBAAkB;AACpB,aAAO;AAAA,QACL,UAAU;AAAA;AAAA,QACV,UAAU;AAAA,QACV,SAAS;AAAA,UACP,GAAG;AAAA,UACH,UAAU,iBAAiB;AAAA,UAC3B,OAAO,iBAAiB;AAAA,UACxB,OAAO;AAAA;AAAA,QACT;AAAA,QACA,aAAa;AAAA,UACX,GAAG;AAAA,UACH;AAAA,YACE,MAAM;AAAA,YACN,QAAQ;AAAA,YACR,MAAM;AAAA,cACJ,OAAO;AAAA,cACP,SAAS,qBAAqB,QAAQ,IAAI,KAAK,OAAO,iBAAiB,QAAQ,IAAI,iBAAiB,KAAK;AAAA,YAC3G;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,WAAO;AAAA,MACL,UAAU;AAAA,MACV;AAAA,IACF;AAAA,EACF;AACF;AAIO,MAAM,eAAe;AAAA,EAC1B,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,UAAU;AAAA,EACV,SAAS,OACP,SACA,YAC+B;AAC/B,UAAM,EAAE,UAAU,OAAO,QAAQ,IAAI;AACrC,UAAM,aAAa,SAAS,cAAc;AAG1C,UAAM,kBAAkB;AAAA,MACtB,QAAQ;AAAA,MACR;AAAA,IACF;AAEA,UAAM,cAA4B;AAAA,MAChC;AAAA,QACE,MAAM;AAAA,QACN,QAAQ;AAAA,QACR,MAAM,EAAE,MAAM,eAAe,QAAQ,IAAI,KAAK,GAAG;AAAA,MACnD;AAAA,IACF;AAGA,UAAM,YAAY,KAAK,IAAI,MAAO,KAAK,IAAI,GAAG,UAAU,GAAG,GAAK;AAChE,UAAM,IAAI,QAAQ,aAAW,WAAW,SAAS,SAAS,CAAC;AAE3D,WAAO;AAAA,MACL,UAAU;AAAA,MACV,UAAU;AAAA,MACV,SAAS;AAAA,QACP,GAAG;AAAA,QACH,SAAS;AAAA,QACT,SAAS;AAAA,UACP,GAAG;AAAA,UACH,YAAY,aAAa;AAAA,QAC3B;AAAA,MACF;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;AAIA,SAAS,iBACP,UACA,OACA,SACQ;AACR,QAAM,aAAa;AAAA,IACjB;AAAA,IACA;AAAA,IACA,UAAU,QAAQ,UAAU,IAAI,QAAM;AAAA,MACpC,MAAM,EAAE;AAAA,MACR,SAAS,EAAE,QAAQ,UAAU,GAAG,GAAG;AAAA;AAAA,IACrC,EAAE;AAAA,IACF,aAAa,QAAQ;AAAA,IACrB,WAAW,QAAQ;AAAA,EACrB;AAEA,SAAO,OAAO,KAAK,KAAK,UAAU,UAAU,CAAC,EAAE,SAAS,QAAQ;AAClE;AAjBS;AAmBT,eAAe,iBACb,UACA,SACqB;AAGrB,SAAO;AACT;AAPe;AASf,eAAe,0BACb,UACA,SACc;AAGd,SAAO;AAAA,IACL,YAAY;AAAA,IACZ,SAAS;AAAA,IACT,WAAW;AAAA,EACb;AACF;AAXe;AAaf,SAAS,0BACP,SACA,eACA,SAC2B;AAE3B,QAAM,YAAY,EAAE,GAAG,QAAQ;AAG/B,MAAI,UAAU,aAAa,UAAU,YAAY,KAAM;AACrD,cAAU,YAAY;AAAA,EACxB;AAGA,MAAI,UAAU,gBAAgB,QAAW;AACvC,cAAU,cAAc;AAAA,EAC1B;AAGA,MAAI,CAAC,UAAU,iBAAiB,UAAU,UAAU;AAClD,cAAU,gBAAgB,CAAC,cAAc,gBAAgB;AAAA,EAC3D;AAEA,SAAO;AACT;AAxBS;AA0BT,SAAS,kBACPA,YACA,UACQ;AAER,UAAQA,YAAW;AAAA,IACjB,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAEH,aAAO,UAAU,OAAO,eAAe,SAAS,MAAM,cAAc,MAChE,OACA;AAAA,IACN;AACE,aAAO;AAAA,EACX;AACF;AAhBS;AAkBT,SAAS,wBACP,SACA,UACA,SACW;AACX,QAAM,WAAsB,CAAC;AAG7B,MAAI,QAAQ,UAAU,KAAM;AAC1B,aAAS,KAAK;AAAA,MACZ,IAAI,QAAQ,KAAK,IAAI,CAAC;AAAA,MACtB,MAAM;AAAA,MACN,YAAY;AAAA,MACZ,aAAa;AAAA,MACb,SAAS;AAAA,QACP,UAAU,QAAQ,iBAAiB,MAAM,cAAc;AAAA,QACvD,aAAa,KAAK,UAAU,OAAO,EAAE;AAAA,QACrC,gBAAgB,UAAU,OAAO,eAAe;AAAA,QAChD,SAAS,QAAQ;AAAA,MACnB;AAAA,IACF,CAAC;AAAA,EACH;AAGA,MAAI,UAAU,UAAU,CAAC,GAAG,iBAAiB,QAAQ;AACnD,aAAS,KAAK;AAAA,MACZ,IAAI,WAAW,KAAK,IAAI,CAAC;AAAA,MACzB,MAAM;AAAA,MACN,YAAY;AAAA,MACZ,aAAa;AAAA,MACb,SAAS;AAAA,QACP,aAAa,QAAQ;AAAA,QACrB,WAAW,QAAQ;AAAA,QACnB,cAAc,SAAS,OAAO,eAAe;AAAA,MAC/C;AAAA,IACF,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAvCS;AAyCT,SAAS,oBAAoB,UAAkB,OAAuB;AAEpE,QAAM,aAAqC;AAAA,IACzC,gBAAgB;AAAA,IAChB,wBAAwB;AAAA,IACxB,sBAAsB;AAAA,IACtB,4BAA4B;AAAA,EAC9B;AAEA,SAAO,WAAW,GAAG,QAAQ,IAAI,KAAK,EAAE,KAAK;AAC/C;AAVS;AAYT,eAAe,qBACb,UACA,QACA,SACe;AAEf,QAAM,YAAY,mBAAmB,QAAQ;AAC7C,QAAM,gBAAgB,MAAM,QAAQ,OAAO,MAAM,IAAI,SAAS,KAAK,CAAC;AAEpE,gBAAc,KAAK;AAAA,IACjB,WAAW,KAAK,IAAI;AAAA,IACpB;AAAA,EACF,CAAC;AAGD,MAAI,cAAc,SAAS,KAAK;AAC9B,kBAAc,MAAM;AAAA,EACtB;AAEA,QAAM,QAAQ,OAAO,MAAM,IAAI,WAAW,aAAa;AACzD;AApBe;AAsBf,eAAe,uBACb,UACA,OACA,OACA,SACqD;AAErD,QAAM,YAAmE;AAAA,IACvE,UAAU;AAAA,MACR,EAAE,UAAU,aAAa,OAAO,WAAW;AAAA,MAC3C,EAAE,UAAU,UAAU,OAAO,UAAU;AAAA,IACzC;AAAA,IACA,aAAa;AAAA,MACX,EAAE,UAAU,UAAU,OAAO,QAAQ;AAAA,MACrC,EAAE,UAAU,UAAU,OAAO,UAAU;AAAA,IACzC;AAAA,EACF;AAEA,QAAM,aAAa,UAAU,QAAQ,KAAK,CAAC;AAG3C,aAAW,aAAa,YAAY;AAClC,UAAM,YAAY,mBAAmB,UAAU,QAAQ;AACvD,UAAM,aAAa,MAAM,QAAQ,OAAO,MAAM,IAAI,SAAS,KAAK,CAAC;AAEjE,QAAI,WAAW,SAAS,GAAG;AACzB,YAAM,YAAY,WAAW;AAAA,QAAO,CAAC,KAAa,MAChD,MAAM,EAAE;AAAA,QAAQ;AAAA,MAClB,IAAI,WAAW;AAEf,UAAI,YAAY,KAAK;AACnB,eAAO;AAAA,MACT;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;AArCe;AAuCf,SAAS,sBACP,SACA,YAC2B;AAC3B,QAAM,WAAW,EAAE,GAAG,QAAQ;AAG9B,MAAI,SAAS,gBAAgB,QAAW;AACtC,aAAS,cAAc,KAAK;AAAA,MAC1B,SAAS,cAAe,MAAM;AAAA,MAC9B;AAAA,IACF;AAAA,EACF;AAGA,MAAI,SAAS,cAAc,QAAW;AACpC,aAAS,YAAY,KAAK;AAAA,MACxB,SAAS,YAAY,KAAK,IAAI,KAAK,UAAU;AAAA,IAC/C;AAAA,EACF;AAEA,SAAO;AACT;AAtBS;AA0BF,SAAS,mBAAyB;AACvC,yCAAmB,SAAS,cAAc;AAC1C,yCAAmB,SAAS,eAAe;AAC3C,yCAAmB,SAAS,YAAY;AACxC,yCAAmB,SAAS,YAAY;AAC1C;AALgB;",
  "names": ["operation"]
}
