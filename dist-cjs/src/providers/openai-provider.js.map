{"version":3,"sources":["../../../src/providers/openai-provider.ts"],"sourcesContent":["/**\r\n * OpenAI Provider Implementation\r\n * Supports GPT-4, GPT-3.5, and other OpenAI models\r\n */\r\n\r\nimport { BaseProvider } from './base-provider.js';\r\nimport {\r\n  LLMProvider,\r\n  LLMModel,\r\n  LLMRequest,\r\n  LLMResponse,\r\n  LLMStreamEvent,\r\n  ModelInfo,\r\n  ProviderCapabilities,\r\n  HealthCheckResult,\r\n  LLMProviderError,\r\n  RateLimitError,\r\n  AuthenticationError,\r\n  ModelNotFoundError,\r\n} from './types.js';\r\n\r\ninterface OpenAIRequest {\r\n  model: string;\r\n  messages: Array<{\r\n    role: 'system' | 'user' | 'assistant' | 'function';\r\n    content: string;\r\n    name?: string;\r\n    function_call?: {\r\n      name: string;\r\n      arguments: string;\r\n    };\r\n  }>;\r\n  temperature?: number;\r\n  max_tokens?: number;\r\n  top_p?: number;\r\n  frequency_penalty?: number;\r\n  presence_penalty?: number;\r\n  stop?: string[];\r\n  stream?: boolean;\r\n  functions?: Array<{\r\n    name: string;\r\n    description: string;\r\n    parameters: any;\r\n  }>;\r\n  function_call?: 'auto' | 'none' | { name: string };\r\n}\r\n\r\ninterface OpenAIResponse {\r\n  id: string;\r\n  object: string;\r\n  created: number;\r\n  model: string;\r\n  choices: Array<{\r\n    index: number;\r\n    message: {\r\n      role: string;\r\n      content: string | null;\r\n      function_call?: {\r\n        name: string;\r\n        arguments: string;\r\n      };\r\n    };\r\n    finish_reason: 'stop' | 'length' | 'function_call' | 'content_filter';\r\n  }>;\r\n  usage: {\r\n    prompt_tokens: number;\r\n    completion_tokens: number;\r\n    total_tokens: number;\r\n  };\r\n}\r\n\r\ninterface OpenAIStreamChunk {\r\n  id: string;\r\n  object: string;\r\n  created: number;\r\n  model: string;\r\n  choices: Array<{\r\n    index: number;\r\n    delta: {\r\n      role?: string;\r\n      content?: string;\r\n      function_call?: {\r\n        name?: string;\r\n        arguments?: string;\r\n      };\r\n    };\r\n    finish_reason?: string;\r\n  }>;\r\n}\r\n\r\nexport class OpenAIProvider extends BaseProvider {\r\n  readonly name: LLMProvider = 'openai';\r\n  readonly capabilities: ProviderCapabilities = {\r\n    supportedModels: [\r\n      'gpt-4-turbo-preview',\r\n      'gpt-4',\r\n      'gpt-4-32k',\r\n      'gpt-3.5-turbo',\r\n      'gpt-3.5-turbo-16k',\r\n    ],\r\n    maxContextLength: {\r\n      'gpt-4-turbo-preview': 128000,\r\n      'gpt-4': 8192,\r\n      'gpt-4-32k': 32768,\r\n      'gpt-3.5-turbo': 4096,\r\n      'gpt-3.5-turbo-16k': 16384,\r\n    } as Record<LLMModel, number>,\r\n    maxOutputTokens: {\r\n      'gpt-4-turbo-preview': 4096,\r\n      'gpt-4': 4096,\r\n      'gpt-4-32k': 4096,\r\n      'gpt-3.5-turbo': 4096,\r\n      'gpt-3.5-turbo-16k': 4096,\r\n    } as Record<LLMModel, number>,\r\n    supportsStreaming: true,\r\n    supportsFunctionCalling: true,\r\n    supportsSystemMessages: true,\r\n    supportsVision: true, // GPT-4 with vision\r\n    supportsAudio: false,\r\n    supportsTools: true,\r\n    supportsFineTuning: true,\r\n    supportsEmbeddings: true,\r\n    supportsLogprobs: true,\r\n    supportsBatching: true,\r\n    rateLimit: {\r\n      requestsPerMinute: 3500,\r\n      tokensPerMinute: 90000,\r\n      concurrentRequests: 100,\r\n    },\r\n    pricing: {\r\n      'gpt-4-turbo-preview': {\r\n        promptCostPer1k: 0.01,\r\n        completionCostPer1k: 0.03,\r\n        currency: 'USD',\r\n      },\r\n      'gpt-4': {\r\n        promptCostPer1k: 0.03,\r\n        completionCostPer1k: 0.06,\r\n        currency: 'USD',\r\n      },\r\n      'gpt-4-32k': {\r\n        promptCostPer1k: 0.06,\r\n        completionCostPer1k: 0.12,\r\n        currency: 'USD',\r\n      },\r\n      'gpt-3.5-turbo': {\r\n        promptCostPer1k: 0.0005,\r\n        completionCostPer1k: 0.0015,\r\n        currency: 'USD',\r\n      },\r\n      'gpt-3.5-turbo-16k': {\r\n        promptCostPer1k: 0.003,\r\n        completionCostPer1k: 0.004,\r\n        currency: 'USD',\r\n      },\r\n    },\r\n  };\r\n\r\n  private baseUrl: string;\r\n  private headers: Record<string, string>;\r\n\r\n  protected async doInitialize(): Promise<void> {\r\n    if (!this.config.apiKey) {\r\n      throw new AuthenticationError('OpenAI API key is required', 'openai');\r\n    }\r\n\r\n    this.baseUrl = this.config.apiUrl || 'https://api.openai.com/v1';\r\n    this.headers = {\r\n      'Authorization': `Bearer ${this.config.apiKey}`,\r\n      'Content-Type': 'application/json',\r\n    };\r\n\r\n    // Add organization header if provided\r\n    if (this.config.providerOptions?.organization) {\r\n      this.headers['OpenAI-Organization'] = this.config.providerOptions.organization;\r\n    }\r\n  }\r\n\r\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\r\n    const openAIRequest: OpenAIRequest = {\r\n      model: this.mapToOpenAIModel(request.model || this.config.model),\r\n      messages: request.messages.map((msg) => ({\r\n        role: msg.role,\r\n        content: msg.content,\r\n        ...(msg.name && { name: msg.name }),\r\n        ...(msg.functionCall && { function_call: msg.functionCall }),\r\n      })),\r\n      temperature: request.temperature ?? this.config.temperature,\r\n      max_tokens: request.maxTokens ?? this.config.maxTokens,\r\n      top_p: request.topP ?? this.config.topP,\r\n      frequency_penalty: request.frequencyPenalty ?? this.config.frequencyPenalty,\r\n      presence_penalty: request.presencePenalty ?? this.config.presencePenalty,\r\n      stop: request.stopSequences ?? this.config.stopSequences,\r\n      stream: false,\r\n    };\r\n\r\n    // Add function calling if present\r\n    if (request.functions) {\r\n      openAIRequest.functions = request.functions;\r\n      openAIRequest.function_call = request.functionCall;\r\n    }\r\n\r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 60000);\r\n\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\r\n        method: 'POST',\r\n        headers: this.headers,\r\n        body: JSON.stringify(openAIRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      clearTimeout(timeout);\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const data: OpenAIResponse = await response.json();\r\n      const choice = data.choices[0];\r\n\r\n      // Calculate cost\r\n      const model = request.model || this.config.model;\r\n      const pricing = this.capabilities.pricing![model];\r\n      const promptCost = (data.usage.prompt_tokens / 1000) * pricing.promptCostPer1k;\r\n      const completionCost = (data.usage.completion_tokens / 1000) * pricing.completionCostPer1k;\r\n\r\n      return {\r\n        id: data.id,\r\n        model: this.mapFromOpenAIModel(data.model),\r\n        provider: 'openai',\r\n        content: choice.message.content || '',\r\n        functionCall: choice.message.function_call,\r\n        usage: {\r\n          promptTokens: data.usage.prompt_tokens,\r\n          completionTokens: data.usage.completion_tokens,\r\n          totalTokens: data.usage.total_tokens,\r\n        },\r\n        cost: {\r\n          promptCost,\r\n          completionCost,\r\n          totalCost: promptCost + completionCost,\r\n          currency: 'USD',\r\n        },\r\n        finishReason: choice.finish_reason,\r\n      };\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      throw this.transformError(error);\r\n    }\r\n  }\r\n\r\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\r\n    const openAIRequest: OpenAIRequest = {\r\n      model: this.mapToOpenAIModel(request.model || this.config.model),\r\n      messages: request.messages.map((msg) => ({\r\n        role: msg.role,\r\n        content: msg.content,\r\n        ...(msg.name && { name: msg.name }),\r\n        ...(msg.functionCall && { function_call: msg.functionCall }),\r\n      })),\r\n      temperature: request.temperature ?? this.config.temperature,\r\n      max_tokens: request.maxTokens ?? this.config.maxTokens,\r\n      top_p: request.topP ?? this.config.topP,\r\n      frequency_penalty: request.frequencyPenalty ?? this.config.frequencyPenalty,\r\n      presence_penalty: request.presencePenalty ?? this.config.presencePenalty,\r\n      stop: request.stopSequences ?? this.config.stopSequences,\r\n      stream: true,\r\n    };\r\n\r\n    if (request.functions) {\r\n      openAIRequest.functions = request.functions;\r\n      openAIRequest.function_call = request.functionCall;\r\n    }\r\n\r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 60000) * 2);\r\n\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\r\n        method: 'POST',\r\n        headers: this.headers,\r\n        body: JSON.stringify(openAIRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const reader = response.body!.getReader();\r\n      const decoder = new TextDecoder();\r\n      let buffer = '';\r\n      const totalPromptTokens = 0;\r\n      const totalCompletionTokens = 0;\r\n\r\n      while (true) {\r\n        const { done, value } = await reader.read();\r\n        if (done) break;\r\n\r\n        buffer += decoder.decode(value, { stream: true });\r\n        const lines = buffer.split('\\n');\r\n        buffer = lines.pop() || '';\r\n\r\n        for (const line of lines) {\r\n          if (line.startsWith('data: ')) {\r\n            const data = line.slice(6);\r\n            if (data === '[DONE]') continue;\r\n\r\n            try {\r\n              const chunk: OpenAIStreamChunk = JSON.parse(data);\r\n              const delta = chunk.choices[0].delta;\r\n\r\n              if (delta.content) {\r\n                yield {\r\n                  type: 'content',\r\n                  delta: { content: delta.content },\r\n                };\r\n              }\r\n\r\n              if (delta.function_call) {\r\n                yield {\r\n                  type: 'function_call',\r\n                  delta: { functionCall: delta.function_call },\r\n                };\r\n              }\r\n\r\n              if (chunk.choices[0].finish_reason) {\r\n                // Estimate tokens for streaming\r\n                const promptTokens = this.estimateTokens(JSON.stringify(request.messages));\r\n                const completionTokens = Math.max(totalCompletionTokens, 100); // Minimum estimate\r\n\r\n                const model = request.model || this.config.model;\r\n                const pricing = this.capabilities.pricing![model];\r\n                const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\r\n                const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\r\n\r\n                yield {\r\n                  type: 'done',\r\n                  usage: {\r\n                    promptTokens,\r\n                    completionTokens,\r\n                    totalTokens: promptTokens + completionTokens,\r\n                  },\r\n                  cost: {\r\n                    promptCost,\r\n                    completionCost,\r\n                    totalCost: promptCost + completionCost,\r\n                    currency: 'USD',\r\n                  },\r\n                };\r\n              }\r\n            } catch (e) {\r\n              this.logger.warn('Failed to parse OpenAI stream chunk', { data, error: e });\r\n            }\r\n          }\r\n        }\r\n      }\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      throw this.transformError(error);\r\n    } finally {\r\n      clearTimeout(timeout);\r\n    }\r\n  }\r\n\r\n  async listModels(): Promise<LLMModel[]> {\r\n    return this.capabilities.supportedModels;\r\n  }\r\n\r\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\r\n    return {\r\n      model,\r\n      name: model,\r\n      description: this.getModelDescription(model),\r\n      contextLength: this.capabilities.maxContextLength[model] || 4096,\r\n      maxOutputTokens: this.capabilities.maxOutputTokens[model] || 4096,\r\n      supportedFeatures: [\r\n        'chat',\r\n        'completion',\r\n        'function_calling',\r\n        ...(model.includes('gpt-4') ? ['vision'] : []),\r\n      ],\r\n      pricing: this.capabilities.pricing![model],\r\n    };\r\n  }\r\n\r\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/models`, {\r\n        headers: this.headers,\r\n      });\r\n\r\n      if (!response.ok) {\r\n        throw new Error(`Health check failed: ${response.status}`);\r\n      }\r\n\r\n      return {\r\n        healthy: true,\r\n        timestamp: new Date(),\r\n      };\r\n    } catch (error) {\r\n      return {\r\n        healthy: false,\r\n        error: error instanceof Error ? error.message : 'Unknown error',\r\n        timestamp: new Date(),\r\n      };\r\n    }\r\n  }\r\n\r\n  private mapToOpenAIModel(model: LLMModel): string {\r\n    // Map our model names to OpenAI model names if needed\r\n    const modelMap: Record<string, string> = {\r\n      'gpt-4-turbo-preview': 'gpt-4-turbo-preview',\r\n      'gpt-4': 'gpt-4',\r\n      'gpt-4-32k': 'gpt-4-32k',\r\n      'gpt-3.5-turbo': 'gpt-3.5-turbo',\r\n      'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\r\n    };\r\n    return modelMap[model] || model;\r\n  }\r\n\r\n  private mapFromOpenAIModel(model: string): LLMModel {\r\n    // Ensure the model is in our supported list\r\n    return this.capabilities.supportedModels.find((m) => m === model) || 'gpt-3.5-turbo';\r\n  }\r\n\r\n  private getModelDescription(model: LLMModel): string {\r\n    const descriptions: Record<string, string> = {\r\n      'gpt-4-turbo-preview': 'Latest GPT-4 Turbo model with improved performance',\r\n      'gpt-4': 'Most capable GPT-4 model for complex tasks',\r\n      'gpt-4-32k': 'GPT-4 with extended 32k context window',\r\n      'gpt-3.5-turbo': 'Fast and efficient model for most tasks',\r\n      'gpt-3.5-turbo-16k': 'GPT-3.5 Turbo with extended context',\r\n    };\r\n    return descriptions[model] || 'OpenAI language model';\r\n  }\r\n\r\n  private async handleErrorResponse(response: Response): Promise<void> {\r\n    const errorText = await response.text();\r\n    let errorData: any;\r\n\r\n    try {\r\n      errorData = JSON.parse(errorText);\r\n    } catch {\r\n      errorData = { error: { message: errorText } };\r\n    }\r\n\r\n    const message = errorData.error?.message || 'Unknown error';\r\n\r\n    switch (response.status) {\r\n      case 401:\r\n        throw new AuthenticationError(message, 'openai', errorData);\r\n      case 429:\r\n        const retryAfter = response.headers.get('retry-after');\r\n        throw new RateLimitError(\r\n          message,\r\n          'openai',\r\n          retryAfter ? parseInt(retryAfter) : undefined,\r\n          errorData\r\n        );\r\n      case 404:\r\n        throw new ModelNotFoundError(this.config.model, 'openai', errorData);\r\n      default:\r\n        throw new LLMProviderError(\r\n          message,\r\n          `OPENAI_${response.status}`,\r\n          'openai',\r\n          response.status,\r\n          response.status >= 500,\r\n          errorData\r\n        );\r\n    }\r\n  }\r\n}"],"names":["BaseProvider","LLMProviderError","RateLimitError","AuthenticationError","ModelNotFoundError","OpenAIProvider","name","capabilities","supportedModels","maxContextLength","maxOutputTokens","supportsStreaming","supportsFunctionCalling","supportsSystemMessages","supportsVision","supportsAudio","supportsTools","supportsFineTuning","supportsEmbeddings","supportsLogprobs","supportsBatching","rateLimit","requestsPerMinute","tokensPerMinute","concurrentRequests","pricing","promptCostPer1k","completionCostPer1k","currency","baseUrl","headers","doInitialize","config","apiKey","apiUrl","providerOptions","organization","doComplete","request","openAIRequest","model","mapToOpenAIModel","messages","map","msg","role","content","functionCall","function_call","temperature","max_tokens","maxTokens","top_p","topP","frequency_penalty","frequencyPenalty","presence_penalty","presencePenalty","stop","stopSequences","stream","functions","controller","AbortController","timeout","setTimeout","abort","response","fetch","method","body","JSON","stringify","signal","clearTimeout","ok","handleErrorResponse","data","json","choice","choices","promptCost","usage","prompt_tokens","completionCost","completion_tokens","id","mapFromOpenAIModel","provider","message","promptTokens","completionTokens","totalTokens","total_tokens","cost","totalCost","finishReason","finish_reason","error","transformError","doStreamComplete","reader","getReader","decoder","TextDecoder","buffer","totalPromptTokens","totalCompletionTokens","done","value","read","decode","lines","split","pop","line","startsWith","slice","chunk","parse","delta","type","estimateTokens","Math","max","e","logger","warn","listModels","getModelInfo","description","getModelDescription","contextLength","supportedFeatures","includes","doHealthCheck","Error","status","healthy","timestamp","Date","modelMap","find","m","descriptions","errorText","text","errorData","retryAfter","get","parseInt","undefined"],"mappings":"AAKA,SAASA,YAAY,QAAQ,qBAAqB;AAClD,SASEC,gBAAgB,EAChBC,cAAc,EACdC,mBAAmB,EACnBC,kBAAkB,QACb,aAAa;AAuEpB,OAAO,MAAMC,uBAAuBL;IACzBM,OAAoB,SAAS;IAC7BC,eAAqC;QAC5CC,iBAAiB;YACf;YACA;YACA;YACA;YACA;SACD;QACDC,kBAAkB;YAChB,uBAAuB;YACvB,SAAS;YACT,aAAa;YACb,iBAAiB;YACjB,qBAAqB;QACvB;QACAC,iBAAiB;YACf,uBAAuB;YACvB,SAAS;YACT,aAAa;YACb,iBAAiB;YACjB,qBAAqB;QACvB;QACAC,mBAAmB;QACnBC,yBAAyB;QACzBC,wBAAwB;QACxBC,gBAAgB;QAChBC,eAAe;QACfC,eAAe;QACfC,oBAAoB;QACpBC,oBAAoB;QACpBC,kBAAkB;QAClBC,kBAAkB;QAClBC,WAAW;YACTC,mBAAmB;YACnBC,iBAAiB;YACjBC,oBAAoB;QACtB;QACAC,SAAS;YACP,uBAAuB;gBACrBC,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,SAAS;gBACPF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,aAAa;gBACXF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,iBAAiB;gBACfF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,qBAAqB;gBACnBF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;QACF;IACF,EAAE;IAEMC,QAAgB;IAChBC,QAAgC;IAExC,MAAgBC,eAA8B;QAC5C,IAAI,CAAC,IAAI,CAACC,MAAM,CAACC,MAAM,EAAE;YACvB,MAAM,IAAI9B,oBAAoB,8BAA8B;QAC9D;QAEA,IAAI,CAAC0B,OAAO,GAAG,IAAI,CAACG,MAAM,CAACE,MAAM,IAAI;QACrC,IAAI,CAACJ,OAAO,GAAG;YACb,iBAAiB,CAAC,OAAO,EAAE,IAAI,CAACE,MAAM,CAACC,MAAM,EAAE;YAC/C,gBAAgB;QAClB;QAGA,IAAI,IAAI,CAACD,MAAM,CAACG,eAAe,EAAEC,cAAc;YAC7C,IAAI,CAACN,OAAO,CAAC,sBAAsB,GAAG,IAAI,CAACE,MAAM,CAACG,eAAe,CAACC,YAAY;QAChF;IACF;IAEA,MAAgBC,WAAWC,OAAmB,EAAwB;QACpE,MAAMC,gBAA+B;YACnCC,OAAO,IAAI,CAACC,gBAAgB,CAACH,QAAQE,KAAK,IAAI,IAAI,CAACR,MAAM,CAACQ,KAAK;YAC/DE,UAAUJ,QAAQI,QAAQ,CAACC,GAAG,CAAC,CAACC,MAAS,CAAA;oBACvCC,MAAMD,IAAIC,IAAI;oBACdC,SAASF,IAAIE,OAAO;oBACpB,GAAIF,IAAItC,IAAI,IAAI;wBAAEA,MAAMsC,IAAItC,IAAI;oBAAC,CAAC;oBAClC,GAAIsC,IAAIG,YAAY,IAAI;wBAAEC,eAAeJ,IAAIG,YAAY;oBAAC,CAAC;gBAC7D,CAAA;YACAE,aAAaX,QAAQW,WAAW,IAAI,IAAI,CAACjB,MAAM,CAACiB,WAAW;YAC3DC,YAAYZ,QAAQa,SAAS,IAAI,IAAI,CAACnB,MAAM,CAACmB,SAAS;YACtDC,OAAOd,QAAQe,IAAI,IAAI,IAAI,CAACrB,MAAM,CAACqB,IAAI;YACvCC,mBAAmBhB,QAAQiB,gBAAgB,IAAI,IAAI,CAACvB,MAAM,CAACuB,gBAAgB;YAC3EC,kBAAkBlB,QAAQmB,eAAe,IAAI,IAAI,CAACzB,MAAM,CAACyB,eAAe;YACxEC,MAAMpB,QAAQqB,aAAa,IAAI,IAAI,CAAC3B,MAAM,CAAC2B,aAAa;YACxDC,QAAQ;QACV;QAGA,IAAItB,QAAQuB,SAAS,EAAE;YACrBtB,cAAcsB,SAAS,GAAGvB,QAAQuB,SAAS;YAC3CtB,cAAcS,aAAa,GAAGV,QAAQS,YAAY;QACpD;QAEA,MAAMe,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,IAAI,CAAClC,MAAM,CAACgC,OAAO,IAAI;QAE5E,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACvC,OAAO,CAAC,iBAAiB,CAAC,EAAE;gBAC/DwC,QAAQ;gBACRvC,SAAS,IAAI,CAACA,OAAO;gBACrBwC,MAAMC,KAAKC,SAAS,CAACjC;gBACrBkC,QAAQX,WAAWW,MAAM;YAC3B;YAEAC,aAAaV;YAEb,IAAI,CAACG,SAASQ,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACT;YACjC;YAEA,MAAMU,OAAuB,MAAMV,SAASW,IAAI;YAChD,MAAMC,SAASF,KAAKG,OAAO,CAAC,EAAE;YAG9B,MAAMxC,QAAQF,QAAQE,KAAK,IAAI,IAAI,CAACR,MAAM,CAACQ,KAAK;YAChD,MAAMf,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACe,MAAM;YACjD,MAAMyC,aAAa,AAACJ,KAAKK,KAAK,CAACC,aAAa,GAAG,OAAQ1D,QAAQC,eAAe;YAC9E,MAAM0D,iBAAiB,AAACP,KAAKK,KAAK,CAACG,iBAAiB,GAAG,OAAQ5D,QAAQE,mBAAmB;YAE1F,OAAO;gBACL2D,IAAIT,KAAKS,EAAE;gBACX9C,OAAO,IAAI,CAAC+C,kBAAkB,CAACV,KAAKrC,KAAK;gBACzCgD,UAAU;gBACV1C,SAASiC,OAAOU,OAAO,CAAC3C,OAAO,IAAI;gBACnCC,cAAcgC,OAAOU,OAAO,CAACzC,aAAa;gBAC1CkC,OAAO;oBACLQ,cAAcb,KAAKK,KAAK,CAACC,aAAa;oBACtCQ,kBAAkBd,KAAKK,KAAK,CAACG,iBAAiB;oBAC9CO,aAAaf,KAAKK,KAAK,CAACW,YAAY;gBACtC;gBACAC,MAAM;oBACJb;oBACAG;oBACAW,WAAWd,aAAaG;oBACxBxD,UAAU;gBACZ;gBACAoE,cAAcjB,OAAOkB,aAAa;YACpC;QACF,EAAE,OAAOC,OAAO;YACdxB,aAAaV;YACb,MAAM,IAAI,CAACmC,cAAc,CAACD;QAC5B;IACF;IAEA,OAAiBE,iBAAiB9D,OAAmB,EAAiC;QACpF,MAAMC,gBAA+B;YACnCC,OAAO,IAAI,CAACC,gBAAgB,CAACH,QAAQE,KAAK,IAAI,IAAI,CAACR,MAAM,CAACQ,KAAK;YAC/DE,UAAUJ,QAAQI,QAAQ,CAACC,GAAG,CAAC,CAACC,MAAS,CAAA;oBACvCC,MAAMD,IAAIC,IAAI;oBACdC,SAASF,IAAIE,OAAO;oBACpB,GAAIF,IAAItC,IAAI,IAAI;wBAAEA,MAAMsC,IAAItC,IAAI;oBAAC,CAAC;oBAClC,GAAIsC,IAAIG,YAAY,IAAI;wBAAEC,eAAeJ,IAAIG,YAAY;oBAAC,CAAC;gBAC7D,CAAA;YACAE,aAAaX,QAAQW,WAAW,IAAI,IAAI,CAACjB,MAAM,CAACiB,WAAW;YAC3DC,YAAYZ,QAAQa,SAAS,IAAI,IAAI,CAACnB,MAAM,CAACmB,SAAS;YACtDC,OAAOd,QAAQe,IAAI,IAAI,IAAI,CAACrB,MAAM,CAACqB,IAAI;YACvCC,mBAAmBhB,QAAQiB,gBAAgB,IAAI,IAAI,CAACvB,MAAM,CAACuB,gBAAgB;YAC3EC,kBAAkBlB,QAAQmB,eAAe,IAAI,IAAI,CAACzB,MAAM,CAACyB,eAAe;YACxEC,MAAMpB,QAAQqB,aAAa,IAAI,IAAI,CAAC3B,MAAM,CAAC2B,aAAa;YACxDC,QAAQ;QACV;QAEA,IAAItB,QAAQuB,SAAS,EAAE;YACrBtB,cAAcsB,SAAS,GAAGvB,QAAQuB,SAAS;YAC3CtB,cAAcS,aAAa,GAAGV,QAAQS,YAAY;QACpD;QAEA,MAAMe,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,AAAC,CAAA,IAAI,CAAClC,MAAM,CAACgC,OAAO,IAAI,KAAI,IAAK;QAEtF,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACvC,OAAO,CAAC,iBAAiB,CAAC,EAAE;gBAC/DwC,QAAQ;gBACRvC,SAAS,IAAI,CAACA,OAAO;gBACrBwC,MAAMC,KAAKC,SAAS,CAACjC;gBACrBkC,QAAQX,WAAWW,MAAM;YAC3B;YAEA,IAAI,CAACN,SAASQ,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACT;YACjC;YAEA,MAAMkC,SAASlC,SAASG,IAAI,CAAEgC,SAAS;YACvC,MAAMC,UAAU,IAAIC;YACpB,IAAIC,SAAS;YACb,MAAMC,oBAAoB;YAC1B,MAAMC,wBAAwB;YAE9B,MAAO,KAAM;gBACX,MAAM,EAAEC,IAAI,EAAEC,KAAK,EAAE,GAAG,MAAMR,OAAOS,IAAI;gBACzC,IAAIF,MAAM;gBAEVH,UAAUF,QAAQQ,MAAM,CAACF,OAAO;oBAAEjD,QAAQ;gBAAK;gBAC/C,MAAMoD,QAAQP,OAAOQ,KAAK,CAAC;gBAC3BR,SAASO,MAAME,GAAG,MAAM;gBAExB,KAAK,MAAMC,QAAQH,MAAO;oBACxB,IAAIG,KAAKC,UAAU,CAAC,WAAW;wBAC7B,MAAMvC,OAAOsC,KAAKE,KAAK,CAAC;wBACxB,IAAIxC,SAAS,UAAU;wBAEvB,IAAI;4BACF,MAAMyC,QAA2B/C,KAAKgD,KAAK,CAAC1C;4BAC5C,MAAM2C,QAAQF,MAAMtC,OAAO,CAAC,EAAE,CAACwC,KAAK;4BAEpC,IAAIA,MAAM1E,OAAO,EAAE;gCACjB,MAAM;oCACJ2E,MAAM;oCACND,OAAO;wCAAE1E,SAAS0E,MAAM1E,OAAO;oCAAC;gCAClC;4BACF;4BAEA,IAAI0E,MAAMxE,aAAa,EAAE;gCACvB,MAAM;oCACJyE,MAAM;oCACND,OAAO;wCAAEzE,cAAcyE,MAAMxE,aAAa;oCAAC;gCAC7C;4BACF;4BAEA,IAAIsE,MAAMtC,OAAO,CAAC,EAAE,CAACiB,aAAa,EAAE;gCAElC,MAAMP,eAAe,IAAI,CAACgC,cAAc,CAACnD,KAAKC,SAAS,CAAClC,QAAQI,QAAQ;gCACxE,MAAMiD,mBAAmBgC,KAAKC,GAAG,CAACjB,uBAAuB;gCAEzD,MAAMnE,QAAQF,QAAQE,KAAK,IAAI,IAAI,CAACR,MAAM,CAACQ,KAAK;gCAChD,MAAMf,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACe,MAAM;gCACjD,MAAMyC,aAAa,AAACS,eAAe,OAAQjE,QAAQC,eAAe;gCAClE,MAAM0D,iBAAiB,AAACO,mBAAmB,OAAQlE,QAAQE,mBAAmB;gCAE9E,MAAM;oCACJ8F,MAAM;oCACNvC,OAAO;wCACLQ;wCACAC;wCACAC,aAAaF,eAAeC;oCAC9B;oCACAG,MAAM;wCACJb;wCACAG;wCACAW,WAAWd,aAAaG;wCACxBxD,UAAU;oCACZ;gCACF;4BACF;wBACF,EAAE,OAAOiG,GAAG;4BACV,IAAI,CAACC,MAAM,CAACC,IAAI,CAAC,uCAAuC;gCAAElD;gCAAMqB,OAAO2B;4BAAE;wBAC3E;oBACF;gBACF;YACF;QACF,EAAE,OAAO3B,OAAO;YACdxB,aAAaV;YACb,MAAM,IAAI,CAACmC,cAAc,CAACD;QAC5B,SAAU;YACRxB,aAAaV;QACf;IACF;IAEA,MAAMgE,aAAkC;QACtC,OAAO,IAAI,CAACzH,YAAY,CAACC,eAAe;IAC1C;IAEA,MAAMyH,aAAazF,KAAe,EAAsB;QACtD,OAAO;YACLA;YACAlC,MAAMkC;YACN0F,aAAa,IAAI,CAACC,mBAAmB,CAAC3F;YACtC4F,eAAe,IAAI,CAAC7H,YAAY,CAACE,gBAAgB,CAAC+B,MAAM,IAAI;YAC5D9B,iBAAiB,IAAI,CAACH,YAAY,CAACG,eAAe,CAAC8B,MAAM,IAAI;YAC7D6F,mBAAmB;gBACjB;gBACA;gBACA;mBACI7F,MAAM8F,QAAQ,CAAC,WAAW;oBAAC;iBAAS,GAAG,EAAE;aAC9C;YACD7G,SAAS,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACe,MAAM;QAC5C;IACF;IAEA,MAAgB+F,gBAA4C;QAC1D,IAAI;YACF,MAAMpE,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACvC,OAAO,CAAC,OAAO,CAAC,EAAE;gBACrDC,SAAS,IAAI,CAACA,OAAO;YACvB;YAEA,IAAI,CAACqC,SAASQ,EAAE,EAAE;gBAChB,MAAM,IAAI6D,MAAM,CAAC,qBAAqB,EAAErE,SAASsE,MAAM,EAAE;YAC3D;YAEA,OAAO;gBACLC,SAAS;gBACTC,WAAW,IAAIC;YACjB;QACF,EAAE,OAAO1C,OAAO;YACd,OAAO;gBACLwC,SAAS;gBACTxC,OAAOA,iBAAiBsC,QAAQtC,MAAMT,OAAO,GAAG;gBAChDkD,WAAW,IAAIC;YACjB;QACF;IACF;IAEQnG,iBAAiBD,KAAe,EAAU;QAEhD,MAAMqG,WAAmC;YACvC,uBAAuB;YACvB,SAAS;YACT,aAAa;YACb,iBAAiB;YACjB,qBAAqB;QACvB;QACA,OAAOA,QAAQ,CAACrG,MAAM,IAAIA;IAC5B;IAEQ+C,mBAAmB/C,KAAa,EAAY;QAElD,OAAO,IAAI,CAACjC,YAAY,CAACC,eAAe,CAACsI,IAAI,CAAC,CAACC,IAAMA,MAAMvG,UAAU;IACvE;IAEQ2F,oBAAoB3F,KAAe,EAAU;QACnD,MAAMwG,eAAuC;YAC3C,uBAAuB;YACvB,SAAS;YACT,aAAa;YACb,iBAAiB;YACjB,qBAAqB;QACvB;QACA,OAAOA,YAAY,CAACxG,MAAM,IAAI;IAChC;IAEA,MAAcoC,oBAAoBT,QAAkB,EAAiB;QACnE,MAAM8E,YAAY,MAAM9E,SAAS+E,IAAI;QACrC,IAAIC;QAEJ,IAAI;YACFA,YAAY5E,KAAKgD,KAAK,CAAC0B;QACzB,EAAE,OAAM;YACNE,YAAY;gBAAEjD,OAAO;oBAAET,SAASwD;gBAAU;YAAE;QAC9C;QAEA,MAAMxD,UAAU0D,UAAUjD,KAAK,EAAET,WAAW;QAE5C,OAAQtB,SAASsE,MAAM;YACrB,KAAK;gBACH,MAAM,IAAItI,oBAAoBsF,SAAS,UAAU0D;YACnD,KAAK;gBACH,MAAMC,aAAajF,SAASrC,OAAO,CAACuH,GAAG,CAAC;gBACxC,MAAM,IAAInJ,eACRuF,SACA,UACA2D,aAAaE,SAASF,cAAcG,WACpCJ;YAEJ,KAAK;gBACH,MAAM,IAAI/I,mBAAmB,IAAI,CAAC4B,MAAM,CAACQ,KAAK,EAAE,UAAU2G;YAC5D;gBACE,MAAM,IAAIlJ,iBACRwF,SACA,CAAC,OAAO,EAAEtB,SAASsE,MAAM,EAAE,EAC3B,UACAtE,SAASsE,MAAM,EACftE,SAASsE,MAAM,IAAI,KACnBU;QAEN;IACF;AACF"}