{"version":3,"sources":["../../../src/providers/types.ts"],"sourcesContent":["/**\r\n * Multi-LLM Provider Types and Interfaces\r\n * Unified type system for all LLM providers\r\n */\r\n\r\nimport { EventEmitter } from 'events';\r\n\r\n// ===== PROVIDER TYPES =====\r\n\r\nexport type LLMProvider = \r\n  | 'openai'\r\n  | 'anthropic'\r\n  | 'google'\r\n  | 'cohere'\r\n  | 'ollama'\r\n  | 'llama-cpp'\r\n  | 'custom';\r\n\r\nexport type LLMModel =\r\n  // OpenAI Models\r\n  | 'gpt-4-turbo-preview'\r\n  | 'gpt-4'\r\n  | 'gpt-4-32k'\r\n  | 'gpt-3.5-turbo'\r\n  | 'gpt-3.5-turbo-16k'\r\n  // Anthropic Models\r\n  | 'claude-3-opus-20240229'\r\n  | 'claude-3-sonnet-20240229'\r\n  | 'claude-3-haiku-20240307'\r\n  | 'claude-2.1'\r\n  | 'claude-2.0'\r\n  | 'claude-instant-1.2'\r\n  // Google Models\r\n  | 'gemini-pro'\r\n  | 'gemini-pro-vision'\r\n  | 'palm-2'\r\n  | 'bison'\r\n  // Cohere Models\r\n  | 'command'\r\n  | 'command-light'\r\n  | 'command-nightly'\r\n  | 'generate-xlarge'\r\n  | 'generate-medium'\r\n  // Local Models\r\n  | 'llama-2-7b'\r\n  | 'llama-2-13b'\r\n  | 'llama-2-70b'\r\n  | 'mistral-7b'\r\n  | 'mixtral-8x7b'\r\n  | 'custom-model';\r\n\r\n// ===== BASE INTERFACES =====\r\n\r\nexport interface LLMProviderConfig {\r\n  provider: LLMProvider;\r\n  apiKey?: string;\r\n  apiUrl?: string;\r\n  model: LLMModel;\r\n  \r\n  // Common parameters\r\n  temperature?: number;\r\n  maxTokens?: number;\r\n  topP?: number;\r\n  topK?: number;\r\n  frequencyPenalty?: number;\r\n  presencePenalty?: number;\r\n  stopSequences?: string[];\r\n  \r\n  // Provider-specific settings\r\n  providerOptions?: Record<string, any>;\r\n  \r\n  // Performance settings\r\n  timeout?: number;\r\n  retryAttempts?: number;\r\n  retryDelay?: number;\r\n  \r\n  // Advanced features\r\n  enableStreaming?: boolean;\r\n  enableCaching?: boolean;\r\n  cacheTimeout?: number;\r\n  \r\n  // Cost optimization\r\n  enableCostOptimization?: boolean;\r\n  maxCostPerRequest?: number;\r\n  fallbackModels?: LLMModel[];\r\n}\r\n\r\nexport interface LLMMessage {\r\n  role: 'system' | 'user' | 'assistant' | 'function';\r\n  content: string;\r\n  name?: string; // For function messages\r\n  functionCall?: {\r\n    name: string;\r\n    arguments: string;\r\n  };\r\n}\r\n\r\nexport interface LLMRequest {\r\n  messages: LLMMessage[];\r\n  model?: LLMModel;\r\n  temperature?: number;\r\n  maxTokens?: number;\r\n  topP?: number;\r\n  topK?: number;\r\n  frequencyPenalty?: number;\r\n  presencePenalty?: number;\r\n  stopSequences?: string[];\r\n  stream?: boolean;\r\n  \r\n  // Function calling\r\n  functions?: LLMFunction[];\r\n  functionCall?: 'auto' | 'none' | { name: string };\r\n  \r\n  // Provider-specific options\r\n  providerOptions?: Record<string, any>;\r\n  \r\n  // Cost optimization\r\n  costConstraints?: {\r\n    maxCost?: number;\r\n    preferredModels?: LLMModel[];\r\n  };\r\n}\r\n\r\nexport interface LLMFunction {\r\n  name: string;\r\n  description: string;\r\n  parameters: {\r\n    type: 'object';\r\n    properties: Record<string, any>;\r\n    required?: string[];\r\n  };\r\n}\r\n\r\nexport interface LLMResponse {\r\n  id: string;\r\n  model: LLMModel;\r\n  provider: LLMProvider;\r\n  \r\n  // Content\r\n  content: string;\r\n  functionCall?: {\r\n    name: string;\r\n    arguments: string;\r\n  };\r\n  \r\n  // Metadata\r\n  usage: {\r\n    promptTokens: number;\r\n    completionTokens: number;\r\n    totalTokens: number;\r\n  };\r\n  \r\n  // Cost tracking\r\n  cost?: {\r\n    promptCost: number;\r\n    completionCost: number;\r\n    totalCost: number;\r\n    currency: string;\r\n  };\r\n  \r\n  // Performance metrics\r\n  latency?: number;\r\n  \r\n  // Additional info\r\n  finishReason?: 'stop' | 'length' | 'function_call' | 'content_filter';\r\n  metadata?: Record<string, any>;\r\n}\r\n\r\nexport interface LLMStreamEvent {\r\n  type: 'content' | 'function_call' | 'error' | 'done';\r\n  delta?: {\r\n    content?: string;\r\n    functionCall?: {\r\n      name?: string;\r\n      arguments?: string;\r\n    };\r\n  };\r\n  error?: Error;\r\n  usage?: LLMResponse['usage'];\r\n  cost?: LLMResponse['cost'];\r\n}\r\n\r\n// ===== PROVIDER CAPABILITIES =====\r\n\r\nexport interface ProviderCapabilities {\r\n  // Model features\r\n  supportedModels: LLMModel[];\r\n  maxContextLength: Record<LLMModel, number>;\r\n  maxOutputTokens: Record<LLMModel, number>;\r\n  \r\n  // Feature support\r\n  supportsStreaming: boolean;\r\n  supportsFunctionCalling: boolean;\r\n  supportsSystemMessages: boolean;\r\n  supportsVision: boolean;\r\n  supportsAudio: boolean;\r\n  supportsTools: boolean;\r\n  \r\n  // Advanced features\r\n  supportsFineTuning: boolean;\r\n  supportsEmbeddings: boolean;\r\n  supportsLogprobs: boolean;\r\n  supportsBatching: boolean;\r\n  \r\n  // Constraints\r\n  rateLimit?: {\r\n    requestsPerMinute: number;\r\n    tokensPerMinute: number;\r\n    concurrentRequests: number;\r\n  };\r\n  \r\n  // Cost information\r\n  pricing?: {\r\n    [model: string]: {\r\n      promptCostPer1k: number;\r\n      completionCostPer1k: number;\r\n      currency: string;\r\n    };\r\n  };\r\n}\r\n\r\n// ===== ERROR HANDLING =====\r\n\r\nexport class LLMProviderError extends Error {\r\n  constructor(\r\n    message: string,\r\n    public code: string,\r\n    public provider: LLMProvider,\r\n    public statusCode?: number,\r\n    public retryable: boolean = true,\r\n    public details?: any\r\n  ) {\r\n    super(message);\r\n    this.name = 'LLMProviderError';\r\n  }\r\n}\r\n\r\nexport class RateLimitError extends LLMProviderError {\r\n  constructor(\r\n    message: string,\r\n    provider: LLMProvider,\r\n    public retryAfter?: number,\r\n    details?: any\r\n  ) {\r\n    super(message, 'RATE_LIMIT', provider, 429, true, details);\r\n    this.name = 'RateLimitError';\r\n  }\r\n}\r\n\r\nexport class AuthenticationError extends LLMProviderError {\r\n  constructor(message: string, provider: LLMProvider, details?: any) {\r\n    super(message, 'AUTHENTICATION', provider, 401, false, details);\r\n    this.name = 'AuthenticationError';\r\n  }\r\n}\r\n\r\nexport class ModelNotFoundError extends LLMProviderError {\r\n  constructor(model: string, provider: LLMProvider, details?: any) {\r\n    super(`Model ${model} not found`, 'MODEL_NOT_FOUND', provider, 404, false, details);\r\n    this.name = 'ModelNotFoundError';\r\n  }\r\n}\r\n\r\nexport class ProviderUnavailableError extends LLMProviderError {\r\n  constructor(provider: LLMProvider, details?: any) {\r\n    super(`Provider ${provider} is unavailable`, 'PROVIDER_UNAVAILABLE', provider, 503, true, details);\r\n    this.name = 'ProviderUnavailableError';\r\n  }\r\n}\r\n\r\n// ===== ABSTRACT PROVIDER INTERFACE =====\r\n\r\nexport interface ILLMProvider extends EventEmitter {\r\n  // Properties\r\n  readonly name: LLMProvider;\r\n  readonly capabilities: ProviderCapabilities;\r\n  config: LLMProviderConfig;\r\n  \r\n  // Core methods\r\n  initialize(): Promise<void>;\r\n  complete(request: LLMRequest): Promise<LLMResponse>;\r\n  streamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent>;\r\n  \r\n  // Model management\r\n  listModels(): Promise<LLMModel[]>;\r\n  getModelInfo(model: LLMModel): Promise<ModelInfo>;\r\n  validateModel(model: LLMModel): boolean;\r\n  \r\n  // Health and status\r\n  healthCheck(): Promise<HealthCheckResult>;\r\n  getStatus(): ProviderStatus;\r\n  \r\n  // Cost management\r\n  estimateCost(request: LLMRequest): Promise<CostEstimate>;\r\n  getUsage(period?: UsagePeriod): Promise<UsageStats>;\r\n  \r\n  // Cleanup\r\n  destroy(): void;\r\n}\r\n\r\nexport interface ModelInfo {\r\n  model: LLMModel;\r\n  name: string;\r\n  description: string;\r\n  contextLength: number;\r\n  maxOutputTokens: number;\r\n  supportedFeatures: string[];\r\n  pricing?: {\r\n    promptCostPer1k: number;\r\n    completionCostPer1k: number;\r\n    currency: string;\r\n  };\r\n  deprecated?: boolean;\r\n  deprecationDate?: Date;\r\n  recommendedReplacement?: LLMModel;\r\n}\r\n\r\nexport interface HealthCheckResult {\r\n  healthy: boolean;\r\n  latency?: number;\r\n  error?: string;\r\n  timestamp: Date;\r\n  details?: Record<string, any>;\r\n}\r\n\r\nexport interface ProviderStatus {\r\n  available: boolean;\r\n  currentLoad: number;\r\n  queueLength: number;\r\n  activeRequests: number;\r\n  rateLimitRemaining?: number;\r\n  rateLimitReset?: Date;\r\n}\r\n\r\nexport interface CostEstimate {\r\n  estimatedPromptTokens: number;\r\n  estimatedCompletionTokens: number;\r\n  estimatedTotalTokens: number;\r\n  estimatedCost: {\r\n    prompt: number;\r\n    completion: number;\r\n    total: number;\r\n    currency: string;\r\n  };\r\n  confidence: number; // 0-1\r\n}\r\n\r\nexport interface UsageStats {\r\n  period: {\r\n    start: Date;\r\n    end: Date;\r\n  };\r\n  requests: number;\r\n  tokens: {\r\n    prompt: number;\r\n    completion: number;\r\n    total: number;\r\n  };\r\n  cost: {\r\n    prompt: number;\r\n    completion: number;\r\n    total: number;\r\n    currency: string;\r\n  };\r\n  errors: number;\r\n  averageLatency: number;\r\n  modelBreakdown: Record<LLMModel, {\r\n    requests: number;\r\n    tokens: number;\r\n    cost: number;\r\n  }>;\r\n}\r\n\r\nexport type UsagePeriod = 'hour' | 'day' | 'week' | 'month' | 'all';\r\n\r\n// ===== FALLBACK AND RETRY STRATEGIES =====\r\n\r\nexport interface FallbackStrategy {\r\n  name: string;\r\n  enabled: boolean;\r\n  rules: FallbackRule[];\r\n  maxAttempts: number;\r\n}\r\n\r\nexport interface FallbackRule {\r\n  condition: 'error' | 'rate_limit' | 'timeout' | 'cost' | 'unavailable';\r\n  errorCodes?: string[];\r\n  fallbackProviders: LLMProvider[];\r\n  fallbackModels?: LLMModel[];\r\n  retryOriginal: boolean;\r\n  retryDelay?: number;\r\n}\r\n\r\nexport interface RetryStrategy {\r\n  maxAttempts: number;\r\n  initialDelay: number;\r\n  maxDelay: number;\r\n  backoffMultiplier: number;\r\n  jitter: boolean;\r\n  retryableErrors: string[];\r\n}\r\n\r\n// ===== CACHING INTERFACES =====\r\n\r\nexport interface CacheConfig {\r\n  enabled: boolean;\r\n  ttl: number; // Time to live in seconds\r\n  maxSize: number; // Max cache size in MB\r\n  strategy: 'lru' | 'lfu' | 'ttl';\r\n  keyGenerator?: (request: LLMRequest) => string;\r\n}\r\n\r\nexport interface CacheEntry {\r\n  key: string;\r\n  request: LLMRequest;\r\n  response: LLMResponse;\r\n  timestamp: Date;\r\n  hits: number;\r\n  size: number;\r\n}\r\n\r\n// ===== RATE LIMITING =====\r\n\r\nexport interface RateLimiter {\r\n  checkLimit(provider: LLMProvider, model?: LLMModel): Promise<boolean>;\r\n  consumeToken(provider: LLMProvider, tokens: number): Promise<void>;\r\n  getRemainingTokens(provider: LLMProvider): Promise<number>;\r\n  getResetTime(provider: LLMProvider): Promise<Date | null>;\r\n  waitForCapacity(provider: LLMProvider, tokens: number): Promise<void>;\r\n}\r\n\r\n// ===== LOAD BALANCING =====\r\n\r\nexport interface LoadBalancer {\r\n  selectProvider(request: LLMRequest, availableProviders: ILLMProvider[]): Promise<ILLMProvider>;\r\n  updateProviderMetrics(provider: LLMProvider, metrics: ProviderMetrics): void;\r\n  rebalance(): Promise<void>;\r\n}\r\n\r\nexport interface ProviderMetrics {\r\n  provider: LLMProvider;\r\n  timestamp: Date;\r\n  latency: number;\r\n  errorRate: number;\r\n  successRate: number;\r\n  load: number;\r\n  cost: number;\r\n  availability: number;\r\n}\r\n\r\n// ===== MONITORING AND ANALYTICS =====\r\n\r\nexport interface ProviderMonitor {\r\n  trackRequest(provider: LLMProvider, request: LLMRequest, response: LLMResponse | Error): void;\r\n  getMetrics(provider?: LLMProvider, period?: UsagePeriod): Promise<ProviderMetrics[]>;\r\n  getAlerts(): Alert[];\r\n  setAlertThreshold(metric: string, threshold: number): void;\r\n}\r\n\r\nexport interface Alert {\r\n  id: string;\r\n  timestamp: Date;\r\n  provider: LLMProvider;\r\n  type: 'error_rate' | 'latency' | 'cost' | 'rate_limit' | 'availability';\r\n  severity: 'info' | 'warning' | 'error' | 'critical';\r\n  message: string;\r\n  value: number;\r\n  threshold: number;\r\n}\r\n\r\n// ===== COST OPTIMIZATION =====\r\n\r\nexport interface CostOptimizer {\r\n  selectOptimalModel(request: LLMRequest, constraints: CostConstraints): Promise<OptimizationResult>;\r\n  analyzeCostTrends(period: UsagePeriod): Promise<CostAnalysis>;\r\n  suggestOptimizations(): Promise<OptimizationSuggestion[]>;\r\n}\r\n\r\nexport interface CostConstraints {\r\n  maxCostPerRequest?: number;\r\n  maxCostPerToken?: number;\r\n  preferredProviders?: LLMProvider[];\r\n  requiredFeatures?: string[];\r\n  minQuality?: number; // 0-1\r\n}\r\n\r\nexport interface OptimizationResult {\r\n  provider: LLMProvider;\r\n  model: LLMModel;\r\n  estimatedCost: number;\r\n  estimatedQuality: number; // 0-1\r\n  reasoning: string;\r\n}\r\n\r\nexport interface CostAnalysis {\r\n  period: UsagePeriod;\r\n  totalCost: number;\r\n  costByProvider: Record<LLMProvider, number>;\r\n  costByModel: Record<LLMModel, number>;\r\n  trends: {\r\n    dailyAverage: number;\r\n    weeklyGrowth: number;\r\n    projection30Days: number;\r\n  };\r\n}\r\n\r\nexport interface OptimizationSuggestion {\r\n  type: 'model_switch' | 'provider_switch' | 'parameter_tuning' | 'caching' | 'batching';\r\n  description: string;\r\n  estimatedSavings: number;\r\n  implementation: string;\r\n  impact: 'low' | 'medium' | 'high';\r\n}\r\n\r\n// ===== TYPE GUARDS =====\r\n\r\nexport function isLLMResponse(obj: any): obj is LLMResponse {\r\n  return obj && typeof obj.id === 'string' && typeof obj.content === 'string';\r\n}\r\n\r\nexport function isLLMStreamEvent(obj: any): obj is LLMStreamEvent {\r\n  return obj && typeof obj.type === 'string';\r\n}\r\n\r\nexport function isLLMProviderError(error: any): error is LLMProviderError {\r\n  return error instanceof LLMProviderError;\r\n}\r\n\r\nexport function isRateLimitError(error: any): error is RateLimitError {\r\n  return error instanceof RateLimitError;\r\n}"],"names":["LLMProviderError","Error","message","code","provider","statusCode","retryable","details","name","RateLimitError","retryAfter","AuthenticationError","ModelNotFoundError","model","ProviderUnavailableError","isLLMResponse","obj","id","content","isLLMStreamEvent","type","isLLMProviderError","error","isRateLimitError"],"mappings":"AA+NA,OAAO,MAAMA,yBAAyBC;;;;;;IACpC,YACEC,OAAe,EACf,AAAOC,IAAY,EACnB,AAAOC,QAAqB,EAC5B,AAAOC,UAAmB,EAC1B,AAAOC,YAAqB,IAAI,EAChC,AAAOC,OAAa,CACpB;QACA,KAAK,CAACL,eANCC,OAAAA,WACAC,WAAAA,eACAC,aAAAA,iBACAC,YAAAA,gBACAC,UAAAA;QAGP,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMC,uBAAuBT;;IAClC,YACEE,OAAe,EACfE,QAAqB,EACrB,AAAOM,UAAmB,EAC1BH,OAAa,CACb;QACA,KAAK,CAACL,SAAS,cAAcE,UAAU,KAAK,MAAMG,eAH3CG,aAAAA;QAIP,IAAI,CAACF,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMG,4BAA4BX;IACvC,YAAYE,OAAe,EAAEE,QAAqB,EAAEG,OAAa,CAAE;QACjE,KAAK,CAACL,SAAS,kBAAkBE,UAAU,KAAK,OAAOG;QACvD,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMI,2BAA2BZ;IACtC,YAAYa,KAAa,EAAET,QAAqB,EAAEG,OAAa,CAAE;QAC/D,KAAK,CAAC,CAAC,MAAM,EAAEM,MAAM,UAAU,CAAC,EAAE,mBAAmBT,UAAU,KAAK,OAAOG;QAC3E,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAEA,OAAO,MAAMM,iCAAiCd;IAC5C,YAAYI,QAAqB,EAAEG,OAAa,CAAE;QAChD,KAAK,CAAC,CAAC,SAAS,EAAEH,SAAS,eAAe,CAAC,EAAE,wBAAwBA,UAAU,KAAK,MAAMG;QAC1F,IAAI,CAACC,IAAI,GAAG;IACd;AACF;AAwPA,OAAO,SAASO,cAAcC,GAAQ;IACpC,OAAOA,OAAO,OAAOA,IAAIC,EAAE,KAAK,YAAY,OAAOD,IAAIE,OAAO,KAAK;AACrE;AAEA,OAAO,SAASC,iBAAiBH,GAAQ;IACvC,OAAOA,OAAO,OAAOA,IAAII,IAAI,KAAK;AACpC;AAEA,OAAO,SAASC,mBAAmBC,KAAU;IAC3C,OAAOA,iBAAiBtB;AAC1B;AAEA,OAAO,SAASuB,iBAAiBD,KAAU;IACzC,OAAOA,iBAAiBb;AAC1B"}