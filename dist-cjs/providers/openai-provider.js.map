{
  "version": 3,
  "sources": ["../../src/providers/openai-provider.ts"],
  "sourcesContent": ["/**\n * OpenAI Provider Implementation\n * Supports GPT-4, GPT-3.5, and other OpenAI models\n */\n\nimport { BaseProvider } from './base-provider.js';\nimport {\n  LLMProvider,\n  LLMModel,\n  LLMRequest,\n  LLMResponse,\n  LLMStreamEvent,\n  ModelInfo,\n  ProviderCapabilities,\n  HealthCheckResult,\n  LLMProviderError,\n  RateLimitError,\n  AuthenticationError,\n  ModelNotFoundError,\n} from './types.js';\n\ninterface OpenAIRequest {\n  model: string;\n  messages: Array<{\n    role: 'system' | 'user' | 'assistant' | 'function';\n    content: string;\n    name?: string;\n    function_call?: {\n      name: string;\n      arguments: string;\n    };\n  }>;\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  frequency_penalty?: number;\n  presence_penalty?: number;\n  stop?: string[];\n  stream?: boolean;\n  functions?: Array<{\n    name: string;\n    description: string;\n    parameters: any;\n  }>;\n  function_call?: 'auto' | 'none' | { name: string };\n}\n\ninterface OpenAIResponse {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    message: {\n      role: string;\n      content: string | null;\n      function_call?: {\n        name: string;\n        arguments: string;\n      };\n    };\n    finish_reason: 'stop' | 'length' | 'function_call' | 'content_filter';\n  }>;\n  usage: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n  };\n}\n\ninterface OpenAIStreamChunk {\n  id: string;\n  object: string;\n  created: number;\n  model: string;\n  choices: Array<{\n    index: number;\n    delta: {\n      role?: string;\n      content?: string;\n      function_call?: {\n        name?: string;\n        arguments?: string;\n      };\n    };\n    finish_reason?: string;\n  }>;\n}\n\nexport class OpenAIProvider extends BaseProvider {\n  readonly name: LLMProvider = 'openai';\n  readonly capabilities: ProviderCapabilities = {\n    supportedModels: [\n      'gpt-4-turbo-preview',\n      'gpt-4',\n      'gpt-4-32k',\n      'gpt-3.5-turbo',\n      'gpt-3.5-turbo-16k',\n    ],\n    maxContextLength: {\n      'gpt-4-turbo-preview': 128000,\n      'gpt-4': 8192,\n      'gpt-4-32k': 32768,\n      'gpt-3.5-turbo': 4096,\n      'gpt-3.5-turbo-16k': 16384,\n    } as Record<LLMModel, number>,\n    maxOutputTokens: {\n      'gpt-4-turbo-preview': 4096,\n      'gpt-4': 4096,\n      'gpt-4-32k': 4096,\n      'gpt-3.5-turbo': 4096,\n      'gpt-3.5-turbo-16k': 4096,\n    } as Record<LLMModel, number>,\n    supportsStreaming: true,\n    supportsFunctionCalling: true,\n    supportsSystemMessages: true,\n    supportsVision: true, // GPT-4 with vision\n    supportsAudio: false,\n    supportsTools: true,\n    supportsFineTuning: true,\n    supportsEmbeddings: true,\n    supportsLogprobs: true,\n    supportsBatching: true,\n    rateLimit: {\n      requestsPerMinute: 3500,\n      tokensPerMinute: 90000,\n      concurrentRequests: 100,\n    },\n    pricing: {\n      'gpt-4-turbo-preview': {\n        promptCostPer1k: 0.01,\n        completionCostPer1k: 0.03,\n        currency: 'USD',\n      },\n      'gpt-4': {\n        promptCostPer1k: 0.03,\n        completionCostPer1k: 0.06,\n        currency: 'USD',\n      },\n      'gpt-4-32k': {\n        promptCostPer1k: 0.06,\n        completionCostPer1k: 0.12,\n        currency: 'USD',\n      },\n      'gpt-3.5-turbo': {\n        promptCostPer1k: 0.0005,\n        completionCostPer1k: 0.0015,\n        currency: 'USD',\n      },\n      'gpt-3.5-turbo-16k': {\n        promptCostPer1k: 0.003,\n        completionCostPer1k: 0.004,\n        currency: 'USD',\n      },\n    },\n  };\n\n  private baseUrl: string;\n  private headers: Record<string, string>;\n\n  protected async doInitialize(): Promise<void> {\n    if (!this.config.apiKey) {\n      throw new AuthenticationError('OpenAI API key is required', 'openai');\n    }\n\n    this.baseUrl = this.config.apiUrl || 'https://api.openai.com/v1';\n    this.headers = {\n      'Authorization': `Bearer ${this.config.apiKey}`,\n      'Content-Type': 'application/json',\n    };\n\n    // Add organization header if provided\n    if (this.config.providerOptions?.organization) {\n      this.headers['OpenAI-Organization'] = this.config.providerOptions.organization;\n    }\n  }\n\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\n    const openAIRequest: OpenAIRequest = {\n      model: this.mapToOpenAIModel(request.model || this.config.model),\n      messages: request.messages.map((msg) => ({\n        role: msg.role,\n        content: msg.content,\n        ...(msg.name && { name: msg.name }),\n        ...(msg.functionCall && { function_call: msg.functionCall }),\n      })),\n      temperature: request.temperature ?? this.config.temperature,\n      max_tokens: request.maxTokens ?? this.config.maxTokens,\n      top_p: request.topP ?? this.config.topP,\n      frequency_penalty: request.frequencyPenalty ?? this.config.frequencyPenalty,\n      presence_penalty: request.presencePenalty ?? this.config.presencePenalty,\n      stop: request.stopSequences ?? this.config.stopSequences,\n      stream: false,\n    };\n\n    // Add function calling if present\n    if (request.functions) {\n      openAIRequest.functions = request.functions;\n      openAIRequest.function_call = request.functionCall;\n    }\n\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 60000);\n\n    try {\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\n        method: 'POST',\n        headers: this.headers,\n        body: JSON.stringify(openAIRequest),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeout);\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const data: OpenAIResponse = await response.json();\n      const choice = data.choices[0];\n\n      // Calculate cost\n      const model = request.model || this.config.model;\n      const pricing = this.capabilities.pricing![model];\n      const promptCost = (data.usage.prompt_tokens / 1000) * pricing.promptCostPer1k;\n      const completionCost = (data.usage.completion_tokens / 1000) * pricing.completionCostPer1k;\n\n      return {\n        id: data.id,\n        model: this.mapFromOpenAIModel(data.model),\n        provider: 'openai',\n        content: choice.message.content || '',\n        functionCall: choice.message.function_call,\n        usage: {\n          promptTokens: data.usage.prompt_tokens,\n          completionTokens: data.usage.completion_tokens,\n          totalTokens: data.usage.total_tokens,\n        },\n        cost: {\n          promptCost,\n          completionCost,\n          totalCost: promptCost + completionCost,\n          currency: 'USD',\n        },\n        finishReason: choice.finish_reason,\n      };\n    } catch (error) {\n      clearTimeout(timeout);\n      throw this.transformError(error);\n    }\n  }\n\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\n    const openAIRequest: OpenAIRequest = {\n      model: this.mapToOpenAIModel(request.model || this.config.model),\n      messages: request.messages.map((msg) => ({\n        role: msg.role,\n        content: msg.content,\n        ...(msg.name && { name: msg.name }),\n        ...(msg.functionCall && { function_call: msg.functionCall }),\n      })),\n      temperature: request.temperature ?? this.config.temperature,\n      max_tokens: request.maxTokens ?? this.config.maxTokens,\n      top_p: request.topP ?? this.config.topP,\n      frequency_penalty: request.frequencyPenalty ?? this.config.frequencyPenalty,\n      presence_penalty: request.presencePenalty ?? this.config.presencePenalty,\n      stop: request.stopSequences ?? this.config.stopSequences,\n      stream: true,\n    };\n\n    if (request.functions) {\n      openAIRequest.functions = request.functions;\n      openAIRequest.function_call = request.functionCall;\n    }\n\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 60000) * 2);\n\n    try {\n      const response = await fetch(`${this.baseUrl}/chat/completions`, {\n        method: 'POST',\n        headers: this.headers,\n        body: JSON.stringify(openAIRequest),\n        signal: controller.signal,\n      });\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n      let totalPromptTokens = 0;\n      let totalCompletionTokens = 0;\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6);\n            if (data === '[DONE]') continue;\n\n            try {\n              const chunk: OpenAIStreamChunk = JSON.parse(data);\n              const delta = chunk.choices[0].delta;\n\n              if (delta.content) {\n                yield {\n                  type: 'content',\n                  delta: { content: delta.content },\n                };\n              }\n\n              if (delta.function_call) {\n                yield {\n                  type: 'function_call',\n                  delta: { functionCall: delta.function_call },\n                };\n              }\n\n              if (chunk.choices[0].finish_reason) {\n                // Estimate tokens for streaming\n                const promptTokens = this.estimateTokens(JSON.stringify(request.messages));\n                const completionTokens = Math.max(totalCompletionTokens, 100); // Minimum estimate\n\n                const model = request.model || this.config.model;\n                const pricing = this.capabilities.pricing![model];\n                const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\n                const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\n\n                yield {\n                  type: 'done',\n                  usage: {\n                    promptTokens,\n                    completionTokens,\n                    totalTokens: promptTokens + completionTokens,\n                  },\n                  cost: {\n                    promptCost,\n                    completionCost,\n                    totalCost: promptCost + completionCost,\n                    currency: 'USD',\n                  },\n                };\n              }\n            } catch (e) {\n              this.logger.warn('Failed to parse OpenAI stream chunk', { data, error: e });\n            }\n          }\n        }\n      }\n    } catch (error) {\n      clearTimeout(timeout);\n      throw this.transformError(error);\n    } finally {\n      clearTimeout(timeout);\n    }\n  }\n\n  async listModels(): Promise<LLMModel[]> {\n    return this.capabilities.supportedModels;\n  }\n\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\n    return {\n      model,\n      name: model,\n      description: this.getModelDescription(model),\n      contextLength: this.capabilities.maxContextLength[model] || 4096,\n      maxOutputTokens: this.capabilities.maxOutputTokens[model] || 4096,\n      supportedFeatures: [\n        'chat',\n        'completion',\n        'function_calling',\n        ...(model.includes('gpt-4') ? ['vision'] : []),\n      ],\n      pricing: this.capabilities.pricing![model],\n    };\n  }\n\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\n    try {\n      const response = await fetch(`${this.baseUrl}/models`, {\n        headers: this.headers,\n      });\n\n      if (!response.ok) {\n        throw new Error(`Health check failed: ${response.status}`);\n      }\n\n      return {\n        healthy: true,\n        timestamp: new Date(),\n      };\n    } catch (error) {\n      return {\n        healthy: false,\n        error: error instanceof Error ? error.message : 'Unknown error',\n        timestamp: new Date(),\n      };\n    }\n  }\n\n  private mapToOpenAIModel(model: LLMModel): string {\n    // Map our model names to OpenAI model names if needed\n    const modelMap: Record<string, string> = {\n      'gpt-4-turbo-preview': 'gpt-4-turbo-preview',\n      'gpt-4': 'gpt-4',\n      'gpt-4-32k': 'gpt-4-32k',\n      'gpt-3.5-turbo': 'gpt-3.5-turbo',\n      'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\n    };\n    return modelMap[model] || model;\n  }\n\n  private mapFromOpenAIModel(model: string): LLMModel {\n    // Ensure the model is in our supported list\n    return this.capabilities.supportedModels.find((m) => m === model) || 'gpt-3.5-turbo';\n  }\n\n  private getModelDescription(model: LLMModel): string {\n    const descriptions: Record<string, string> = {\n      'gpt-4-turbo-preview': 'Latest GPT-4 Turbo model with improved performance',\n      'gpt-4': 'Most capable GPT-4 model for complex tasks',\n      'gpt-4-32k': 'GPT-4 with extended 32k context window',\n      'gpt-3.5-turbo': 'Fast and efficient model for most tasks',\n      'gpt-3.5-turbo-16k': 'GPT-3.5 Turbo with extended context',\n    };\n    return descriptions[model] || 'OpenAI language model';\n  }\n\n  private async handleErrorResponse(response: Response): Promise<void> {\n    const errorText = await response.text();\n    let errorData: any;\n\n    try {\n      errorData = JSON.parse(errorText);\n    } catch {\n      errorData = { error: { message: errorText } };\n    }\n\n    const message = errorData.error?.message || 'Unknown error';\n\n    switch (response.status) {\n      case 401:\n        throw new AuthenticationError(message, 'openai', errorData);\n      case 429:\n        const retryAfter = response.headers.get('retry-after');\n        throw new RateLimitError(\n          message,\n          'openai',\n          retryAfter ? parseInt(retryAfter) : undefined,\n          errorData\n        );\n      case 404:\n        throw new ModelNotFoundError(this.config.model, 'openai', errorData);\n      default:\n        throw new LLMProviderError(\n          message,\n          `OPENAI_${response.status}`,\n          'openai',\n          response.status,\n          response.status >= 500,\n          errorData\n        );\n    }\n  }\n}"],
  "mappings": ";;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA,2BAA6B;AAC7B,mBAaO;AAuEA,MAAM,uBAAuB,kCAAa;AAAA,EA1FjD,OA0FiD;AAAA;AAAA;AAAA,EACtC,OAAoB;AAAA,EACpB,eAAqC;AAAA,IAC5C,iBAAiB;AAAA,MACf;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,kBAAkB;AAAA,MAChB,uBAAuB;AAAA,MACvB,SAAS;AAAA,MACT,aAAa;AAAA,MACb,iBAAiB;AAAA,MACjB,qBAAqB;AAAA,IACvB;AAAA,IACA,iBAAiB;AAAA,MACf,uBAAuB;AAAA,MACvB,SAAS;AAAA,MACT,aAAa;AAAA,MACb,iBAAiB;AAAA,MACjB,qBAAqB;AAAA,IACvB;AAAA,IACA,mBAAmB;AAAA,IACnB,yBAAyB;AAAA,IACzB,wBAAwB;AAAA,IACxB,gBAAgB;AAAA;AAAA,IAChB,eAAe;AAAA,IACf,eAAe;AAAA,IACf,oBAAoB;AAAA,IACpB,oBAAoB;AAAA,IACpB,kBAAkB;AAAA,IAClB,kBAAkB;AAAA,IAClB,WAAW;AAAA,MACT,mBAAmB;AAAA,MACnB,iBAAiB;AAAA,MACjB,oBAAoB;AAAA,IACtB;AAAA,IACA,SAAS;AAAA,MACP,uBAAuB;AAAA,QACrB,iBAAiB;AAAA,QACjB,qBAAqB;AAAA,QACrB,UAAU;AAAA,MACZ;AAAA,MACA,SAAS;AAAA,QACP,iBAAiB;AAAA,QACjB,qBAAqB;AAAA,QACrB,UAAU;AAAA,MACZ;AAAA,MACA,aAAa;AAAA,QACX,iBAAiB;AAAA,QACjB,qBAAqB;AAAA,QACrB,UAAU;AAAA,MACZ;AAAA,MACA,iBAAiB;AAAA,QACf,iBAAiB;AAAA,QACjB,qBAAqB;AAAA,QACrB,UAAU;AAAA,MACZ;AAAA,MACA,qBAAqB;AAAA,QACnB,iBAAiB;AAAA,QACjB,qBAAqB;AAAA,QACrB,UAAU;AAAA,MACZ;AAAA,IACF;AAAA,EACF;AAAA,EAEQ;AAAA,EACA;AAAA,EAER,MAAgB,eAA8B;AAC5C,QAAI,CAAC,KAAK,OAAO,QAAQ;AACvB,YAAM,IAAI,iCAAoB,8BAA8B,QAAQ;AAAA,IACtE;AAEA,SAAK,UAAU,KAAK,OAAO,UAAU;AACrC,SAAK,UAAU;AAAA,MACb,iBAAiB,UAAU,KAAK,OAAO,MAAM;AAAA,MAC7C,gBAAgB;AAAA,IAClB;AAGA,QAAI,KAAK,OAAO,iBAAiB,cAAc;AAC7C,WAAK,QAAQ,qBAAqB,IAAI,KAAK,OAAO,gBAAgB;AAAA,IACpE;AAAA,EACF;AAAA,EAEA,MAAgB,WAAW,SAA2C;AACpE,UAAM,gBAA+B;AAAA,MACnC,OAAO,KAAK,iBAAiB,QAAQ,SAAS,KAAK,OAAO,KAAK;AAAA,MAC/D,UAAU,QAAQ,SAAS,IAAI,CAAC,SAAS;AAAA,QACvC,MAAM,IAAI;AAAA,QACV,SAAS,IAAI;AAAA,QACb,GAAI,IAAI,QAAQ,EAAE,MAAM,IAAI,KAAK;AAAA,QACjC,GAAI,IAAI,gBAAgB,EAAE,eAAe,IAAI,aAAa;AAAA,MAC5D,EAAE;AAAA,MACF,aAAa,QAAQ,eAAe,KAAK,OAAO;AAAA,MAChD,YAAY,QAAQ,aAAa,KAAK,OAAO;AAAA,MAC7C,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,MACnC,mBAAmB,QAAQ,oBAAoB,KAAK,OAAO;AAAA,MAC3D,kBAAkB,QAAQ,mBAAmB,KAAK,OAAO;AAAA,MACzD,MAAM,QAAQ,iBAAiB,KAAK,OAAO;AAAA,MAC3C,QAAQ;AAAA,IACV;AAGA,QAAI,QAAQ,WAAW;AACrB,oBAAc,YAAY,QAAQ;AAClC,oBAAc,gBAAgB,QAAQ;AAAA,IACxC;AAEA,UAAM,aAAa,IAAI,gBAAgB;AACvC,UAAM,UAAU,WAAW,MAAM,WAAW,MAAM,GAAG,KAAK,OAAO,WAAW,GAAK;AAEjF,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,qBAAqB;AAAA,QAC/D,QAAQ;AAAA,QACR,SAAS,KAAK;AAAA,QACd,MAAM,KAAK,UAAU,aAAa;AAAA,QAClC,QAAQ,WAAW;AAAA,MACrB,CAAC;AAED,mBAAa,OAAO;AAEpB,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,KAAK,oBAAoB,QAAQ;AAAA,MACzC;AAEA,YAAM,OAAuB,MAAM,SAAS,KAAK;AACjD,YAAM,SAAS,KAAK,QAAQ,CAAC;AAG7B,YAAM,QAAQ,QAAQ,SAAS,KAAK,OAAO;AAC3C,YAAM,UAAU,KAAK,aAAa,QAAS,KAAK;AAChD,YAAM,aAAc,KAAK,MAAM,gBAAgB,MAAQ,QAAQ;AAC/D,YAAM,iBAAkB,KAAK,MAAM,oBAAoB,MAAQ,QAAQ;AAEvE,aAAO;AAAA,QACL,IAAI,KAAK;AAAA,QACT,OAAO,KAAK,mBAAmB,KAAK,KAAK;AAAA,QACzC,UAAU;AAAA,QACV,SAAS,OAAO,QAAQ,WAAW;AAAA,QACnC,cAAc,OAAO,QAAQ;AAAA,QAC7B,OAAO;AAAA,UACL,cAAc,KAAK,MAAM;AAAA,UACzB,kBAAkB,KAAK,MAAM;AAAA,UAC7B,aAAa,KAAK,MAAM;AAAA,QAC1B;AAAA,QACA,MAAM;AAAA,UACJ;AAAA,UACA;AAAA,UACA,WAAW,aAAa;AAAA,UACxB,UAAU;AAAA,QACZ;AAAA,QACA,cAAc,OAAO;AAAA,MACvB;AAAA,IACF,SAAS,OAAO;AACd,mBAAa,OAAO;AACpB,YAAM,KAAK,eAAe,KAAK;AAAA,IACjC;AAAA,EACF;AAAA,EAEA,OAAiB,iBAAiB,SAAoD;AACpF,UAAM,gBAA+B;AAAA,MACnC,OAAO,KAAK,iBAAiB,QAAQ,SAAS,KAAK,OAAO,KAAK;AAAA,MAC/D,UAAU,QAAQ,SAAS,IAAI,CAAC,SAAS;AAAA,QACvC,MAAM,IAAI;AAAA,QACV,SAAS,IAAI;AAAA,QACb,GAAI,IAAI,QAAQ,EAAE,MAAM,IAAI,KAAK;AAAA,QACjC,GAAI,IAAI,gBAAgB,EAAE,eAAe,IAAI,aAAa;AAAA,MAC5D,EAAE;AAAA,MACF,aAAa,QAAQ,eAAe,KAAK,OAAO;AAAA,MAChD,YAAY,QAAQ,aAAa,KAAK,OAAO;AAAA,MAC7C,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,MACnC,mBAAmB,QAAQ,oBAAoB,KAAK,OAAO;AAAA,MAC3D,kBAAkB,QAAQ,mBAAmB,KAAK,OAAO;AAAA,MACzD,MAAM,QAAQ,iBAAiB,KAAK,OAAO;AAAA,MAC3C,QAAQ;AAAA,IACV;AAEA,QAAI,QAAQ,WAAW;AACrB,oBAAc,YAAY,QAAQ;AAClC,oBAAc,gBAAgB,QAAQ;AAAA,IACxC;AAEA,UAAM,aAAa,IAAI,gBAAgB;AACvC,UAAM,UAAU,WAAW,MAAM,WAAW,MAAM,IAAI,KAAK,OAAO,WAAW,OAAS,CAAC;AAEvF,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,qBAAqB;AAAA,QAC/D,QAAQ;AAAA,QACR,SAAS,KAAK;AAAA,QACd,MAAM,KAAK,UAAU,aAAa;AAAA,QAClC,QAAQ,WAAW;AAAA,MACrB,CAAC;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,KAAK,oBAAoB,QAAQ;AAAA,MACzC;AAEA,YAAM,SAAS,SAAS,KAAM,UAAU;AACxC,YAAM,UAAU,IAAI,YAAY;AAChC,UAAI,SAAS;AACb,UAAI,oBAAoB;AACxB,UAAI,wBAAwB;AAE5B,aAAO,MAAM;AACX,cAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,YAAI;AAAM;AAEV,kBAAU,QAAQ,OAAO,OAAO,EAAE,QAAQ,KAAK,CAAC;AAChD,cAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,iBAAS,MAAM,IAAI,KAAK;AAExB,mBAAW,QAAQ,OAAO;AACxB,cAAI,KAAK,WAAW,QAAQ,GAAG;AAC7B,kBAAM,OAAO,KAAK,MAAM,CAAC;AACzB,gBAAI,SAAS;AAAU;AAEvB,gBAAI;AACF,oBAAM,QAA2B,KAAK,MAAM,IAAI;AAChD,oBAAM,QAAQ,MAAM,QAAQ,CAAC,EAAE;AAE/B,kBAAI,MAAM,SAAS;AACjB,sBAAM;AAAA,kBACJ,MAAM;AAAA,kBACN,OAAO,EAAE,SAAS,MAAM,QAAQ;AAAA,gBAClC;AAAA,cACF;AAEA,kBAAI,MAAM,eAAe;AACvB,sBAAM;AAAA,kBACJ,MAAM;AAAA,kBACN,OAAO,EAAE,cAAc,MAAM,cAAc;AAAA,gBAC7C;AAAA,cACF;AAEA,kBAAI,MAAM,QAAQ,CAAC,EAAE,eAAe;AAElC,sBAAM,eAAe,KAAK,eAAe,KAAK,UAAU,QAAQ,QAAQ,CAAC;AACzE,sBAAM,mBAAmB,KAAK,IAAI,uBAAuB,GAAG;AAE5D,sBAAM,QAAQ,QAAQ,SAAS,KAAK,OAAO;AAC3C,sBAAM,UAAU,KAAK,aAAa,QAAS,KAAK;AAChD,sBAAM,aAAc,eAAe,MAAQ,QAAQ;AACnD,sBAAM,iBAAkB,mBAAmB,MAAQ,QAAQ;AAE3D,sBAAM;AAAA,kBACJ,MAAM;AAAA,kBACN,OAAO;AAAA,oBACL;AAAA,oBACA;AAAA,oBACA,aAAa,eAAe;AAAA,kBAC9B;AAAA,kBACA,MAAM;AAAA,oBACJ;AAAA,oBACA;AAAA,oBACA,WAAW,aAAa;AAAA,oBACxB,UAAU;AAAA,kBACZ;AAAA,gBACF;AAAA,cACF;AAAA,YACF,SAAS,GAAG;AACV,mBAAK,OAAO,KAAK,uCAAuC,EAAE,MAAM,OAAO,EAAE,CAAC;AAAA,YAC5E;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,mBAAa,OAAO;AACpB,YAAM,KAAK,eAAe,KAAK;AAAA,IACjC,UAAE;AACA,mBAAa,OAAO;AAAA,IACtB;AAAA,EACF;AAAA,EAEA,MAAM,aAAkC;AACtC,WAAO,KAAK,aAAa;AAAA,EAC3B;AAAA,EAEA,MAAM,aAAa,OAAqC;AACtD,WAAO;AAAA,MACL;AAAA,MACA,MAAM;AAAA,MACN,aAAa,KAAK,oBAAoB,KAAK;AAAA,MAC3C,eAAe,KAAK,aAAa,iBAAiB,KAAK,KAAK;AAAA,MAC5D,iBAAiB,KAAK,aAAa,gBAAgB,KAAK,KAAK;AAAA,MAC7D,mBAAmB;AAAA,QACjB;AAAA,QACA;AAAA,QACA;AAAA,QACA,GAAI,MAAM,SAAS,OAAO,IAAI,CAAC,QAAQ,IAAI,CAAC;AAAA,MAC9C;AAAA,MACA,SAAS,KAAK,aAAa,QAAS,KAAK;AAAA,IAC3C;AAAA,EACF;AAAA,EAEA,MAAgB,gBAA4C;AAC1D,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,WAAW;AAAA,QACrD,SAAS,KAAK;AAAA,MAChB,CAAC;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI,MAAM,wBAAwB,SAAS,MAAM,EAAE;AAAA,MAC3D;AAEA,aAAO;AAAA,QACL,SAAS;AAAA,QACT,WAAW,oBAAI,KAAK;AAAA,MACtB;AAAA,IACF,SAAS,OAAO;AACd,aAAO;AAAA,QACL,SAAS;AAAA,QACT,OAAO,iBAAiB,QAAQ,MAAM,UAAU;AAAA,QAChD,WAAW,oBAAI,KAAK;AAAA,MACtB;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,iBAAiB,OAAyB;AAEhD,UAAM,WAAmC;AAAA,MACvC,uBAAuB;AAAA,MACvB,SAAS;AAAA,MACT,aAAa;AAAA,MACb,iBAAiB;AAAA,MACjB,qBAAqB;AAAA,IACvB;AACA,WAAO,SAAS,KAAK,KAAK;AAAA,EAC5B;AAAA,EAEQ,mBAAmB,OAAyB;AAElD,WAAO,KAAK,aAAa,gBAAgB,KAAK,CAAC,MAAM,MAAM,KAAK,KAAK;AAAA,EACvE;AAAA,EAEQ,oBAAoB,OAAyB;AACnD,UAAM,eAAuC;AAAA,MAC3C,uBAAuB;AAAA,MACvB,SAAS;AAAA,MACT,aAAa;AAAA,MACb,iBAAiB;AAAA,MACjB,qBAAqB;AAAA,IACvB;AACA,WAAO,aAAa,KAAK,KAAK;AAAA,EAChC;AAAA,EAEA,MAAc,oBAAoB,UAAmC;AACnE,UAAM,YAAY,MAAM,SAAS,KAAK;AACtC,QAAI;AAEJ,QAAI;AACF,kBAAY,KAAK,MAAM,SAAS;AAAA,IAClC,QAAQ;AACN,kBAAY,EAAE,OAAO,EAAE,SAAS,UAAU,EAAE;AAAA,IAC9C;AAEA,UAAM,UAAU,UAAU,OAAO,WAAW;AAE5C,YAAQ,SAAS,QAAQ;AAAA,MACvB,KAAK;AACH,cAAM,IAAI,iCAAoB,SAAS,UAAU,SAAS;AAAA,MAC5D,KAAK;AACH,cAAM,aAAa,SAAS,QAAQ,IAAI,aAAa;AACrD,cAAM,IAAI;AAAA,UACR;AAAA,UACA;AAAA,UACA,aAAa,SAAS,UAAU,IAAI;AAAA,UACpC;AAAA,QACF;AAAA,MACF,KAAK;AACH,cAAM,IAAI,gCAAmB,KAAK,OAAO,OAAO,UAAU,SAAS;AAAA,MACrE;AACE,cAAM,IAAI;AAAA,UACR;AAAA,UACA,UAAU,SAAS,MAAM;AAAA,UACzB;AAAA,UACA,SAAS;AAAA,UACT,SAAS,UAAU;AAAA,UACnB;AAAA,QACF;AAAA,IACJ;AAAA,EACF;AACF;",
  "names": []
}
