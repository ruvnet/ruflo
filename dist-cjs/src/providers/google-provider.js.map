{"version":3,"sources":["../../../src/providers/google-provider.ts"],"sourcesContent":["/**\r\n * Google AI Provider Implementation\r\n * Supports Gemini Pro, PaLM, and other Google models\r\n */\r\n\r\nimport { BaseProvider } from './base-provider.js';\r\nimport {\r\n  LLMProvider,\r\n  LLMModel,\r\n  LLMRequest,\r\n  LLMResponse,\r\n  LLMStreamEvent,\r\n  ModelInfo,\r\n  ProviderCapabilities,\r\n  HealthCheckResult,\r\n  LLMProviderError,\r\n  RateLimitError,\r\n  AuthenticationError,\r\n} from './types.js';\r\n\r\ninterface GoogleAIRequest {\r\n  contents: Array<{\r\n    role: 'user' | 'model';\r\n    parts: Array<{\r\n      text?: string;\r\n      inlineData?: {\r\n        mimeType: string;\r\n        data: string;\r\n      };\r\n    }>;\r\n  }>;\r\n  generationConfig?: {\r\n    temperature?: number;\r\n    topK?: number;\r\n    topP?: number;\r\n    maxOutputTokens?: number;\r\n    stopSequences?: string[];\r\n  };\r\n  safetySettings?: Array<{\r\n    category: string;\r\n    threshold: string;\r\n  }>;\r\n}\r\n\r\ninterface GoogleAIResponse {\r\n  candidates: Array<{\r\n    content: {\r\n      parts: Array<{\r\n        text: string;\r\n      }>;\r\n      role: string;\r\n    };\r\n    finishReason: string;\r\n    index: number;\r\n    safetyRatings: Array<{\r\n      category: string;\r\n      probability: string;\r\n    }>;\r\n  }>;\r\n  promptFeedback?: {\r\n    safetyRatings: Array<{\r\n      category: string;\r\n      probability: string;\r\n    }>;\r\n  };\r\n  usageMetadata?: {\r\n    promptTokenCount: number;\r\n    candidatesTokenCount: number;\r\n    totalTokenCount: number;\r\n  };\r\n}\r\n\r\nexport class GoogleProvider extends BaseProvider {\r\n  readonly name: LLMProvider = 'google';\r\n  readonly capabilities: ProviderCapabilities = {\r\n    supportedModels: [\r\n      'gemini-pro',\r\n      'gemini-pro-vision',\r\n      'palm-2',\r\n      'bison',\r\n    ],\r\n    maxContextLength: {\r\n      'gemini-pro': 32768,\r\n      'gemini-pro-vision': 16384,\r\n      'palm-2': 8192,\r\n      'bison': 4096,\r\n    } as Record<LLMModel, number>,\r\n    maxOutputTokens: {\r\n      'gemini-pro': 2048,\r\n      'gemini-pro-vision': 2048,\r\n      'palm-2': 1024,\r\n      'bison': 1024,\r\n    } as Record<LLMModel, number>,\r\n    supportsStreaming: true,\r\n    supportsFunctionCalling: true,\r\n    supportsSystemMessages: false, // Google AI doesn't have explicit system messages\r\n    supportsVision: true, // Gemini Pro Vision\r\n    supportsAudio: false,\r\n    supportsTools: true,\r\n    supportsFineTuning: false,\r\n    supportsEmbeddings: true,\r\n    supportsLogprobs: false,\r\n    supportsBatching: true,\r\n    rateLimit: {\r\n      requestsPerMinute: 60,\r\n      tokensPerMinute: 60000,\r\n      concurrentRequests: 10,\r\n    },\r\n    pricing: {\r\n      'gemini-pro': {\r\n        promptCostPer1k: 0.00025,\r\n        completionCostPer1k: 0.0005,\r\n        currency: 'USD',\r\n      },\r\n      'gemini-pro-vision': {\r\n        promptCostPer1k: 0.00025,\r\n        completionCostPer1k: 0.0005,\r\n        currency: 'USD',\r\n      },\r\n      'palm-2': {\r\n        promptCostPer1k: 0.0005,\r\n        completionCostPer1k: 0.001,\r\n        currency: 'USD',\r\n      },\r\n      'bison': {\r\n        promptCostPer1k: 0.0005,\r\n        completionCostPer1k: 0.001,\r\n        currency: 'USD',\r\n      },\r\n    },\r\n  };\r\n\r\n  private baseUrl: string;\r\n\r\n  protected async doInitialize(): Promise<void> {\r\n    if (!this.config.apiKey) {\r\n      throw new AuthenticationError('Google AI API key is required', 'google');\r\n    }\r\n\r\n    // Use Gemini API for newer models, PaLM API for older ones\r\n    const model = this.config.model;\r\n    if (model.startsWith('gemini')) {\r\n      this.baseUrl = 'https://generativelanguage.googleapis.com/v1beta';\r\n    } else {\r\n      this.baseUrl = 'https://generativelanguage.googleapis.com/v1beta2';\r\n    }\r\n  }\r\n\r\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\r\n    const googleRequest = this.buildGoogleRequest(request);\r\n    const model = this.mapToGoogleModel(request.model || this.config.model);\r\n    \r\n    const url = `${this.baseUrl}/models/${model}:generateContent?key=${this.config.apiKey}`;\r\n    \r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 60000);\r\n\r\n    try {\r\n      const response = await fetch(url, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify(googleRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      clearTimeout(timeout);\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const data: GoogleAIResponse = await response.json();\r\n      \r\n      if (!data.candidates || data.candidates.length === 0) {\r\n        throw new LLMProviderError(\r\n          'No response generated',\r\n          'NO_RESPONSE',\r\n          'google',\r\n          undefined,\r\n          false\r\n        );\r\n      }\r\n\r\n      const candidate = data.candidates[0];\r\n      const content = candidate.content.parts.map(part => part.text).join('');\r\n      \r\n      // Calculate cost\r\n      const usageData = data.usageMetadata || {\r\n        promptTokenCount: this.estimateTokens(JSON.stringify(request.messages)),\r\n        candidatesTokenCount: this.estimateTokens(content),\r\n        totalTokenCount: 0,\r\n      };\r\n      usageData.totalTokenCount = usageData.promptTokenCount + usageData.candidatesTokenCount;\r\n\r\n      const pricing = this.capabilities.pricing![request.model || this.config.model];\r\n      const promptCost = (usageData.promptTokenCount / 1000) * pricing.promptCostPer1k;\r\n      const completionCost = (usageData.candidatesTokenCount / 1000) * pricing.completionCostPer1k;\r\n\r\n      return {\r\n        id: `google-${Date.now()}`,\r\n        model: request.model || this.config.model,\r\n        provider: 'google',\r\n        content,\r\n        usage: {\r\n          promptTokens: usageData.promptTokenCount,\r\n          completionTokens: usageData.candidatesTokenCount,\r\n          totalTokens: usageData.totalTokenCount,\r\n        },\r\n        cost: {\r\n          promptCost,\r\n          completionCost,\r\n          totalCost: promptCost + completionCost,\r\n          currency: 'USD',\r\n        },\r\n        finishReason: this.mapFinishReason(candidate.finishReason),\r\n      };\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      throw this.transformError(error);\r\n    }\r\n  }\r\n\r\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\r\n    const googleRequest = this.buildGoogleRequest(request);\r\n    const model = this.mapToGoogleModel(request.model || this.config.model);\r\n    \r\n    const url = `${this.baseUrl}/models/${model}:streamGenerateContent?key=${this.config.apiKey}`;\r\n    \r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 60000) * 2);\r\n\r\n    try {\r\n      const response = await fetch(url, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify(googleRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const reader = response.body!.getReader();\r\n      const decoder = new TextDecoder();\r\n      let buffer = '';\r\n      let totalContent = '';\r\n      let promptTokens = 0;\r\n      let completionTokens = 0;\r\n\r\n      while (true) {\r\n        const { done, value } = await reader.read();\r\n        if (done) break;\r\n\r\n        buffer += decoder.decode(value, { stream: true });\r\n        const lines = buffer.split('\\n');\r\n        buffer = lines.pop() || '';\r\n\r\n        for (const line of lines) {\r\n          if (line.trim() === '') continue;\r\n          \r\n          try {\r\n            const data: GoogleAIResponse = JSON.parse(line);\r\n            \r\n            if (data.candidates && data.candidates.length > 0) {\r\n              const candidate = data.candidates[0];\r\n              const content = candidate.content.parts.map(part => part.text).join('');\r\n              \r\n              if (content) {\r\n                totalContent += content;\r\n                yield {\r\n                  type: 'content',\r\n                  delta: { content },\r\n                };\r\n              }\r\n              \r\n              if (data.usageMetadata) {\r\n                promptTokens = data.usageMetadata.promptTokenCount;\r\n                completionTokens = data.usageMetadata.candidatesTokenCount;\r\n              }\r\n            }\r\n          } catch (e) {\r\n            this.logger.warn('Failed to parse Google AI stream chunk', { line, error: e });\r\n          }\r\n        }\r\n      }\r\n\r\n      // Final event with usage and cost\r\n      const pricing = this.capabilities.pricing![request.model || this.config.model];\r\n      const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\r\n      const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\r\n\r\n      yield {\r\n        type: 'done',\r\n        usage: {\r\n          promptTokens,\r\n          completionTokens,\r\n          totalTokens: promptTokens + completionTokens,\r\n        },\r\n        cost: {\r\n          promptCost,\r\n          completionCost,\r\n          totalCost: promptCost + completionCost,\r\n          currency: 'USD',\r\n        },\r\n      };\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      throw this.transformError(error);\r\n    } finally {\r\n      clearTimeout(timeout);\r\n    }\r\n  }\r\n\r\n  async listModels(): Promise<LLMModel[]> {\r\n    return this.capabilities.supportedModels;\r\n  }\r\n\r\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\r\n    return {\r\n      model,\r\n      name: model,\r\n      description: this.getModelDescription(model),\r\n      contextLength: this.capabilities.maxContextLength[model] || 4096,\r\n      maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\r\n      supportedFeatures: [\r\n        'chat',\r\n        'completion',\r\n        ...(model.includes('vision') ? ['vision'] : []),\r\n        ...(model.startsWith('gemini') ? ['function_calling'] : []),\r\n      ],\r\n      pricing: this.capabilities.pricing![model],\r\n    };\r\n  }\r\n\r\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\r\n    try {\r\n      const url = `${this.baseUrl}/models?key=${this.config.apiKey}`;\r\n      const response = await fetch(url);\r\n\r\n      if (!response.ok) {\r\n        throw new Error(`Health check failed: ${response.status}`);\r\n      }\r\n\r\n      return {\r\n        healthy: true,\r\n        timestamp: new Date(),\r\n      };\r\n    } catch (error) {\r\n      return {\r\n        healthy: false,\r\n        error: error instanceof Error ? error.message : 'Unknown error',\r\n        timestamp: new Date(),\r\n      };\r\n    }\r\n  }\r\n\r\n  private buildGoogleRequest(request: LLMRequest): GoogleAIRequest {\r\n    // Convert messages to Google format\r\n    const contents: GoogleAIRequest['contents'] = [];\r\n    \r\n    for (const message of request.messages) {\r\n      // Skip system messages or prepend to first user message\r\n      if (message.role === 'system') {\r\n        if (contents.length === 0) {\r\n          contents.push({\r\n            role: 'user',\r\n            parts: [{ text: `Instructions: ${message.content}` }],\r\n          });\r\n        }\r\n        continue;\r\n      }\r\n      \r\n      contents.push({\r\n        role: message.role === 'assistant' ? 'model' : 'user',\r\n        parts: [{ text: message.content }],\r\n      });\r\n    }\r\n\r\n    return {\r\n      contents,\r\n      generationConfig: {\r\n        temperature: request.temperature ?? this.config.temperature,\r\n        topK: request.topK ?? this.config.topK,\r\n        topP: request.topP ?? this.config.topP,\r\n        maxOutputTokens: request.maxTokens ?? this.config.maxTokens,\r\n        stopSequences: request.stopSequences ?? this.config.stopSequences,\r\n      },\r\n      safetySettings: [\r\n        {\r\n          category: 'HARM_CATEGORY_HARASSMENT',\r\n          threshold: 'BLOCK_NONE',\r\n        },\r\n        {\r\n          category: 'HARM_CATEGORY_HATE_SPEECH',\r\n          threshold: 'BLOCK_NONE',\r\n        },\r\n        {\r\n          category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\r\n          threshold: 'BLOCK_NONE',\r\n        },\r\n        {\r\n          category: 'HARM_CATEGORY_DANGEROUS_CONTENT',\r\n          threshold: 'BLOCK_NONE',\r\n        },\r\n      ],\r\n    };\r\n  }\r\n\r\n  private mapToGoogleModel(model: LLMModel): string {\r\n    const modelMap: Record<string, string> = {\r\n      'gemini-pro': 'gemini-pro',\r\n      'gemini-pro-vision': 'gemini-pro-vision',\r\n      'palm-2': 'text-bison-001',\r\n      'bison': 'text-bison-001',\r\n    };\r\n    return modelMap[model] || model;\r\n  }\r\n\r\n  private mapFinishReason(reason: string): 'stop' | 'length' | 'content_filter' {\r\n    switch (reason) {\r\n      case 'STOP':\r\n        return 'stop';\r\n      case 'MAX_TOKENS':\r\n        return 'length';\r\n      case 'SAFETY':\r\n      case 'RECITATION':\r\n        return 'content_filter';\r\n      default:\r\n        return 'stop';\r\n    }\r\n  }\r\n\r\n  private getModelDescription(model: LLMModel): string {\r\n    const descriptions: Record<string, string> = {\r\n      'gemini-pro': 'Google\\'s most capable text model',\r\n      'gemini-pro-vision': 'Gemini Pro with vision capabilities',\r\n      'palm-2': 'Previous generation large language model',\r\n      'bison': 'Efficient model for various tasks',\r\n    };\r\n    return descriptions[model] || 'Google AI language model';\r\n  }\r\n\r\n  private async handleErrorResponse(response: Response): Promise<void> {\r\n    const errorText = await response.text();\r\n    let errorData: any;\r\n\r\n    try {\r\n      errorData = JSON.parse(errorText);\r\n    } catch {\r\n      errorData = { error: { message: errorText } };\r\n    }\r\n\r\n    const message = errorData.error?.message || 'Unknown error';\r\n\r\n    switch (response.status) {\r\n      case 401:\r\n      case 403:\r\n        throw new AuthenticationError(message, 'google', errorData);\r\n      case 429:\r\n        throw new RateLimitError(message, 'google', undefined, errorData);\r\n      default:\r\n        throw new LLMProviderError(\r\n          message,\r\n          `GOOGLE_${response.status}`,\r\n          'google',\r\n          response.status,\r\n          response.status >= 500,\r\n          errorData\r\n        );\r\n    }\r\n  }\r\n}"],"names":["BaseProvider","LLMProviderError","RateLimitError","AuthenticationError","GoogleProvider","name","capabilities","supportedModels","maxContextLength","maxOutputTokens","supportsStreaming","supportsFunctionCalling","supportsSystemMessages","supportsVision","supportsAudio","supportsTools","supportsFineTuning","supportsEmbeddings","supportsLogprobs","supportsBatching","rateLimit","requestsPerMinute","tokensPerMinute","concurrentRequests","pricing","promptCostPer1k","completionCostPer1k","currency","baseUrl","doInitialize","config","apiKey","model","startsWith","doComplete","request","googleRequest","buildGoogleRequest","mapToGoogleModel","url","controller","AbortController","timeout","setTimeout","abort","response","fetch","method","headers","body","JSON","stringify","signal","clearTimeout","ok","handleErrorResponse","data","json","candidates","length","undefined","candidate","content","parts","map","part","text","join","usageData","usageMetadata","promptTokenCount","estimateTokens","messages","candidatesTokenCount","totalTokenCount","promptCost","completionCost","id","Date","now","provider","usage","promptTokens","completionTokens","totalTokens","cost","totalCost","finishReason","mapFinishReason","error","transformError","doStreamComplete","reader","getReader","decoder","TextDecoder","buffer","totalContent","done","value","read","decode","stream","lines","split","pop","line","trim","parse","type","delta","e","logger","warn","listModels","getModelInfo","description","getModelDescription","contextLength","supportedFeatures","includes","doHealthCheck","Error","status","healthy","timestamp","message","contents","role","push","generationConfig","temperature","topK","topP","maxTokens","stopSequences","safetySettings","category","threshold","modelMap","reason","descriptions","errorText","errorData"],"mappings":"AAKA,SAASA,YAAY,QAAQ,qBAAqB;AAClD,SASEC,gBAAgB,EAChBC,cAAc,EACdC,mBAAmB,QACd,aAAa;AAsDpB,OAAO,MAAMC,uBAAuBJ;IACzBK,OAAoB,SAAS;IAC7BC,eAAqC;QAC5CC,iBAAiB;YACf;YACA;YACA;YACA;SACD;QACDC,kBAAkB;YAChB,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACAC,iBAAiB;YACf,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACAC,mBAAmB;QACnBC,yBAAyB;QACzBC,wBAAwB;QACxBC,gBAAgB;QAChBC,eAAe;QACfC,eAAe;QACfC,oBAAoB;QACpBC,oBAAoB;QACpBC,kBAAkB;QAClBC,kBAAkB;QAClBC,WAAW;YACTC,mBAAmB;YACnBC,iBAAiB;YACjBC,oBAAoB;QACtB;QACAC,SAAS;YACP,cAAc;gBACZC,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,qBAAqB;gBACnBF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,UAAU;gBACRF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;YACA,SAAS;gBACPF,iBAAiB;gBACjBC,qBAAqB;gBACrBC,UAAU;YACZ;QACF;IACF,EAAE;IAEMC,QAAgB;IAExB,MAAgBC,eAA8B;QAC5C,IAAI,CAAC,IAAI,CAACC,MAAM,CAACC,MAAM,EAAE;YACvB,MAAM,IAAI5B,oBAAoB,iCAAiC;QACjE;QAGA,MAAM6B,QAAQ,IAAI,CAACF,MAAM,CAACE,KAAK;QAC/B,IAAIA,MAAMC,UAAU,CAAC,WAAW;YAC9B,IAAI,CAACL,OAAO,GAAG;QACjB,OAAO;YACL,IAAI,CAACA,OAAO,GAAG;QACjB;IACF;IAEA,MAAgBM,WAAWC,OAAmB,EAAwB;QACpE,MAAMC,gBAAgB,IAAI,CAACC,kBAAkB,CAACF;QAC9C,MAAMH,QAAQ,IAAI,CAACM,gBAAgB,CAACH,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;QAEtE,MAAMO,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,QAAQ,EAAEI,MAAM,qBAAqB,EAAE,IAAI,CAACF,MAAM,CAACC,MAAM,EAAE;QAEvF,MAAMS,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,IAAI,CAACd,MAAM,CAACY,OAAO,IAAI;QAE5E,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAMP,KAAK;gBAChCQ,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAACf;gBACrBgB,QAAQZ,WAAWY,MAAM;YAC3B;YAEAC,aAAaX;YAEb,IAAI,CAACG,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACV;YACjC;YAEA,MAAMW,OAAyB,MAAMX,SAASY,IAAI;YAElD,IAAI,CAACD,KAAKE,UAAU,IAAIF,KAAKE,UAAU,CAACC,MAAM,KAAK,GAAG;gBACpD,MAAM,IAAI1D,iBACR,yBACA,eACA,UACA2D,WACA;YAEJ;YAEA,MAAMC,YAAYL,KAAKE,UAAU,CAAC,EAAE;YACpC,MAAMI,UAAUD,UAAUC,OAAO,CAACC,KAAK,CAACC,GAAG,CAACC,CAAAA,OAAQA,KAAKC,IAAI,EAAEC,IAAI,CAAC;YAGpE,MAAMC,YAAYZ,KAAKa,aAAa,IAAI;gBACtCC,kBAAkB,IAAI,CAACC,cAAc,CAACrB,KAAKC,SAAS,CAAChB,QAAQqC,QAAQ;gBACrEC,sBAAsB,IAAI,CAACF,cAAc,CAACT;gBAC1CY,iBAAiB;YACnB;YACAN,UAAUM,eAAe,GAAGN,UAAUE,gBAAgB,GAAGF,UAAUK,oBAAoB;YAEvF,MAAMjD,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACW,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK,CAAC;YAC9E,MAAM2C,aAAa,AAACP,UAAUE,gBAAgB,GAAG,OAAQ9C,QAAQC,eAAe;YAChF,MAAMmD,iBAAiB,AAACR,UAAUK,oBAAoB,GAAG,OAAQjD,QAAQE,mBAAmB;YAE5F,OAAO;gBACLmD,IAAI,CAAC,OAAO,EAAEC,KAAKC,GAAG,IAAI;gBAC1B/C,OAAOG,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;gBACzCgD,UAAU;gBACVlB;gBACAmB,OAAO;oBACLC,cAAcd,UAAUE,gBAAgB;oBACxCa,kBAAkBf,UAAUK,oBAAoB;oBAChDW,aAAahB,UAAUM,eAAe;gBACxC;gBACAW,MAAM;oBACJV;oBACAC;oBACAU,WAAWX,aAAaC;oBACxBjD,UAAU;gBACZ;gBACA4D,cAAc,IAAI,CAACC,eAAe,CAAC3B,UAAU0B,YAAY;YAC3D;QACF,EAAE,OAAOE,OAAO;YACdpC,aAAaX;YACb,MAAM,IAAI,CAACgD,cAAc,CAACD;QAC5B;IACF;IAEA,OAAiBE,iBAAiBxD,OAAmB,EAAiC;QACpF,MAAMC,gBAAgB,IAAI,CAACC,kBAAkB,CAACF;QAC9C,MAAMH,QAAQ,IAAI,CAACM,gBAAgB,CAACH,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK;QAEtE,MAAMO,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,QAAQ,EAAEI,MAAM,2BAA2B,EAAE,IAAI,CAACF,MAAM,CAACC,MAAM,EAAE;QAE7F,MAAMS,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,AAAC,CAAA,IAAI,CAACd,MAAM,CAACY,OAAO,IAAI,KAAI,IAAK;QAEtF,IAAI;YACF,MAAMG,WAAW,MAAMC,MAAMP,KAAK;gBAChCQ,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAACf;gBACrBgB,QAAQZ,WAAWY,MAAM;YAC3B;YAEA,IAAI,CAACP,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAI,CAACC,mBAAmB,CAACV;YACjC;YAEA,MAAM+C,SAAS/C,SAASI,IAAI,CAAE4C,SAAS;YACvC,MAAMC,UAAU,IAAIC;YACpB,IAAIC,SAAS;YACb,IAAIC,eAAe;YACnB,IAAIf,eAAe;YACnB,IAAIC,mBAAmB;YAEvB,MAAO,KAAM;gBACX,MAAM,EAAEe,IAAI,EAAEC,KAAK,EAAE,GAAG,MAAMP,OAAOQ,IAAI;gBACzC,IAAIF,MAAM;gBAEVF,UAAUF,QAAQO,MAAM,CAACF,OAAO;oBAAEG,QAAQ;gBAAK;gBAC/C,MAAMC,QAAQP,OAAOQ,KAAK,CAAC;gBAC3BR,SAASO,MAAME,GAAG,MAAM;gBAExB,KAAK,MAAMC,QAAQH,MAAO;oBACxB,IAAIG,KAAKC,IAAI,OAAO,IAAI;oBAExB,IAAI;wBACF,MAAMnD,OAAyBN,KAAK0D,KAAK,CAACF;wBAE1C,IAAIlD,KAAKE,UAAU,IAAIF,KAAKE,UAAU,CAACC,MAAM,GAAG,GAAG;4BACjD,MAAME,YAAYL,KAAKE,UAAU,CAAC,EAAE;4BACpC,MAAMI,UAAUD,UAAUC,OAAO,CAACC,KAAK,CAACC,GAAG,CAACC,CAAAA,OAAQA,KAAKC,IAAI,EAAEC,IAAI,CAAC;4BAEpE,IAAIL,SAAS;gCACXmC,gBAAgBnC;gCAChB,MAAM;oCACJ+C,MAAM;oCACNC,OAAO;wCAAEhD;oCAAQ;gCACnB;4BACF;4BAEA,IAAIN,KAAKa,aAAa,EAAE;gCACtBa,eAAe1B,KAAKa,aAAa,CAACC,gBAAgB;gCAClDa,mBAAmB3B,KAAKa,aAAa,CAACI,oBAAoB;4BAC5D;wBACF;oBACF,EAAE,OAAOsC,GAAG;wBACV,IAAI,CAACC,MAAM,CAACC,IAAI,CAAC,0CAA0C;4BAAEP;4BAAMjB,OAAOsB;wBAAE;oBAC9E;gBACF;YACF;YAGA,MAAMvF,UAAU,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACW,QAAQH,KAAK,IAAI,IAAI,CAACF,MAAM,CAACE,KAAK,CAAC;YAC9E,MAAM2C,aAAa,AAACO,eAAe,OAAQ1D,QAAQC,eAAe;YAClE,MAAMmD,iBAAiB,AAACO,mBAAmB,OAAQ3D,QAAQE,mBAAmB;YAE9E,MAAM;gBACJmF,MAAM;gBACN5B,OAAO;oBACLC;oBACAC;oBACAC,aAAaF,eAAeC;gBAC9B;gBACAE,MAAM;oBACJV;oBACAC;oBACAU,WAAWX,aAAaC;oBACxBjD,UAAU;gBACZ;YACF;QACF,EAAE,OAAO8D,OAAO;YACdpC,aAAaX;YACb,MAAM,IAAI,CAACgD,cAAc,CAACD;QAC5B,SAAU;YACRpC,aAAaX;QACf;IACF;IAEA,MAAMwE,aAAkC;QACtC,OAAO,IAAI,CAAC5G,YAAY,CAACC,eAAe;IAC1C;IAEA,MAAM4G,aAAanF,KAAe,EAAsB;QACtD,OAAO;YACLA;YACA3B,MAAM2B;YACNoF,aAAa,IAAI,CAACC,mBAAmB,CAACrF;YACtCsF,eAAe,IAAI,CAAChH,YAAY,CAACE,gBAAgB,CAACwB,MAAM,IAAI;YAC5DvB,iBAAiB,IAAI,CAACH,YAAY,CAACG,eAAe,CAACuB,MAAM,IAAI;YAC7DuF,mBAAmB;gBACjB;gBACA;mBACIvF,MAAMwF,QAAQ,CAAC,YAAY;oBAAC;iBAAS,GAAG,EAAE;mBAC1CxF,MAAMC,UAAU,CAAC,YAAY;oBAAC;iBAAmB,GAAG,EAAE;aAC3D;YACDT,SAAS,IAAI,CAAClB,YAAY,CAACkB,OAAO,AAAC,CAACQ,MAAM;QAC5C;IACF;IAEA,MAAgByF,gBAA4C;QAC1D,IAAI;YACF,MAAMlF,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,YAAY,EAAE,IAAI,CAACE,MAAM,CAACC,MAAM,EAAE;YAC9D,MAAMc,WAAW,MAAMC,MAAMP;YAE7B,IAAI,CAACM,SAASS,EAAE,EAAE;gBAChB,MAAM,IAAIoE,MAAM,CAAC,qBAAqB,EAAE7E,SAAS8E,MAAM,EAAE;YAC3D;YAEA,OAAO;gBACLC,SAAS;gBACTC,WAAW,IAAI/C;YACjB;QACF,EAAE,OAAOW,OAAO;YACd,OAAO;gBACLmC,SAAS;gBACTnC,OAAOA,iBAAiBiC,QAAQjC,MAAMqC,OAAO,GAAG;gBAChDD,WAAW,IAAI/C;YACjB;QACF;IACF;IAEQzC,mBAAmBF,OAAmB,EAAmB;QAE/D,MAAM4F,WAAwC,EAAE;QAEhD,KAAK,MAAMD,WAAW3F,QAAQqC,QAAQ,CAAE;YAEtC,IAAIsD,QAAQE,IAAI,KAAK,UAAU;gBAC7B,IAAID,SAASpE,MAAM,KAAK,GAAG;oBACzBoE,SAASE,IAAI,CAAC;wBACZD,MAAM;wBACNjE,OAAO;4BAAC;gCAAEG,MAAM,CAAC,cAAc,EAAE4D,QAAQhE,OAAO,EAAE;4BAAC;yBAAE;oBACvD;gBACF;gBACA;YACF;YAEAiE,SAASE,IAAI,CAAC;gBACZD,MAAMF,QAAQE,IAAI,KAAK,cAAc,UAAU;gBAC/CjE,OAAO;oBAAC;wBAAEG,MAAM4D,QAAQhE,OAAO;oBAAC;iBAAE;YACpC;QACF;QAEA,OAAO;YACLiE;YACAG,kBAAkB;gBAChBC,aAAahG,QAAQgG,WAAW,IAAI,IAAI,CAACrG,MAAM,CAACqG,WAAW;gBAC3DC,MAAMjG,QAAQiG,IAAI,IAAI,IAAI,CAACtG,MAAM,CAACsG,IAAI;gBACtCC,MAAMlG,QAAQkG,IAAI,IAAI,IAAI,CAACvG,MAAM,CAACuG,IAAI;gBACtC5H,iBAAiB0B,QAAQmG,SAAS,IAAI,IAAI,CAACxG,MAAM,CAACwG,SAAS;gBAC3DC,eAAepG,QAAQoG,aAAa,IAAI,IAAI,CAACzG,MAAM,CAACyG,aAAa;YACnE;YACAC,gBAAgB;gBACd;oBACEC,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;gBACA;oBACED,UAAU;oBACVC,WAAW;gBACb;aACD;QACH;IACF;IAEQpG,iBAAiBN,KAAe,EAAU;QAChD,MAAM2G,WAAmC;YACvC,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACA,OAAOA,QAAQ,CAAC3G,MAAM,IAAIA;IAC5B;IAEQwD,gBAAgBoD,MAAc,EAAwC;QAC5E,OAAQA;YACN,KAAK;gBACH,OAAO;YACT,KAAK;gBACH,OAAO;YACT,KAAK;YACL,KAAK;gBACH,OAAO;YACT;gBACE,OAAO;QACX;IACF;IAEQvB,oBAAoBrF,KAAe,EAAU;QACnD,MAAM6G,eAAuC;YAC3C,cAAc;YACd,qBAAqB;YACrB,UAAU;YACV,SAAS;QACX;QACA,OAAOA,YAAY,CAAC7G,MAAM,IAAI;IAChC;IAEA,MAAcuB,oBAAoBV,QAAkB,EAAiB;QACnE,MAAMiG,YAAY,MAAMjG,SAASqB,IAAI;QACrC,IAAI6E;QAEJ,IAAI;YACFA,YAAY7F,KAAK0D,KAAK,CAACkC;QACzB,EAAE,OAAM;YACNC,YAAY;gBAAEtD,OAAO;oBAAEqC,SAASgB;gBAAU;YAAE;QAC9C;QAEA,MAAMhB,UAAUiB,UAAUtD,KAAK,EAAEqC,WAAW;QAE5C,OAAQjF,SAAS8E,MAAM;YACrB,KAAK;YACL,KAAK;gBACH,MAAM,IAAIxH,oBAAoB2H,SAAS,UAAUiB;YACnD,KAAK;gBACH,MAAM,IAAI7I,eAAe4H,SAAS,UAAUlE,WAAWmF;YACzD;gBACE,MAAM,IAAI9I,iBACR6H,SACA,CAAC,OAAO,EAAEjF,SAAS8E,MAAM,EAAE,EAC3B,UACA9E,SAAS8E,MAAM,EACf9E,SAAS8E,MAAM,IAAI,KACnBoB;QAEN;IACF;AACF"}