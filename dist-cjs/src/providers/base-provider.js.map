{"version":3,"sources":["../../../src/providers/base-provider.ts"],"sourcesContent":["/**\r\n * Abstract Base Provider for LLM integrations\r\n * Provides common functionality for all LLM providers\r\n */\r\n\r\nimport { EventEmitter } from 'events';\r\nimport { ILogger } from '../core/logger.js';\r\nimport { circuitBreaker, CircuitBreaker } from '../utils/helpers.js';\r\nimport {\r\n  ILLMProvider,\r\n  LLMProvider,\r\n  LLMProviderConfig,\r\n  LLMRequest,\r\n  LLMResponse,\r\n  LLMStreamEvent,\r\n  LLMModel,\r\n  ModelInfo,\r\n  ProviderCapabilities,\r\n  HealthCheckResult,\r\n  ProviderStatus,\r\n  CostEstimate,\r\n  UsageStats,\r\n  UsagePeriod,\r\n  LLMProviderError,\r\n  RateLimitError,\r\n  ProviderUnavailableError,\r\n} from './types.js';\r\n\r\nexport interface BaseProviderOptions {\r\n  logger: ILogger;\r\n  config: LLMProviderConfig;\r\n  cacheTTL?: number;\r\n  circuitBreakerOptions?: {\r\n    threshold?: number;\r\n    timeout?: number;\r\n    resetTimeout?: number;\r\n  };\r\n}\r\n\r\nexport abstract class BaseProvider extends EventEmitter implements ILLMProvider {\r\n  abstract readonly name: LLMProvider;\r\n  abstract readonly capabilities: ProviderCapabilities;\r\n  \r\n  protected logger: ILogger;\r\n  protected circuitBreaker: CircuitBreaker;\r\n  protected healthCheckInterval?: NodeJS.Timeout;\r\n  protected lastHealthCheck?: HealthCheckResult;\r\n  protected requestCount = 0;\r\n  protected errorCount = 0;\r\n  protected totalTokens = 0;\r\n  protected totalCost = 0;\r\n  protected requestMetrics: Map<string, any> = new Map();\r\n  \r\n  public config: LLMProviderConfig;\r\n\r\n  constructor(options: BaseProviderOptions) {\r\n    super();\r\n    this.logger = options.logger;\r\n    this.config = options.config;\r\n    \r\n    // Initialize circuit breaker\r\n    this.circuitBreaker = circuitBreaker(`llm-${this.name}`, {\r\n      threshold: options.circuitBreakerOptions?.threshold || 5,\r\n      timeout: options.circuitBreakerOptions?.timeout || 60000,\r\n      resetTimeout: options.circuitBreakerOptions?.resetTimeout || 300000,\r\n    });\r\n    \r\n    // Start health checks if enabled\r\n    if (this.config.enableCaching) {\r\n      this.startHealthChecks();\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Initialize the provider\r\n   */\r\n  async initialize(): Promise<void> {\r\n    this.logger.info(`Initializing ${this.name} provider`, {\r\n      model: this.config.model,\r\n      temperature: this.config.temperature,\r\n      maxTokens: this.config.maxTokens,\r\n    });\r\n    \r\n    // Validate configuration\r\n    this.validateConfig();\r\n    \r\n    // Provider-specific initialization\r\n    await this.doInitialize();\r\n    \r\n    // Perform initial health check\r\n    await this.healthCheck();\r\n  }\r\n\r\n  /**\r\n   * Provider-specific initialization\r\n   */\r\n  protected abstract doInitialize(): Promise<void>;\r\n\r\n  /**\r\n   * Validate provider configuration\r\n   */\r\n  protected validateConfig(): void {\r\n    if (!this.config.model) {\r\n      throw new Error(`Model is required for ${this.name} provider`);\r\n    }\r\n    \r\n    if (!this.validateModel(this.config.model)) {\r\n      throw new Error(`Model ${this.config.model} is not supported by ${this.name} provider`);\r\n    }\r\n    \r\n    if (this.config.temperature !== undefined) {\r\n      if (this.config.temperature < 0 || this.config.temperature > 2) {\r\n        throw new Error('Temperature must be between 0 and 2');\r\n      }\r\n    }\r\n    \r\n    if (this.config.maxTokens !== undefined) {\r\n      const maxAllowed = this.capabilities.maxOutputTokens[this.config.model] || 4096;\r\n      if (this.config.maxTokens > maxAllowed) {\r\n        throw new Error(`Max tokens exceeds limit of ${maxAllowed} for model ${this.config.model}`);\r\n      }\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Complete a request\r\n   */\r\n  async complete(request: LLMRequest): Promise<LLMResponse> {\r\n    const startTime = Date.now();\r\n    \r\n    try {\r\n      // Use circuit breaker\r\n      const response = await this.circuitBreaker.execute(async () => {\r\n        return await this.doComplete(request);\r\n      });\r\n      \r\n      // Track metrics\r\n      const latency = Date.now() - startTime;\r\n      this.trackRequest(request, response, latency);\r\n      \r\n      // Emit events\r\n      this.emit('response', {\r\n        provider: this.name,\r\n        model: response.model,\r\n        latency,\r\n        tokens: response.usage.totalTokens,\r\n        cost: response.cost?.totalCost,\r\n      });\r\n      \r\n      return response;\r\n    } catch (error) {\r\n      this.errorCount++;\r\n      \r\n      // Transform to provider error\r\n      const providerError = this.transformError(error);\r\n      \r\n      // Track error\r\n      this.emit('error', {\r\n        provider: this.name,\r\n        error: providerError,\r\n        request,\r\n      });\r\n      \r\n      throw providerError;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Provider-specific completion implementation\r\n   */\r\n  protected abstract doComplete(request: LLMRequest): Promise<LLMResponse>;\r\n\r\n  /**\r\n   * Stream complete a request\r\n   */\r\n  async *streamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\r\n    const startTime = Date.now();\r\n    let totalTokens = 0;\r\n    let totalCost = 0;\r\n    \r\n    try {\r\n      // Check if streaming is supported\r\n      if (!this.capabilities.supportsStreaming) {\r\n        throw new LLMProviderError(\r\n          'Streaming not supported',\r\n          'STREAMING_NOT_SUPPORTED',\r\n          this.name,\r\n          undefined,\r\n          false\r\n        );\r\n      }\r\n      \r\n      // Use circuit breaker\r\n      const stream = await this.circuitBreaker.execute(async () => {\r\n        return this.doStreamComplete(request);\r\n      });\r\n      \r\n      // Process stream\r\n      for await (const event of stream) {\r\n        if (event.usage) {\r\n          totalTokens = event.usage.totalTokens;\r\n        }\r\n        if (event.cost) {\r\n          totalCost = event.cost.totalCost;\r\n        }\r\n        \r\n        yield event;\r\n      }\r\n      \r\n      // Track metrics\r\n      const latency = Date.now() - startTime;\r\n      this.trackStreamRequest(request, totalTokens, totalCost, latency);\r\n      \r\n    } catch (error) {\r\n      this.errorCount++;\r\n      \r\n      // Transform to provider error\r\n      const providerError = this.transformError(error);\r\n      \r\n      // Yield error event\r\n      yield {\r\n        type: 'error',\r\n        error: providerError,\r\n      };\r\n      \r\n      throw providerError;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Provider-specific stream completion implementation\r\n   */\r\n  protected abstract doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent>;\r\n\r\n  /**\r\n   * List available models\r\n   */\r\n  abstract listModels(): Promise<LLMModel[]>;\r\n\r\n  /**\r\n   * Get model information\r\n   */\r\n  abstract getModelInfo(model: LLMModel): Promise<ModelInfo>;\r\n\r\n  /**\r\n   * Validate if a model is supported\r\n   */\r\n  validateModel(model: LLMModel): boolean {\r\n    return this.capabilities.supportedModels.includes(model);\r\n  }\r\n\r\n  /**\r\n   * Perform health check\r\n   */\r\n  async healthCheck(): Promise<HealthCheckResult> {\r\n    const startTime = Date.now();\r\n    \r\n    try {\r\n      // Provider-specific health check\r\n      const result = await this.doHealthCheck();\r\n      \r\n      this.lastHealthCheck = {\r\n        ...result,\r\n        latency: Date.now() - startTime,\r\n        timestamp: new Date(),\r\n      };\r\n      \r\n      this.emit('health_check', this.lastHealthCheck);\r\n      return this.lastHealthCheck;\r\n      \r\n    } catch (error) {\r\n      this.lastHealthCheck = {\r\n        healthy: false,\r\n        error: error instanceof Error ? error.message : 'Unknown error',\r\n        latency: Date.now() - startTime,\r\n        timestamp: new Date(),\r\n      };\r\n      \r\n      this.emit('health_check', this.lastHealthCheck);\r\n      return this.lastHealthCheck;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Provider-specific health check implementation\r\n   */\r\n  protected abstract doHealthCheck(): Promise<HealthCheckResult>;\r\n\r\n  /**\r\n   * Get provider status\r\n   */\r\n  getStatus(): ProviderStatus {\r\n    const queueLength = this.requestMetrics.size;\r\n    const errorRate = this.requestCount > 0 ? this.errorCount / this.requestCount : 0;\r\n    \r\n    return {\r\n      available: this.lastHealthCheck?.healthy ?? false,\r\n      currentLoad: queueLength / 100, // Normalize to 0-1\r\n      queueLength,\r\n      activeRequests: queueLength,\r\n      rateLimitRemaining: this.getRateLimitRemaining(),\r\n      rateLimitReset: this.getRateLimitReset(),\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Get remaining rate limit (override in provider)\r\n   */\r\n  protected getRateLimitRemaining(): number | undefined {\r\n    return undefined;\r\n  }\r\n\r\n  /**\r\n   * Get rate limit reset time (override in provider)\r\n   */\r\n  protected getRateLimitReset(): Date | undefined {\r\n    return undefined;\r\n  }\r\n\r\n  /**\r\n   * Estimate cost for a request\r\n   */\r\n  async estimateCost(request: LLMRequest): Promise<CostEstimate> {\r\n    const model = request.model || this.config.model;\r\n    const pricing = this.capabilities.pricing?.[model];\r\n    \r\n    if (!pricing) {\r\n      return {\r\n        estimatedPromptTokens: 0,\r\n        estimatedCompletionTokens: 0,\r\n        estimatedTotalTokens: 0,\r\n        estimatedCost: {\r\n          prompt: 0,\r\n          completion: 0,\r\n          total: 0,\r\n          currency: 'USD',\r\n        },\r\n        confidence: 0,\r\n      };\r\n    }\r\n    \r\n    // Estimate tokens (simple approximation, providers should override)\r\n    const promptTokens = this.estimateTokens(JSON.stringify(request.messages));\r\n    const completionTokens = request.maxTokens || this.config.maxTokens || 1000;\r\n    \r\n    const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\r\n    const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\r\n    \r\n    return {\r\n      estimatedPromptTokens: promptTokens,\r\n      estimatedCompletionTokens: completionTokens,\r\n      estimatedTotalTokens: promptTokens + completionTokens,\r\n      estimatedCost: {\r\n        prompt: promptCost,\r\n        completion: completionCost,\r\n        total: promptCost + completionCost,\r\n        currency: pricing.currency,\r\n      },\r\n      confidence: 0.7, // 70% confidence in estimation\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Simple token estimation (4 chars = 1 token approximation)\r\n   */\r\n  protected estimateTokens(text: string): number {\r\n    return Math.ceil(text.length / 4);\r\n  }\r\n\r\n  /**\r\n   * Get usage statistics\r\n   */\r\n  async getUsage(period: UsagePeriod = 'day'): Promise<UsageStats> {\r\n    const now = new Date();\r\n    const start = this.getStartDate(now, period);\r\n    \r\n    // In a real implementation, this would query a database\r\n    // For now, return current session stats\r\n    return {\r\n      period: { start, end: now },\r\n      requests: this.requestCount,\r\n      tokens: {\r\n        prompt: Math.floor(this.totalTokens * 0.7), // Estimate\r\n        completion: Math.floor(this.totalTokens * 0.3),\r\n        total: this.totalTokens,\r\n      },\r\n      cost: {\r\n        prompt: this.totalCost * 0.7,\r\n        completion: this.totalCost * 0.3,\r\n        total: this.totalCost,\r\n        currency: 'USD',\r\n      },\r\n      errors: this.errorCount,\r\n      averageLatency: this.calculateAverageLatency(),\r\n      modelBreakdown: {}, // Would need to track per model\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Get start date for period\r\n   */\r\n  private getStartDate(end: Date, period: UsagePeriod): Date {\r\n    const start = new Date(end);\r\n    switch (period) {\r\n      case 'hour':\r\n        start.setHours(start.getHours() - 1);\r\n        break;\r\n      case 'day':\r\n        start.setDate(start.getDate() - 1);\r\n        break;\r\n      case 'week':\r\n        start.setDate(start.getDate() - 7);\r\n        break;\r\n      case 'month':\r\n        start.setMonth(start.getMonth() - 1);\r\n        break;\r\n      case 'all':\r\n        start.setFullYear(2020); // Arbitrary old date\r\n        break;\r\n    }\r\n    return start;\r\n  }\r\n\r\n  /**\r\n   * Calculate average latency\r\n   */\r\n  private calculateAverageLatency(): number {\r\n    if (this.requestMetrics.size === 0) return 0;\r\n    \r\n    let totalLatency = 0;\r\n    let count = 0;\r\n    \r\n    this.requestMetrics.forEach((metrics) => {\r\n      if (metrics.latency) {\r\n        totalLatency += metrics.latency;\r\n        count++;\r\n      }\r\n    });\r\n    \r\n    return count > 0 ? totalLatency / count : 0;\r\n  }\r\n\r\n  /**\r\n   * Track successful request\r\n   */\r\n  protected trackRequest(request: LLMRequest, response: LLMResponse, latency: number): void {\r\n    this.requestCount++;\r\n    this.totalTokens += response.usage.totalTokens;\r\n    \r\n    if (response.cost) {\r\n      this.totalCost += response.cost.totalCost;\r\n    }\r\n    \r\n    // Store metrics (in memory for now)\r\n    const requestId = response.id;\r\n    this.requestMetrics.set(requestId, {\r\n      timestamp: new Date(),\r\n      model: response.model,\r\n      tokens: response.usage.totalTokens,\r\n      cost: response.cost?.totalCost,\r\n      latency,\r\n    });\r\n    \r\n    // Clean up old metrics (keep last 1000)\r\n    if (this.requestMetrics.size > 1000) {\r\n      const oldestKey = this.requestMetrics.keys().next().value;\r\n      this.requestMetrics.delete(oldestKey);\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Track streaming request\r\n   */\r\n  protected trackStreamRequest(\r\n    request: LLMRequest,\r\n    totalTokens: number,\r\n    totalCost: number,\r\n    latency: number\r\n  ): void {\r\n    this.requestCount++;\r\n    this.totalTokens += totalTokens;\r\n    this.totalCost += totalCost;\r\n    \r\n    // Store metrics\r\n    const requestId = `stream-${Date.now()}`;\r\n    this.requestMetrics.set(requestId, {\r\n      timestamp: new Date(),\r\n      model: request.model || this.config.model,\r\n      tokens: totalTokens,\r\n      cost: totalCost,\r\n      latency,\r\n      stream: true,\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Transform errors to provider errors\r\n   */\r\n  protected transformError(error: unknown): LLMProviderError {\r\n    if (error instanceof LLMProviderError) {\r\n      return error;\r\n    }\r\n    \r\n    if (error instanceof Error) {\r\n      // Check for common error patterns\r\n      if (error.message.includes('rate limit')) {\r\n        return new RateLimitError(error.message, this.name);\r\n      }\r\n      \r\n      if (error.message.includes('timeout') || error.message.includes('ETIMEDOUT')) {\r\n        return new LLMProviderError(\r\n          'Request timed out',\r\n          'TIMEOUT',\r\n          this.name,\r\n          undefined,\r\n          true\r\n        );\r\n      }\r\n      \r\n      if (error.message.includes('ECONNREFUSED') || error.message.includes('fetch failed')) {\r\n        return new ProviderUnavailableError(this.name, { originalError: error.message });\r\n      }\r\n    }\r\n    \r\n    return new LLMProviderError(\r\n      error instanceof Error ? error.message : String(error),\r\n      'UNKNOWN',\r\n      this.name,\r\n      undefined,\r\n      true\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Start periodic health checks\r\n   */\r\n  protected startHealthChecks(): void {\r\n    const interval = this.config.cacheTimeout || 300000; // 5 minutes default\r\n    \r\n    this.healthCheckInterval = setInterval(() => {\r\n      this.healthCheck().catch((error) => {\r\n        this.logger.error(`Health check failed for ${this.name}`, error);\r\n      });\r\n    }, interval);\r\n  }\r\n\r\n  /**\r\n   * Clean up resources\r\n   */\r\n  destroy(): void {\r\n    if (this.healthCheckInterval) {\r\n      clearInterval(this.healthCheckInterval);\r\n    }\r\n    \r\n    this.requestMetrics.clear();\r\n    this.removeAllListeners();\r\n    \r\n    this.logger.info(`${this.name} provider destroyed`);\r\n  }\r\n}"],"names":["EventEmitter","circuitBreaker","LLMProviderError","RateLimitError","ProviderUnavailableError","BaseProvider","logger","healthCheckInterval","lastHealthCheck","requestCount","errorCount","totalTokens","totalCost","requestMetrics","Map","config","options","name","threshold","circuitBreakerOptions","timeout","resetTimeout","enableCaching","startHealthChecks","initialize","info","model","temperature","maxTokens","validateConfig","doInitialize","healthCheck","Error","validateModel","undefined","maxAllowed","capabilities","maxOutputTokens","complete","request","startTime","Date","now","response","execute","doComplete","latency","trackRequest","emit","provider","tokens","usage","cost","error","providerError","transformError","streamComplete","supportsStreaming","stream","doStreamComplete","event","trackStreamRequest","type","supportedModels","includes","result","doHealthCheck","timestamp","healthy","message","getStatus","queueLength","size","errorRate","available","currentLoad","activeRequests","rateLimitRemaining","getRateLimitRemaining","rateLimitReset","getRateLimitReset","estimateCost","pricing","estimatedPromptTokens","estimatedCompletionTokens","estimatedTotalTokens","estimatedCost","prompt","completion","total","currency","confidence","promptTokens","estimateTokens","JSON","stringify","messages","completionTokens","promptCost","promptCostPer1k","completionCost","completionCostPer1k","text","Math","ceil","length","getUsage","period","start","getStartDate","end","requests","floor","errors","averageLatency","calculateAverageLatency","modelBreakdown","setHours","getHours","setDate","getDate","setMonth","getMonth","setFullYear","totalLatency","count","forEach","metrics","requestId","id","set","oldestKey","keys","next","value","delete","originalError","String","interval","cacheTimeout","setInterval","catch","destroy","clearInterval","clear","removeAllListeners"],"mappings":"AAKA,SAASA,YAAY,QAAQ,SAAS;AAEtC,SAASC,cAAc,QAAwB,sBAAsB;AACrE,SAeEC,gBAAgB,EAChBC,cAAc,EACdC,wBAAwB,QACnB,aAAa;AAapB,OAAO,MAAeC,qBAAqBL;IAI/BM,OAAgB;IAChBL,eAA+B;IAC/BM,oBAAqC;IACrCC,gBAAoC;IACpCC,eAAe,EAAE;IACjBC,aAAa,EAAE;IACfC,cAAc,EAAE;IAChBC,YAAY,EAAE;IACdC,iBAAmC,IAAIC,MAAM;IAEhDC,OAA0B;IAEjC,YAAYC,OAA4B,CAAE;QACxC,KAAK;QACL,IAAI,CAACV,MAAM,GAAGU,QAAQV,MAAM;QAC5B,IAAI,CAACS,MAAM,GAAGC,QAAQD,MAAM;QAG5B,IAAI,CAACd,cAAc,GAAGA,eAAe,CAAC,IAAI,EAAE,IAAI,CAACgB,IAAI,EAAE,EAAE;YACvDC,WAAWF,QAAQG,qBAAqB,EAAED,aAAa;YACvDE,SAASJ,QAAQG,qBAAqB,EAAEC,WAAW;YACnDC,cAAcL,QAAQG,qBAAqB,EAAEE,gBAAgB;QAC/D;QAGA,IAAI,IAAI,CAACN,MAAM,CAACO,aAAa,EAAE;YAC7B,IAAI,CAACC,iBAAiB;QACxB;IACF;IAKA,MAAMC,aAA4B;QAChC,IAAI,CAAClB,MAAM,CAACmB,IAAI,CAAC,CAAC,aAAa,EAAE,IAAI,CAACR,IAAI,CAAC,SAAS,CAAC,EAAE;YACrDS,OAAO,IAAI,CAACX,MAAM,CAACW,KAAK;YACxBC,aAAa,IAAI,CAACZ,MAAM,CAACY,WAAW;YACpCC,WAAW,IAAI,CAACb,MAAM,CAACa,SAAS;QAClC;QAGA,IAAI,CAACC,cAAc;QAGnB,MAAM,IAAI,CAACC,YAAY;QAGvB,MAAM,IAAI,CAACC,WAAW;IACxB;IAUUF,iBAAuB;QAC/B,IAAI,CAAC,IAAI,CAACd,MAAM,CAACW,KAAK,EAAE;YACtB,MAAM,IAAIM,MAAM,CAAC,sBAAsB,EAAE,IAAI,CAACf,IAAI,CAAC,SAAS,CAAC;QAC/D;QAEA,IAAI,CAAC,IAAI,CAACgB,aAAa,CAAC,IAAI,CAAClB,MAAM,CAACW,KAAK,GAAG;YAC1C,MAAM,IAAIM,MAAM,CAAC,MAAM,EAAE,IAAI,CAACjB,MAAM,CAACW,KAAK,CAAC,qBAAqB,EAAE,IAAI,CAACT,IAAI,CAAC,SAAS,CAAC;QACxF;QAEA,IAAI,IAAI,CAACF,MAAM,CAACY,WAAW,KAAKO,WAAW;YACzC,IAAI,IAAI,CAACnB,MAAM,CAACY,WAAW,GAAG,KAAK,IAAI,CAACZ,MAAM,CAACY,WAAW,GAAG,GAAG;gBAC9D,MAAM,IAAIK,MAAM;YAClB;QACF;QAEA,IAAI,IAAI,CAACjB,MAAM,CAACa,SAAS,KAAKM,WAAW;YACvC,MAAMC,aAAa,IAAI,CAACC,YAAY,CAACC,eAAe,CAAC,IAAI,CAACtB,MAAM,CAACW,KAAK,CAAC,IAAI;YAC3E,IAAI,IAAI,CAACX,MAAM,CAACa,SAAS,GAAGO,YAAY;gBACtC,MAAM,IAAIH,MAAM,CAAC,4BAA4B,EAAEG,WAAW,WAAW,EAAE,IAAI,CAACpB,MAAM,CAACW,KAAK,EAAE;YAC5F;QACF;IACF;IAKA,MAAMY,SAASC,OAAmB,EAAwB;QACxD,MAAMC,YAAYC,KAAKC,GAAG;QAE1B,IAAI;YAEF,MAAMC,WAAW,MAAM,IAAI,CAAC1C,cAAc,CAAC2C,OAAO,CAAC;gBACjD,OAAO,MAAM,IAAI,CAACC,UAAU,CAACN;YAC/B;YAGA,MAAMO,UAAUL,KAAKC,GAAG,KAAKF;YAC7B,IAAI,CAACO,YAAY,CAACR,SAASI,UAAUG;YAGrC,IAAI,CAACE,IAAI,CAAC,YAAY;gBACpBC,UAAU,IAAI,CAAChC,IAAI;gBACnBS,OAAOiB,SAASjB,KAAK;gBACrBoB;gBACAI,QAAQP,SAASQ,KAAK,CAACxC,WAAW;gBAClCyC,MAAMT,SAASS,IAAI,EAAExC;YACvB;YAEA,OAAO+B;QACT,EAAE,OAAOU,OAAO;YACd,IAAI,CAAC3C,UAAU;YAGf,MAAM4C,gBAAgB,IAAI,CAACC,cAAc,CAACF;YAG1C,IAAI,CAACL,IAAI,CAAC,SAAS;gBACjBC,UAAU,IAAI,CAAChC,IAAI;gBACnBoC,OAAOC;gBACPf;YACF;YAEA,MAAMe;QACR;IACF;IAUA,OAAOE,eAAejB,OAAmB,EAAiC;QACxE,MAAMC,YAAYC,KAAKC,GAAG;QAC1B,IAAI/B,cAAc;QAClB,IAAIC,YAAY;QAEhB,IAAI;YAEF,IAAI,CAAC,IAAI,CAACwB,YAAY,CAACqB,iBAAiB,EAAE;gBACxC,MAAM,IAAIvD,iBACR,2BACA,2BACA,IAAI,CAACe,IAAI,EACTiB,WACA;YAEJ;YAGA,MAAMwB,SAAS,MAAM,IAAI,CAACzD,cAAc,CAAC2C,OAAO,CAAC;gBAC/C,OAAO,IAAI,CAACe,gBAAgB,CAACpB;YAC/B;YAGA,WAAW,MAAMqB,SAASF,OAAQ;gBAChC,IAAIE,MAAMT,KAAK,EAAE;oBACfxC,cAAciD,MAAMT,KAAK,CAACxC,WAAW;gBACvC;gBACA,IAAIiD,MAAMR,IAAI,EAAE;oBACdxC,YAAYgD,MAAMR,IAAI,CAACxC,SAAS;gBAClC;gBAEA,MAAMgD;YACR;YAGA,MAAMd,UAAUL,KAAKC,GAAG,KAAKF;YAC7B,IAAI,CAACqB,kBAAkB,CAACtB,SAAS5B,aAAaC,WAAWkC;QAE3D,EAAE,OAAOO,OAAO;YACd,IAAI,CAAC3C,UAAU;YAGf,MAAM4C,gBAAgB,IAAI,CAACC,cAAc,CAACF;YAG1C,MAAM;gBACJS,MAAM;gBACNT,OAAOC;YACT;YAEA,MAAMA;QACR;IACF;IAoBArB,cAAcP,KAAe,EAAW;QACtC,OAAO,IAAI,CAACU,YAAY,CAAC2B,eAAe,CAACC,QAAQ,CAACtC;IACpD;IAKA,MAAMK,cAA0C;QAC9C,MAAMS,YAAYC,KAAKC,GAAG;QAE1B,IAAI;YAEF,MAAMuB,SAAS,MAAM,IAAI,CAACC,aAAa;YAEvC,IAAI,CAAC1D,eAAe,GAAG;gBACrB,GAAGyD,MAAM;gBACTnB,SAASL,KAAKC,GAAG,KAAKF;gBACtB2B,WAAW,IAAI1B;YACjB;YAEA,IAAI,CAACO,IAAI,CAAC,gBAAgB,IAAI,CAACxC,eAAe;YAC9C,OAAO,IAAI,CAACA,eAAe;QAE7B,EAAE,OAAO6C,OAAO;YACd,IAAI,CAAC7C,eAAe,GAAG;gBACrB4D,SAAS;gBACTf,OAAOA,iBAAiBrB,QAAQqB,MAAMgB,OAAO,GAAG;gBAChDvB,SAASL,KAAKC,GAAG,KAAKF;gBACtB2B,WAAW,IAAI1B;YACjB;YAEA,IAAI,CAACO,IAAI,CAAC,gBAAgB,IAAI,CAACxC,eAAe;YAC9C,OAAO,IAAI,CAACA,eAAe;QAC7B;IACF;IAUA8D,YAA4B;QAC1B,MAAMC,cAAc,IAAI,CAAC1D,cAAc,CAAC2D,IAAI;QAC5C,MAAMC,YAAY,IAAI,CAAChE,YAAY,GAAG,IAAI,IAAI,CAACC,UAAU,GAAG,IAAI,CAACD,YAAY,GAAG;QAEhF,OAAO;YACLiE,WAAW,IAAI,CAAClE,eAAe,EAAE4D,WAAW;YAC5CO,aAAaJ,cAAc;YAC3BA;YACAK,gBAAgBL;YAChBM,oBAAoB,IAAI,CAACC,qBAAqB;YAC9CC,gBAAgB,IAAI,CAACC,iBAAiB;QACxC;IACF;IAKUF,wBAA4C;QACpD,OAAO5C;IACT;IAKU8C,oBAAsC;QAC9C,OAAO9C;IACT;IAKA,MAAM+C,aAAa1C,OAAmB,EAAyB;QAC7D,MAAMb,QAAQa,QAAQb,KAAK,IAAI,IAAI,CAACX,MAAM,CAACW,KAAK;QAChD,MAAMwD,UAAU,IAAI,CAAC9C,YAAY,CAAC8C,OAAO,EAAE,CAACxD,MAAM;QAElD,IAAI,CAACwD,SAAS;YACZ,OAAO;gBACLC,uBAAuB;gBACvBC,2BAA2B;gBAC3BC,sBAAsB;gBACtBC,eAAe;oBACbC,QAAQ;oBACRC,YAAY;oBACZC,OAAO;oBACPC,UAAU;gBACZ;gBACAC,YAAY;YACd;QACF;QAGA,MAAMC,eAAe,IAAI,CAACC,cAAc,CAACC,KAAKC,SAAS,CAACxD,QAAQyD,QAAQ;QACxE,MAAMC,mBAAmB1D,QAAQX,SAAS,IAAI,IAAI,CAACb,MAAM,CAACa,SAAS,IAAI;QAEvE,MAAMsE,aAAa,AAACN,eAAe,OAAQV,QAAQiB,eAAe;QAClE,MAAMC,iBAAiB,AAACH,mBAAmB,OAAQf,QAAQmB,mBAAmB;QAE9E,OAAO;YACLlB,uBAAuBS;YACvBR,2BAA2Ba;YAC3BZ,sBAAsBO,eAAeK;YACrCX,eAAe;gBACbC,QAAQW;gBACRV,YAAYY;gBACZX,OAAOS,aAAaE;gBACpBV,UAAUR,QAAQQ,QAAQ;YAC5B;YACAC,YAAY;QACd;IACF;IAKUE,eAAeS,IAAY,EAAU;QAC7C,OAAOC,KAAKC,IAAI,CAACF,KAAKG,MAAM,GAAG;IACjC;IAKA,MAAMC,SAASC,SAAsB,KAAK,EAAuB;QAC/D,MAAMjE,MAAM,IAAID;QAChB,MAAMmE,QAAQ,IAAI,CAACC,YAAY,CAACnE,KAAKiE;QAIrC,OAAO;YACLA,QAAQ;gBAAEC;gBAAOE,KAAKpE;YAAI;YAC1BqE,UAAU,IAAI,CAACtG,YAAY;YAC3ByC,QAAQ;gBACNqC,QAAQgB,KAAKS,KAAK,CAAC,IAAI,CAACrG,WAAW,GAAG;gBACtC6E,YAAYe,KAAKS,KAAK,CAAC,IAAI,CAACrG,WAAW,GAAG;gBAC1C8E,OAAO,IAAI,CAAC9E,WAAW;YACzB;YACAyC,MAAM;gBACJmC,QAAQ,IAAI,CAAC3E,SAAS,GAAG;gBACzB4E,YAAY,IAAI,CAAC5E,SAAS,GAAG;gBAC7B6E,OAAO,IAAI,CAAC7E,SAAS;gBACrB8E,UAAU;YACZ;YACAuB,QAAQ,IAAI,CAACvG,UAAU;YACvBwG,gBAAgB,IAAI,CAACC,uBAAuB;YAC5CC,gBAAgB,CAAC;QACnB;IACF;IAKQP,aAAaC,GAAS,EAAEH,MAAmB,EAAQ;QACzD,MAAMC,QAAQ,IAAInE,KAAKqE;QACvB,OAAQH;YACN,KAAK;gBACHC,MAAMS,QAAQ,CAACT,MAAMU,QAAQ,KAAK;gBAClC;YACF,KAAK;gBACHV,MAAMW,OAAO,CAACX,MAAMY,OAAO,KAAK;gBAChC;YACF,KAAK;gBACHZ,MAAMW,OAAO,CAACX,MAAMY,OAAO,KAAK;gBAChC;YACF,KAAK;gBACHZ,MAAMa,QAAQ,CAACb,MAAMc,QAAQ,KAAK;gBAClC;YACF,KAAK;gBACHd,MAAMe,WAAW,CAAC;gBAClB;QACJ;QACA,OAAOf;IACT;IAKQO,0BAAkC;QACxC,IAAI,IAAI,CAACtG,cAAc,CAAC2D,IAAI,KAAK,GAAG,OAAO;QAE3C,IAAIoD,eAAe;QACnB,IAAIC,QAAQ;QAEZ,IAAI,CAAChH,cAAc,CAACiH,OAAO,CAAC,CAACC;YAC3B,IAAIA,QAAQjF,OAAO,EAAE;gBACnB8E,gBAAgBG,QAAQjF,OAAO;gBAC/B+E;YACF;QACF;QAEA,OAAOA,QAAQ,IAAID,eAAeC,QAAQ;IAC5C;IAKU9E,aAAaR,OAAmB,EAAEI,QAAqB,EAAEG,OAAe,EAAQ;QACxF,IAAI,CAACrC,YAAY;QACjB,IAAI,CAACE,WAAW,IAAIgC,SAASQ,KAAK,CAACxC,WAAW;QAE9C,IAAIgC,SAASS,IAAI,EAAE;YACjB,IAAI,CAACxC,SAAS,IAAI+B,SAASS,IAAI,CAACxC,SAAS;QAC3C;QAGA,MAAMoH,YAAYrF,SAASsF,EAAE;QAC7B,IAAI,CAACpH,cAAc,CAACqH,GAAG,CAACF,WAAW;YACjC7D,WAAW,IAAI1B;YACff,OAAOiB,SAASjB,KAAK;YACrBwB,QAAQP,SAASQ,KAAK,CAACxC,WAAW;YAClCyC,MAAMT,SAASS,IAAI,EAAExC;YACrBkC;QACF;QAGA,IAAI,IAAI,CAACjC,cAAc,CAAC2D,IAAI,GAAG,MAAM;YACnC,MAAM2D,YAAY,IAAI,CAACtH,cAAc,CAACuH,IAAI,GAAGC,IAAI,GAAGC,KAAK;YACzD,IAAI,CAACzH,cAAc,CAAC0H,MAAM,CAACJ;QAC7B;IACF;IAKUtE,mBACRtB,OAAmB,EACnB5B,WAAmB,EACnBC,SAAiB,EACjBkC,OAAe,EACT;QACN,IAAI,CAACrC,YAAY;QACjB,IAAI,CAACE,WAAW,IAAIA;QACpB,IAAI,CAACC,SAAS,IAAIA;QAGlB,MAAMoH,YAAY,CAAC,OAAO,EAAEvF,KAAKC,GAAG,IAAI;QACxC,IAAI,CAAC7B,cAAc,CAACqH,GAAG,CAACF,WAAW;YACjC7D,WAAW,IAAI1B;YACff,OAAOa,QAAQb,KAAK,IAAI,IAAI,CAACX,MAAM,CAACW,KAAK;YACzCwB,QAAQvC;YACRyC,MAAMxC;YACNkC;YACAY,QAAQ;QACV;IACF;IAKUH,eAAeF,KAAc,EAAoB;QACzD,IAAIA,iBAAiBnD,kBAAkB;YACrC,OAAOmD;QACT;QAEA,IAAIA,iBAAiBrB,OAAO;YAE1B,IAAIqB,MAAMgB,OAAO,CAACL,QAAQ,CAAC,eAAe;gBACxC,OAAO,IAAI7D,eAAekD,MAAMgB,OAAO,EAAE,IAAI,CAACpD,IAAI;YACpD;YAEA,IAAIoC,MAAMgB,OAAO,CAACL,QAAQ,CAAC,cAAcX,MAAMgB,OAAO,CAACL,QAAQ,CAAC,cAAc;gBAC5E,OAAO,IAAI9D,iBACT,qBACA,WACA,IAAI,CAACe,IAAI,EACTiB,WACA;YAEJ;YAEA,IAAImB,MAAMgB,OAAO,CAACL,QAAQ,CAAC,mBAAmBX,MAAMgB,OAAO,CAACL,QAAQ,CAAC,iBAAiB;gBACpF,OAAO,IAAI5D,yBAAyB,IAAI,CAACa,IAAI,EAAE;oBAAEuH,eAAenF,MAAMgB,OAAO;gBAAC;YAChF;QACF;QAEA,OAAO,IAAInE,iBACTmD,iBAAiBrB,QAAQqB,MAAMgB,OAAO,GAAGoE,OAAOpF,QAChD,WACA,IAAI,CAACpC,IAAI,EACTiB,WACA;IAEJ;IAKUX,oBAA0B;QAClC,MAAMmH,WAAW,IAAI,CAAC3H,MAAM,CAAC4H,YAAY,IAAI;QAE7C,IAAI,CAACpI,mBAAmB,GAAGqI,YAAY;YACrC,IAAI,CAAC7G,WAAW,GAAG8G,KAAK,CAAC,CAACxF;gBACxB,IAAI,CAAC/C,MAAM,CAAC+C,KAAK,CAAC,CAAC,wBAAwB,EAAE,IAAI,CAACpC,IAAI,EAAE,EAAEoC;YAC5D;QACF,GAAGqF;IACL;IAKAI,UAAgB;QACd,IAAI,IAAI,CAACvI,mBAAmB,EAAE;YAC5BwI,cAAc,IAAI,CAACxI,mBAAmB;QACxC;QAEA,IAAI,CAACM,cAAc,CAACmI,KAAK;QACzB,IAAI,CAACC,kBAAkB;QAEvB,IAAI,CAAC3I,MAAM,CAACmB,IAAI,CAAC,GAAG,IAAI,CAACR,IAAI,CAAC,mBAAmB,CAAC;IACpD;AACF"}