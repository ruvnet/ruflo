{"version":3,"sources":["../../../src/providers/ollama-provider.ts"],"sourcesContent":["/**\r\n * Ollama Provider Implementation\r\n * Supports local models running via Ollama\r\n */\r\n\r\nimport { BaseProvider } from './base-provider.js';\r\nimport {\r\n  LLMProvider,\r\n  LLMModel,\r\n  LLMRequest,\r\n  LLMResponse,\r\n  LLMStreamEvent,\r\n  ModelInfo,\r\n  ProviderCapabilities,\r\n  HealthCheckResult,\r\n  LLMProviderError,\r\n  ProviderUnavailableError,\r\n} from './types.js';\r\n\r\ninterface OllamaGenerateRequest {\r\n  model: string;\r\n  prompt: string;\r\n  system?: string;\r\n  template?: string;\r\n  context?: number[];\r\n  stream?: boolean;\r\n  raw?: boolean;\r\n  format?: 'json';\r\n  options?: {\r\n    temperature?: number;\r\n    top_k?: number;\r\n    top_p?: number;\r\n    num_predict?: number;\r\n    stop?: string[];\r\n    seed?: number;\r\n    num_ctx?: number;\r\n  };\r\n}\r\n\r\ninterface OllamaChatRequest {\r\n  model: string;\r\n  messages: Array<{\r\n    role: 'system' | 'user' | 'assistant';\r\n    content: string;\r\n  }>;\r\n  stream?: boolean;\r\n  format?: 'json';\r\n  options?: OllamaGenerateRequest['options'];\r\n}\r\n\r\ninterface OllamaResponse {\r\n  model: string;\r\n  created_at: string;\r\n  response?: string;\r\n  message?: {\r\n    role: string;\r\n    content: string;\r\n  };\r\n  done: boolean;\r\n  context?: number[];\r\n  total_duration?: number;\r\n  load_duration?: number;\r\n  prompt_eval_count?: number;\r\n  prompt_eval_duration?: number;\r\n  eval_count?: number;\r\n  eval_duration?: number;\r\n}\r\n\r\ninterface OllamaModelInfo {\r\n  name: string;\r\n  modified_at: string;\r\n  size: number;\r\n  digest: string;\r\n  details: {\r\n    format: string;\r\n    family: string;\r\n    families: string[] | null;\r\n    parameter_size: string;\r\n    quantization_level: string;\r\n  };\r\n}\r\n\r\nexport class OllamaProvider extends BaseProvider {\r\n  readonly name: LLMProvider = 'ollama';\r\n  readonly capabilities: ProviderCapabilities = {\r\n    supportedModels: [\r\n      'llama-2-7b',\r\n      'llama-2-13b',\r\n      'llama-2-70b',\r\n      'mistral-7b',\r\n      'mixtral-8x7b',\r\n      'custom-model',\r\n    ],\r\n    maxContextLength: {\r\n      'llama-2-7b': 4096,\r\n      'llama-2-13b': 4096,\r\n      'llama-2-70b': 4096,\r\n      'mistral-7b': 8192,\r\n      'mixtral-8x7b': 32768,\r\n      'custom-model': 4096,\r\n    } as Record<LLMModel, number>,\r\n    maxOutputTokens: {\r\n      'llama-2-7b': 2048,\r\n      'llama-2-13b': 2048,\r\n      'llama-2-70b': 2048,\r\n      'mistral-7b': 4096,\r\n      'mixtral-8x7b': 4096,\r\n      'custom-model': 2048,\r\n    } as Record<LLMModel, number>,\r\n    supportsStreaming: true,\r\n    supportsFunctionCalling: false,\r\n    supportsSystemMessages: true,\r\n    supportsVision: false,\r\n    supportsAudio: false,\r\n    supportsTools: false,\r\n    supportsFineTuning: false,\r\n    supportsEmbeddings: true,\r\n    supportsLogprobs: false,\r\n    supportsBatching: false,\r\n    pricing: {\r\n      // Local models have no API cost\r\n      'llama-2-7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n      'llama-2-13b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n      'llama-2-70b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n      'mistral-7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n      'mixtral-8x7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n      'custom-model': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\r\n    },\r\n  };\r\n\r\n  private baseUrl: string;\r\n  private availableModels: Set<string> = new Set();\r\n\r\n  protected async doInitialize(): Promise<void> {\r\n    this.baseUrl = this.config.apiUrl || 'http://localhost:11434';\r\n    \r\n    // Check if Ollama is running and get available models\r\n    try {\r\n      await this.fetchAvailableModels();\r\n    } catch (error) {\r\n      this.logger.warn('Failed to fetch Ollama models, will retry on first request', error);\r\n    }\r\n  }\r\n\r\n  private async fetchAvailableModels(): Promise<void> {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/tags`);\r\n      if (!response.ok) {\r\n        throw new Error(`Failed to fetch models: ${response.status}`);\r\n      }\r\n      \r\n      const data = await response.json();\r\n      this.availableModels.clear();\r\n      \r\n      if (data.models && Array.isArray(data.models)) {\r\n        data.models.forEach((model: OllamaModelInfo) => {\r\n          this.availableModels.add(model.name);\r\n          // Map common model names\r\n          if (model.name.includes('llama2:7b')) {\r\n            this.availableModels.add('llama-2-7b');\r\n          } else if (model.name.includes('llama2:13b')) {\r\n            this.availableModels.add('llama-2-13b');\r\n          } else if (model.name.includes('llama2:70b')) {\r\n            this.availableModels.add('llama-2-70b');\r\n          } else if (model.name.includes('mistral')) {\r\n            this.availableModels.add('mistral-7b');\r\n          } else if (model.name.includes('mixtral')) {\r\n            this.availableModels.add('mixtral-8x7b');\r\n          }\r\n        });\r\n      }\r\n    } catch (error) {\r\n      throw new ProviderUnavailableError('ollama', {\r\n        message: 'Ollama service is not available',\r\n        details: error,\r\n      });\r\n    }\r\n  }\r\n\r\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\r\n    // Use chat endpoint for multi-turn conversations\r\n    const ollamaRequest: OllamaChatRequest = {\r\n      model: this.mapToOllamaModel(request.model || this.config.model),\r\n      messages: request.messages.map(msg => ({\r\n        role: msg.role === 'system' ? 'system' : msg.role === 'assistant' ? 'assistant' : 'user',\r\n        content: msg.content,\r\n      })),\r\n      stream: false,\r\n      options: {\r\n        temperature: request.temperature ?? this.config.temperature,\r\n        top_k: request.topK ?? this.config.topK,\r\n        top_p: request.topP ?? this.config.topP,\r\n        num_predict: request.maxTokens ?? this.config.maxTokens,\r\n        stop: request.stopSequences ?? this.config.stopSequences,\r\n      },\r\n    };\r\n\r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 120000); // Longer timeout for local models\r\n\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/chat`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify(ollamaRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      clearTimeout(timeout);\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const data: OllamaResponse = await response.json();\r\n      \r\n      // Calculate metrics\r\n      const promptTokens = data.prompt_eval_count || this.estimateTokens(JSON.stringify(request.messages));\r\n      const completionTokens = data.eval_count || this.estimateTokens(data.message?.content || '');\r\n      const totalDuration = data.total_duration ? data.total_duration / 1000000 : 0; // Convert nanoseconds to milliseconds\r\n\r\n      return {\r\n        id: `ollama-${Date.now()}`,\r\n        model: request.model || this.config.model,\r\n        provider: 'ollama',\r\n        content: data.message?.content || '',\r\n        usage: {\r\n          promptTokens,\r\n          completionTokens,\r\n          totalTokens: promptTokens + completionTokens,\r\n        },\r\n        cost: {\r\n          promptCost: 0,\r\n          completionCost: 0,\r\n          totalCost: 0,\r\n          currency: 'USD',\r\n        },\r\n        latency: totalDuration,\r\n        finishReason: data.done ? 'stop' : 'length',\r\n        metadata: {\r\n          loadDuration: data.load_duration,\r\n          promptEvalDuration: data.prompt_eval_duration,\r\n          evalDuration: data.eval_duration,\r\n        },\r\n      };\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      \r\n      // Check if Ollama is running\r\n      if (error instanceof TypeError && error.message.includes('fetch')) {\r\n        throw new ProviderUnavailableError('ollama', {\r\n          message: 'Cannot connect to Ollama. Make sure Ollama is running on ' + this.baseUrl,\r\n        });\r\n      }\r\n      \r\n      throw this.transformError(error);\r\n    }\r\n  }\r\n\r\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\r\n    const ollamaRequest: OllamaChatRequest = {\r\n      model: this.mapToOllamaModel(request.model || this.config.model),\r\n      messages: request.messages.map(msg => ({\r\n        role: msg.role === 'system' ? 'system' : msg.role === 'assistant' ? 'assistant' : 'user',\r\n        content: msg.content,\r\n      })),\r\n      stream: true,\r\n      options: {\r\n        temperature: request.temperature ?? this.config.temperature,\r\n        top_k: request.topK ?? this.config.topK,\r\n        top_p: request.topP ?? this.config.topP,\r\n        num_predict: request.maxTokens ?? this.config.maxTokens,\r\n        stop: request.stopSequences ?? this.config.stopSequences,\r\n      },\r\n    };\r\n\r\n    const controller = new AbortController();\r\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 120000) * 2);\r\n\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/chat`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify(ollamaRequest),\r\n        signal: controller.signal,\r\n      });\r\n\r\n      if (!response.ok) {\r\n        await this.handleErrorResponse(response);\r\n      }\r\n\r\n      const reader = response.body!.getReader();\r\n      const decoder = new TextDecoder();\r\n      let buffer = '';\r\n      let totalContent = '';\r\n      let promptTokens = 0;\r\n      let completionTokens = 0;\r\n\r\n      while (true) {\r\n        const { done, value } = await reader.read();\r\n        if (done) break;\r\n\r\n        buffer += decoder.decode(value, { stream: true });\r\n        const lines = buffer.split('\\n');\r\n        buffer = lines.pop() || '';\r\n\r\n        for (const line of lines) {\r\n          if (line.trim() === '') continue;\r\n          \r\n          try {\r\n            const data: OllamaResponse = JSON.parse(line);\r\n            \r\n            if (data.message?.content) {\r\n              totalContent += data.message.content;\r\n              yield {\r\n                type: 'content',\r\n                delta: { content: data.message.content },\r\n              };\r\n            }\r\n            \r\n            if (data.done) {\r\n              promptTokens = data.prompt_eval_count || this.estimateTokens(JSON.stringify(request.messages));\r\n              completionTokens = data.eval_count || this.estimateTokens(totalContent);\r\n              \r\n              yield {\r\n                type: 'done',\r\n                usage: {\r\n                  promptTokens,\r\n                  completionTokens,\r\n                  totalTokens: promptTokens + completionTokens,\r\n                },\r\n                cost: {\r\n                  promptCost: 0,\r\n                  completionCost: 0,\r\n                  totalCost: 0,\r\n                  currency: 'USD',\r\n                },\r\n              };\r\n            }\r\n          } catch (e) {\r\n            this.logger.warn('Failed to parse Ollama stream chunk', { line, error: e });\r\n          }\r\n        }\r\n      }\r\n    } catch (error) {\r\n      clearTimeout(timeout);\r\n      \r\n      if (error instanceof TypeError && error.message.includes('fetch')) {\r\n        throw new ProviderUnavailableError('ollama', {\r\n          message: 'Cannot connect to Ollama. Make sure Ollama is running on ' + this.baseUrl,\r\n        });\r\n      }\r\n      \r\n      throw this.transformError(error);\r\n    } finally {\r\n      clearTimeout(timeout);\r\n    }\r\n  }\r\n\r\n  async listModels(): Promise<LLMModel[]> {\r\n    // Refresh available models\r\n    await this.fetchAvailableModels();\r\n    \r\n    // Return intersection of supported models and available models\r\n    return this.capabilities.supportedModels.filter(model => \r\n      this.availableModels.has(this.mapToOllamaModel(model))\r\n    );\r\n  }\r\n\r\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\r\n    const ollamaModel = this.mapToOllamaModel(model);\r\n    \r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/show`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n        },\r\n        body: JSON.stringify({ name: ollamaModel }),\r\n      });\r\n\r\n      if (!response.ok) {\r\n        throw new Error('Model not found');\r\n      }\r\n\r\n      const data = await response.json();\r\n      \r\n      return {\r\n        model,\r\n        name: data.name || model,\r\n        description: data.description || this.getModelDescription(model),\r\n        contextLength: this.capabilities.maxContextLength[model] || 4096,\r\n        maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\r\n        supportedFeatures: ['chat', 'completion'],\r\n        pricing: this.capabilities.pricing![model],\r\n        metadata: {\r\n          parameterSize: data.details?.parameter_size,\r\n          quantization: data.details?.quantization_level,\r\n          format: data.details?.format,\r\n        },\r\n      };\r\n    } catch (error) {\r\n      // Fallback to default info\r\n      return {\r\n        model,\r\n        name: model,\r\n        description: this.getModelDescription(model),\r\n        contextLength: this.capabilities.maxContextLength[model] || 4096,\r\n        maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\r\n        supportedFeatures: ['chat', 'completion'],\r\n        pricing: this.capabilities.pricing![model],\r\n      };\r\n    }\r\n  }\r\n\r\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\r\n    try {\r\n      const response = await fetch(`${this.baseUrl}/api/tags`);\r\n      \r\n      if (!response.ok) {\r\n        throw new Error(`Health check failed: ${response.status}`);\r\n      }\r\n\r\n      return {\r\n        healthy: true,\r\n        timestamp: new Date(),\r\n        details: {\r\n          modelsAvailable: this.availableModels.size,\r\n        },\r\n      };\r\n    } catch (error) {\r\n      return {\r\n        healthy: false,\r\n        error: error instanceof Error ? error.message : 'Cannot connect to Ollama',\r\n        timestamp: new Date(),\r\n      };\r\n    }\r\n  }\r\n\r\n  private mapToOllamaModel(model: LLMModel): string {\r\n    const modelMap: Record<string, string> = {\r\n      'llama-2-7b': 'llama2:7b',\r\n      'llama-2-13b': 'llama2:13b',\r\n      'llama-2-70b': 'llama2:70b',\r\n      'mistral-7b': 'mistral:7b',\r\n      'mixtral-8x7b': 'mixtral:8x7b',\r\n      'custom-model': this.config.providerOptions?.customModel || 'llama2:latest',\r\n    };\r\n    return modelMap[model] || model;\r\n  }\r\n\r\n  private getModelDescription(model: LLMModel): string {\r\n    const descriptions: Record<string, string> = {\r\n      'llama-2-7b': 'Llama 2 7B - Efficient open-source model',\r\n      'llama-2-13b': 'Llama 2 13B - Balanced performance model',\r\n      'llama-2-70b': 'Llama 2 70B - Large open-source model',\r\n      'mistral-7b': 'Mistral 7B - Fast and efficient model',\r\n      'mixtral-8x7b': 'Mixtral 8x7B - Mixture of experts model',\r\n      'custom-model': 'Custom local model',\r\n    };\r\n    return descriptions[model] || 'Local language model via Ollama';\r\n  }\r\n\r\n  private async handleErrorResponse(response: Response): Promise<void> {\r\n    const errorText = await response.text();\r\n    let errorData: any;\r\n\r\n    try {\r\n      errorData = JSON.parse(errorText);\r\n    } catch {\r\n      errorData = { error: errorText };\r\n    }\r\n\r\n    const message = errorData.error || 'Unknown error';\r\n\r\n    throw new LLMProviderError(\r\n      message,\r\n      `OLLAMA_${response.status}`,\r\n      'ollama',\r\n      response.status,\r\n      response.status >= 500,\r\n      errorData\r\n    );\r\n  }\r\n}"],"names":["BaseProvider","LLMProviderError","ProviderUnavailableError","OllamaProvider","name","capabilities","supportedModels","maxContextLength","maxOutputTokens","supportsStreaming","supportsFunctionCalling","supportsSystemMessages","supportsVision","supportsAudio","supportsTools","supportsFineTuning","supportsEmbeddings","supportsLogprobs","supportsBatching","pricing","promptCostPer1k","completionCostPer1k","currency","baseUrl","availableModels","Set","doInitialize","config","apiUrl","fetchAvailableModels","error","logger","warn","response","fetch","ok","Error","status","data","json","clear","models","Array","isArray","forEach","model","add","includes","message","details","doComplete","request","ollamaRequest","mapToOllamaModel","messages","map","msg","role","content","stream","options","temperature","top_k","topK","top_p","topP","num_predict","maxTokens","stop","stopSequences","controller","AbortController","timeout","setTimeout","abort","method","headers","body","JSON","stringify","signal","clearTimeout","handleErrorResponse","promptTokens","prompt_eval_count","estimateTokens","completionTokens","eval_count","totalDuration","total_duration","id","Date","now","provider","usage","totalTokens","cost","promptCost","completionCost","totalCost","latency","finishReason","done","metadata","loadDuration","load_duration","promptEvalDuration","prompt_eval_duration","evalDuration","eval_duration","TypeError","transformError","doStreamComplete","reader","getReader","decoder","TextDecoder","buffer","totalContent","value","read","decode","lines","split","pop","line","trim","parse","type","delta","e","listModels","filter","has","getModelInfo","ollamaModel","description","getModelDescription","contextLength","supportedFeatures","parameterSize","parameter_size","quantization","quantization_level","format","doHealthCheck","healthy","timestamp","modelsAvailable","size","modelMap","providerOptions","customModel","descriptions","errorText","text","errorData"],"mappings":"AAKA,SAASA,YAAY,QAAQ,qBAAqB;AAClD,SASEC,gBAAgB,EAChBC,wBAAwB,QACnB,aAAa;AAiEpB,OAAO,MAAMC,uBAAuBH;IACzBI,OAAoB,SAAS;IAC7BC,eAAqC;QAC5CC,iBAAiB;YACf;YACA;YACA;YACA;YACA;YACA;SACD;QACDC,kBAAkB;YAChB,cAAc;YACd,eAAe;YACf,eAAe;YACf,cAAc;YACd,gBAAgB;YAChB,gBAAgB;QAClB;QACAC,iBAAiB;YACf,cAAc;YACd,eAAe;YACf,eAAe;YACf,cAAc;YACd,gBAAgB;YAChB,gBAAgB;QAClB;QACAC,mBAAmB;QACnBC,yBAAyB;QACzBC,wBAAwB;QACxBC,gBAAgB;QAChBC,eAAe;QACfC,eAAe;QACfC,oBAAoB;QACpBC,oBAAoB;QACpBC,kBAAkB;QAClBC,kBAAkB;QAClBC,SAAS;YAEP,cAAc;gBAAEC,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;YAC5E,eAAe;gBAAEF,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;YAC7E,eAAe;gBAAEF,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;YAC7E,cAAc;gBAAEF,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;YAC5E,gBAAgB;gBAAEF,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;YAC9E,gBAAgB;gBAAEF,iBAAiB;gBAAGC,qBAAqB;gBAAGC,UAAU;YAAM;QAChF;IACF,EAAE;IAEMC,QAAgB;IAChBC,kBAA+B,IAAIC,MAAM;IAEjD,MAAgBC,eAA8B;QAC5C,IAAI,CAACH,OAAO,GAAG,IAAI,CAACI,MAAM,CAACC,MAAM,IAAI;QAGrC,IAAI;YACF,MAAM,IAAI,CAACC,oBAAoB;QACjC,EAAE,OAAOC,OAAO;YACd,IAAI,CAACC,MAAM,CAACC,IAAI,CAAC,8DAA8DF;QACjF;IACF;IAEA,MAAcD,uBAAsC;QAClD,IAAI;YACF,MAAMI,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,SAAS,CAAC;YACvD,IAAI,CAACU,SAASE,EAAE,EAAE;gBAChB,MAAM,IAAIC,MAAM,CAAC,wBAAwB,EAAEH,SAASI,MAAM,EAAE;YAC9D;YAEA,MAAMC,OAAO,MAAML,SAASM,IAAI;YAChC,IAAI,CAACf,eAAe,CAACgB,KAAK;YAE1B,IAAIF,KAAKG,MAAM,IAAIC,MAAMC,OAAO,CAACL,KAAKG,MAAM,GAAG;gBAC7CH,KAAKG,MAAM,CAACG,OAAO,CAAC,CAACC;oBACnB,IAAI,CAACrB,eAAe,CAACsB,GAAG,CAACD,MAAMzC,IAAI;oBAEnC,IAAIyC,MAAMzC,IAAI,CAAC2C,QAAQ,CAAC,cAAc;wBACpC,IAAI,CAACvB,eAAe,CAACsB,GAAG,CAAC;oBAC3B,OAAO,IAAID,MAAMzC,IAAI,CAAC2C,QAAQ,CAAC,eAAe;wBAC5C,IAAI,CAACvB,eAAe,CAACsB,GAAG,CAAC;oBAC3B,OAAO,IAAID,MAAMzC,IAAI,CAAC2C,QAAQ,CAAC,eAAe;wBAC5C,IAAI,CAACvB,eAAe,CAACsB,GAAG,CAAC;oBAC3B,OAAO,IAAID,MAAMzC,IAAI,CAAC2C,QAAQ,CAAC,YAAY;wBACzC,IAAI,CAACvB,eAAe,CAACsB,GAAG,CAAC;oBAC3B,OAAO,IAAID,MAAMzC,IAAI,CAAC2C,QAAQ,CAAC,YAAY;wBACzC,IAAI,CAACvB,eAAe,CAACsB,GAAG,CAAC;oBAC3B;gBACF;YACF;QACF,EAAE,OAAOhB,OAAO;YACd,MAAM,IAAI5B,yBAAyB,UAAU;gBAC3C8C,SAAS;gBACTC,SAASnB;YACX;QACF;IACF;IAEA,MAAgBoB,WAAWC,OAAmB,EAAwB;QAEpE,MAAMC,gBAAmC;YACvCP,OAAO,IAAI,CAACQ,gBAAgB,CAACF,QAAQN,KAAK,IAAI,IAAI,CAAClB,MAAM,CAACkB,KAAK;YAC/DS,UAAUH,QAAQG,QAAQ,CAACC,GAAG,CAACC,CAAAA,MAAQ,CAAA;oBACrCC,MAAMD,IAAIC,IAAI,KAAK,WAAW,WAAWD,IAAIC,IAAI,KAAK,cAAc,cAAc;oBAClFC,SAASF,IAAIE,OAAO;gBACtB,CAAA;YACAC,QAAQ;YACRC,SAAS;gBACPC,aAAaV,QAAQU,WAAW,IAAI,IAAI,CAAClC,MAAM,CAACkC,WAAW;gBAC3DC,OAAOX,QAAQY,IAAI,IAAI,IAAI,CAACpC,MAAM,CAACoC,IAAI;gBACvCC,OAAOb,QAAQc,IAAI,IAAI,IAAI,CAACtC,MAAM,CAACsC,IAAI;gBACvCC,aAAaf,QAAQgB,SAAS,IAAI,IAAI,CAACxC,MAAM,CAACwC,SAAS;gBACvDC,MAAMjB,QAAQkB,aAAa,IAAI,IAAI,CAAC1C,MAAM,CAAC0C,aAAa;YAC1D;QACF;QAEA,MAAMC,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,IAAI,CAAC/C,MAAM,CAAC6C,OAAO,IAAI;QAE5E,IAAI;YACF,MAAMvC,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,SAAS,CAAC,EAAE;gBACvDoD,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAAC3B;gBACrB4B,QAAQV,WAAWU,MAAM;YAC3B;YAEAC,aAAaT;YAEb,IAAI,CAACvC,SAASE,EAAE,EAAE;gBAChB,MAAM,IAAI,CAAC+C,mBAAmB,CAACjD;YACjC;YAEA,MAAMK,OAAuB,MAAML,SAASM,IAAI;YAGhD,MAAM4C,eAAe7C,KAAK8C,iBAAiB,IAAI,IAAI,CAACC,cAAc,CAACP,KAAKC,SAAS,CAAC5B,QAAQG,QAAQ;YAClG,MAAMgC,mBAAmBhD,KAAKiD,UAAU,IAAI,IAAI,CAACF,cAAc,CAAC/C,KAAKU,OAAO,EAAEU,WAAW;YACzF,MAAM8B,gBAAgBlD,KAAKmD,cAAc,GAAGnD,KAAKmD,cAAc,GAAG,UAAU;YAE5E,OAAO;gBACLC,IAAI,CAAC,OAAO,EAAEC,KAAKC,GAAG,IAAI;gBAC1B/C,OAAOM,QAAQN,KAAK,IAAI,IAAI,CAAClB,MAAM,CAACkB,KAAK;gBACzCgD,UAAU;gBACVnC,SAASpB,KAAKU,OAAO,EAAEU,WAAW;gBAClCoC,OAAO;oBACLX;oBACAG;oBACAS,aAAaZ,eAAeG;gBAC9B;gBACAU,MAAM;oBACJC,YAAY;oBACZC,gBAAgB;oBAChBC,WAAW;oBACX7E,UAAU;gBACZ;gBACA8E,SAASZ;gBACTa,cAAc/D,KAAKgE,IAAI,GAAG,SAAS;gBACnCC,UAAU;oBACRC,cAAclE,KAAKmE,aAAa;oBAChCC,oBAAoBpE,KAAKqE,oBAAoB;oBAC7CC,cAActE,KAAKuE,aAAa;gBAClC;YACF;QACF,EAAE,OAAO/E,OAAO;YACdmD,aAAaT;YAGb,IAAI1C,iBAAiBgF,aAAahF,MAAMkB,OAAO,CAACD,QAAQ,CAAC,UAAU;gBACjE,MAAM,IAAI7C,yBAAyB,UAAU;oBAC3C8C,SAAS,8DAA8D,IAAI,CAACzB,OAAO;gBACrF;YACF;YAEA,MAAM,IAAI,CAACwF,cAAc,CAACjF;QAC5B;IACF;IAEA,OAAiBkF,iBAAiB7D,OAAmB,EAAiC;QACpF,MAAMC,gBAAmC;YACvCP,OAAO,IAAI,CAACQ,gBAAgB,CAACF,QAAQN,KAAK,IAAI,IAAI,CAAClB,MAAM,CAACkB,KAAK;YAC/DS,UAAUH,QAAQG,QAAQ,CAACC,GAAG,CAACC,CAAAA,MAAQ,CAAA;oBACrCC,MAAMD,IAAIC,IAAI,KAAK,WAAW,WAAWD,IAAIC,IAAI,KAAK,cAAc,cAAc;oBAClFC,SAASF,IAAIE,OAAO;gBACtB,CAAA;YACAC,QAAQ;YACRC,SAAS;gBACPC,aAAaV,QAAQU,WAAW,IAAI,IAAI,CAAClC,MAAM,CAACkC,WAAW;gBAC3DC,OAAOX,QAAQY,IAAI,IAAI,IAAI,CAACpC,MAAM,CAACoC,IAAI;gBACvCC,OAAOb,QAAQc,IAAI,IAAI,IAAI,CAACtC,MAAM,CAACsC,IAAI;gBACvCC,aAAaf,QAAQgB,SAAS,IAAI,IAAI,CAACxC,MAAM,CAACwC,SAAS;gBACvDC,MAAMjB,QAAQkB,aAAa,IAAI,IAAI,CAAC1C,MAAM,CAAC0C,aAAa;YAC1D;QACF;QAEA,MAAMC,aAAa,IAAIC;QACvB,MAAMC,UAAUC,WAAW,IAAMH,WAAWI,KAAK,IAAI,AAAC,CAAA,IAAI,CAAC/C,MAAM,CAAC6C,OAAO,IAAI,MAAK,IAAK;QAEvF,IAAI;YACF,MAAMvC,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,SAAS,CAAC,EAAE;gBACvDoD,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAAC3B;gBACrB4B,QAAQV,WAAWU,MAAM;YAC3B;YAEA,IAAI,CAAC/C,SAASE,EAAE,EAAE;gBAChB,MAAM,IAAI,CAAC+C,mBAAmB,CAACjD;YACjC;YAEA,MAAMgF,SAAShF,SAAS4C,IAAI,CAAEqC,SAAS;YACvC,MAAMC,UAAU,IAAIC;YACpB,IAAIC,SAAS;YACb,IAAIC,eAAe;YACnB,IAAInC,eAAe;YACnB,IAAIG,mBAAmB;YAEvB,MAAO,KAAM;gBACX,MAAM,EAAEgB,IAAI,EAAEiB,KAAK,EAAE,GAAG,MAAMN,OAAOO,IAAI;gBACzC,IAAIlB,MAAM;gBAEVe,UAAUF,QAAQM,MAAM,CAACF,OAAO;oBAAE5D,QAAQ;gBAAK;gBAC/C,MAAM+D,QAAQL,OAAOM,KAAK,CAAC;gBAC3BN,SAASK,MAAME,GAAG,MAAM;gBAExB,KAAK,MAAMC,QAAQH,MAAO;oBACxB,IAAIG,KAAKC,IAAI,OAAO,IAAI;oBAExB,IAAI;wBACF,MAAMxF,OAAuBwC,KAAKiD,KAAK,CAACF;wBAExC,IAAIvF,KAAKU,OAAO,EAAEU,SAAS;4BACzB4D,gBAAgBhF,KAAKU,OAAO,CAACU,OAAO;4BACpC,MAAM;gCACJsE,MAAM;gCACNC,OAAO;oCAAEvE,SAASpB,KAAKU,OAAO,CAACU,OAAO;gCAAC;4BACzC;wBACF;wBAEA,IAAIpB,KAAKgE,IAAI,EAAE;4BACbnB,eAAe7C,KAAK8C,iBAAiB,IAAI,IAAI,CAACC,cAAc,CAACP,KAAKC,SAAS,CAAC5B,QAAQG,QAAQ;4BAC5FgC,mBAAmBhD,KAAKiD,UAAU,IAAI,IAAI,CAACF,cAAc,CAACiC;4BAE1D,MAAM;gCACJU,MAAM;gCACNlC,OAAO;oCACLX;oCACAG;oCACAS,aAAaZ,eAAeG;gCAC9B;gCACAU,MAAM;oCACJC,YAAY;oCACZC,gBAAgB;oCAChBC,WAAW;oCACX7E,UAAU;gCACZ;4BACF;wBACF;oBACF,EAAE,OAAO4G,GAAG;wBACV,IAAI,CAACnG,MAAM,CAACC,IAAI,CAAC,uCAAuC;4BAAE6F;4BAAM/F,OAAOoG;wBAAE;oBAC3E;gBACF;YACF;QACF,EAAE,OAAOpG,OAAO;YACdmD,aAAaT;YAEb,IAAI1C,iBAAiBgF,aAAahF,MAAMkB,OAAO,CAACD,QAAQ,CAAC,UAAU;gBACjE,MAAM,IAAI7C,yBAAyB,UAAU;oBAC3C8C,SAAS,8DAA8D,IAAI,CAACzB,OAAO;gBACrF;YACF;YAEA,MAAM,IAAI,CAACwF,cAAc,CAACjF;QAC5B,SAAU;YACRmD,aAAaT;QACf;IACF;IAEA,MAAM2D,aAAkC;QAEtC,MAAM,IAAI,CAACtG,oBAAoB;QAG/B,OAAO,IAAI,CAACxB,YAAY,CAACC,eAAe,CAAC8H,MAAM,CAACvF,CAAAA,QAC9C,IAAI,CAACrB,eAAe,CAAC6G,GAAG,CAAC,IAAI,CAAChF,gBAAgB,CAACR;IAEnD;IAEA,MAAMyF,aAAazF,KAAe,EAAsB;QACtD,MAAM0F,cAAc,IAAI,CAAClF,gBAAgB,CAACR;QAE1C,IAAI;YACF,MAAMZ,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,SAAS,CAAC,EAAE;gBACvDoD,QAAQ;gBACRC,SAAS;oBACP,gBAAgB;gBAClB;gBACAC,MAAMC,KAAKC,SAAS,CAAC;oBAAE3E,MAAMmI;gBAAY;YAC3C;YAEA,IAAI,CAACtG,SAASE,EAAE,EAAE;gBAChB,MAAM,IAAIC,MAAM;YAClB;YAEA,MAAME,OAAO,MAAML,SAASM,IAAI;YAEhC,OAAO;gBACLM;gBACAzC,MAAMkC,KAAKlC,IAAI,IAAIyC;gBACnB2F,aAAalG,KAAKkG,WAAW,IAAI,IAAI,CAACC,mBAAmB,CAAC5F;gBAC1D6F,eAAe,IAAI,CAACrI,YAAY,CAACE,gBAAgB,CAACsC,MAAM,IAAI;gBAC5DrC,iBAAiB,IAAI,CAACH,YAAY,CAACG,eAAe,CAACqC,MAAM,IAAI;gBAC7D8F,mBAAmB;oBAAC;oBAAQ;iBAAa;gBACzCxH,SAAS,IAAI,CAACd,YAAY,CAACc,OAAO,AAAC,CAAC0B,MAAM;gBAC1C0D,UAAU;oBACRqC,eAAetG,KAAKW,OAAO,EAAE4F;oBAC7BC,cAAcxG,KAAKW,OAAO,EAAE8F;oBAC5BC,QAAQ1G,KAAKW,OAAO,EAAE+F;gBACxB;YACF;QACF,EAAE,OAAOlH,OAAO;YAEd,OAAO;gBACLe;gBACAzC,MAAMyC;gBACN2F,aAAa,IAAI,CAACC,mBAAmB,CAAC5F;gBACtC6F,eAAe,IAAI,CAACrI,YAAY,CAACE,gBAAgB,CAACsC,MAAM,IAAI;gBAC5DrC,iBAAiB,IAAI,CAACH,YAAY,CAACG,eAAe,CAACqC,MAAM,IAAI;gBAC7D8F,mBAAmB;oBAAC;oBAAQ;iBAAa;gBACzCxH,SAAS,IAAI,CAACd,YAAY,CAACc,OAAO,AAAC,CAAC0B,MAAM;YAC5C;QACF;IACF;IAEA,MAAgBoG,gBAA4C;QAC1D,IAAI;YACF,MAAMhH,WAAW,MAAMC,MAAM,GAAG,IAAI,CAACX,OAAO,CAAC,SAAS,CAAC;YAEvD,IAAI,CAACU,SAASE,EAAE,EAAE;gBAChB,MAAM,IAAIC,MAAM,CAAC,qBAAqB,EAAEH,SAASI,MAAM,EAAE;YAC3D;YAEA,OAAO;gBACL6G,SAAS;gBACTC,WAAW,IAAIxD;gBACf1C,SAAS;oBACPmG,iBAAiB,IAAI,CAAC5H,eAAe,CAAC6H,IAAI;gBAC5C;YACF;QACF,EAAE,OAAOvH,OAAO;YACd,OAAO;gBACLoH,SAAS;gBACTpH,OAAOA,iBAAiBM,QAAQN,MAAMkB,OAAO,GAAG;gBAChDmG,WAAW,IAAIxD;YACjB;QACF;IACF;IAEQtC,iBAAiBR,KAAe,EAAU;QAChD,MAAMyG,WAAmC;YACvC,cAAc;YACd,eAAe;YACf,eAAe;YACf,cAAc;YACd,gBAAgB;YAChB,gBAAgB,IAAI,CAAC3H,MAAM,CAAC4H,eAAe,EAAEC,eAAe;QAC9D;QACA,OAAOF,QAAQ,CAACzG,MAAM,IAAIA;IAC5B;IAEQ4F,oBAAoB5F,KAAe,EAAU;QACnD,MAAM4G,eAAuC;YAC3C,cAAc;YACd,eAAe;YACf,eAAe;YACf,cAAc;YACd,gBAAgB;YAChB,gBAAgB;QAClB;QACA,OAAOA,YAAY,CAAC5G,MAAM,IAAI;IAChC;IAEA,MAAcqC,oBAAoBjD,QAAkB,EAAiB;QACnE,MAAMyH,YAAY,MAAMzH,SAAS0H,IAAI;QACrC,IAAIC;QAEJ,IAAI;YACFA,YAAY9E,KAAKiD,KAAK,CAAC2B;QACzB,EAAE,OAAM;YACNE,YAAY;gBAAE9H,OAAO4H;YAAU;QACjC;QAEA,MAAM1G,UAAU4G,UAAU9H,KAAK,IAAI;QAEnC,MAAM,IAAI7B,iBACR+C,SACA,CAAC,OAAO,EAAEf,SAASI,MAAM,EAAE,EAC3B,UACAJ,SAASI,MAAM,EACfJ,SAASI,MAAM,IAAI,KACnBuH;IAEJ;AACF"}