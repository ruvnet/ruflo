{"version":3,"sources":["../../../src/providers/utils.ts"],"sourcesContent":["/**\r\n * Utility functions for multi-LLM provider system\r\n */\r\n\r\nimport { ILogger } from '../core/logger.js';\r\nimport { ConfigManager } from '../config/config-manager.js';\r\nimport { ProviderManager, ProviderManagerConfig } from './provider-manager.js';\r\nimport { LLMProvider, LLMProviderConfig, FallbackStrategy } from './types.js';\r\n\r\n/**\r\n * Create a provider manager with default configuration\r\n */\r\nexport function createProviderManager(\r\n  logger: ILogger,\r\n  configManager: ConfigManager,\r\n  customConfig?: Partial<ProviderManagerConfig>\r\n): ProviderManager {\r\n  const defaultConfig = getDefaultProviderConfig();\r\n  const config = { ...defaultConfig, ...customConfig };\r\n  \r\n  // Load provider configs from environment\r\n  config.providers = loadProviderConfigs(config.providers);\r\n  \r\n  return new ProviderManager(logger, configManager, config);\r\n}\r\n\r\n/**\r\n * Get default provider configuration\r\n */\r\nexport function getDefaultProviderConfig(): ProviderManagerConfig {\r\n  const defaultProvider = (process.env.DEFAULT_LLM_PROVIDER as LLMProvider) || 'anthropic';\r\n  \r\n  return {\r\n    defaultProvider,\r\n    providers: {\r\n      anthropic: {\r\n        provider: 'anthropic',\r\n        apiKey: process.env.ANTHROPIC_API_KEY,\r\n        model: 'claude-3-sonnet-20240229',\r\n        temperature: 0.7,\r\n        maxTokens: 4096,\r\n        enableStreaming: true,\r\n        enableCaching: true,\r\n        timeout: 60000,\r\n        retryAttempts: 3,\r\n      },\r\n      openai: {\r\n        provider: 'openai',\r\n        apiKey: process.env.OPENAI_API_KEY,\r\n        model: 'gpt-4-turbo-preview',\r\n        temperature: 0.7,\r\n        maxTokens: 4096,\r\n        enableStreaming: true,\r\n        enableCaching: true,\r\n        timeout: 60000,\r\n        retryAttempts: 3,\r\n      },\r\n      google: {\r\n        provider: 'google',\r\n        apiKey: process.env.GOOGLE_AI_API_KEY,\r\n        model: 'gemini-pro',\r\n        temperature: 0.7,\r\n        maxTokens: 2048,\r\n        enableStreaming: true,\r\n        enableCaching: true,\r\n        timeout: 60000,\r\n        retryAttempts: 3,\r\n      },\r\n      cohere: {\r\n        provider: 'cohere',\r\n        apiKey: process.env.COHERE_API_KEY,\r\n        model: 'command',\r\n        temperature: 0.7,\r\n        maxTokens: 4096,\r\n        enableStreaming: true,\r\n        enableCaching: true,\r\n        timeout: 60000,\r\n        retryAttempts: 3,\r\n      },\r\n      ollama: {\r\n        provider: 'ollama',\r\n        apiUrl: process.env.OLLAMA_API_URL || 'http://localhost:11434',\r\n        model: 'llama-2-7b',\r\n        temperature: 0.7,\r\n        maxTokens: 2048,\r\n        enableStreaming: true,\r\n        enableCaching: false,\r\n        timeout: 120000, // Longer timeout for local models\r\n        retryAttempts: 2,\r\n      },\r\n    },\r\n    fallbackStrategy: getDefaultFallbackStrategy(),\r\n    loadBalancing: {\r\n      enabled: false,\r\n      strategy: 'round-robin',\r\n    },\r\n    costOptimization: {\r\n      enabled: true,\r\n      maxCostPerRequest: 1.0, // $1 max per request\r\n      preferredProviders: ['anthropic', 'openai'],\r\n    },\r\n    caching: {\r\n      enabled: true,\r\n      ttl: 3600, // 1 hour\r\n      maxSize: 100, // 100MB\r\n      strategy: 'lru',\r\n    },\r\n    monitoring: {\r\n      enabled: true,\r\n      metricsInterval: 60000, // 1 minute\r\n    },\r\n  };\r\n}\r\n\r\n/**\r\n * Get default fallback strategy\r\n */\r\nfunction getDefaultFallbackStrategy(): FallbackStrategy {\r\n  return {\r\n    name: 'default',\r\n    enabled: true,\r\n    maxAttempts: 3,\r\n    rules: [\r\n      {\r\n        condition: 'rate_limit',\r\n        fallbackProviders: ['openai', 'google', 'cohere', 'ollama'],\r\n        retryOriginal: true,\r\n        retryDelay: 60000, // 1 minute\r\n      },\r\n      {\r\n        condition: 'unavailable',\r\n        fallbackProviders: ['openai', 'google', 'anthropic', 'cohere'],\r\n        retryOriginal: true,\r\n        retryDelay: 30000, // 30 seconds\r\n      },\r\n      {\r\n        condition: 'timeout',\r\n        fallbackProviders: ['anthropic', 'openai', 'cohere'],\r\n        retryOriginal: false,\r\n      },\r\n      {\r\n        condition: 'cost',\r\n        fallbackProviders: ['ollama', 'cohere', 'google'],\r\n        retryOriginal: false,\r\n      },\r\n      {\r\n        condition: 'error',\r\n        errorCodes: ['AUTHENTICATION', 'MODEL_NOT_FOUND'],\r\n        fallbackProviders: [],\r\n        retryOriginal: false, // Don't retry auth errors\r\n      },\r\n    ],\r\n  };\r\n}\r\n\r\n/**\r\n * Load provider configurations from environment variables\r\n */\r\nfunction loadProviderConfigs(\r\n  configs: Record<LLMProvider, LLMProviderConfig>\r\n): Record<LLMProvider, LLMProviderConfig> {\r\n  const loaded = { ...configs };\r\n  \r\n  // Override with environment variables if present\r\n  for (const [provider, config] of Object.entries(loaded)) {\r\n    const envPrefix = `${provider.toUpperCase()}_`;\r\n    \r\n    // Check for provider-specific overrides\r\n    if (process.env[`${envPrefix}MODEL`]) {\r\n      config.model = process.env[`${envPrefix}MODEL`] as any;\r\n    }\r\n    if (process.env[`${envPrefix}TEMPERATURE`]) {\r\n      config.temperature = parseFloat(process.env[`${envPrefix}TEMPERATURE`]);\r\n    }\r\n    if (process.env[`${envPrefix}MAX_TOKENS`]) {\r\n      config.maxTokens = parseInt(process.env[`${envPrefix}MAX_TOKENS`], 10);\r\n    }\r\n    if (process.env[`${envPrefix}API_URL`]) {\r\n      config.apiUrl = process.env[`${envPrefix}API_URL`];\r\n    }\r\n  }\r\n  \r\n  return loaded;\r\n}\r\n\r\n/**\r\n * Validate provider configuration\r\n */\r\nexport function validateProviderConfig(config: LLMProviderConfig): string[] {\r\n  const errors: string[] = [];\r\n  \r\n  if (!config.provider) {\r\n    errors.push('Provider name is required');\r\n  }\r\n  \r\n  if (!config.model) {\r\n    errors.push('Model is required');\r\n  }\r\n  \r\n  if (config.temperature !== undefined) {\r\n    if (config.temperature < 0 || config.temperature > 2) {\r\n      errors.push('Temperature must be between 0 and 2');\r\n    }\r\n  }\r\n  \r\n  if (config.maxTokens !== undefined) {\r\n    if (config.maxTokens < 1 || config.maxTokens > 100000) {\r\n      errors.push('Max tokens must be between 1 and 100000');\r\n    }\r\n  }\r\n  \r\n  if (config.topP !== undefined) {\r\n    if (config.topP < 0 || config.topP > 1) {\r\n      errors.push('Top-p must be between 0 and 1');\r\n    }\r\n  }\r\n  \r\n  if (config.timeout !== undefined) {\r\n    if (config.timeout < 1000 || config.timeout > 600000) {\r\n      errors.push('Timeout must be between 1000ms and 600000ms');\r\n    }\r\n  }\r\n  \r\n  return errors;\r\n}\r\n\r\n/**\r\n * Get model recommendations based on use case\r\n */\r\nexport function getModelRecommendations(useCase: string): {\r\n  provider: LLMProvider;\r\n  model: string;\r\n  reasoning: string;\r\n}[] {\r\n  const recommendations: Record<string, any[]> = {\r\n    'code-generation': [\r\n      {\r\n        provider: 'anthropic',\r\n        model: 'claude-3-opus-20240229',\r\n        reasoning: 'Best for complex code generation with high accuracy',\r\n      },\r\n      {\r\n        provider: 'openai',\r\n        model: 'gpt-4-turbo-preview',\r\n        reasoning: 'Excellent code generation with function calling support',\r\n      },\r\n    ],\r\n    'chat': [\r\n      {\r\n        provider: 'anthropic',\r\n        model: 'claude-3-sonnet-20240229',\r\n        reasoning: 'Balanced performance for conversational AI',\r\n      },\r\n      {\r\n        provider: 'openai',\r\n        model: 'gpt-3.5-turbo',\r\n        reasoning: 'Fast and cost-effective for chat applications',\r\n      },\r\n    ],\r\n    'analysis': [\r\n      {\r\n        provider: 'anthropic',\r\n        model: 'claude-3-opus-20240229',\r\n        reasoning: 'Excellent for deep analysis and reasoning',\r\n      },\r\n      {\r\n        provider: 'google',\r\n        model: 'gemini-pro',\r\n        reasoning: 'Good for data analysis with multimodal support',\r\n      },\r\n    ],\r\n    'local': [\r\n      {\r\n        provider: 'ollama',\r\n        model: 'llama-2-13b',\r\n        reasoning: 'Good balance of performance and resource usage for local deployment',\r\n      },\r\n      {\r\n        provider: 'ollama',\r\n        model: 'mistral-7b',\r\n        reasoning: 'Fast local model with good performance',\r\n      },\r\n    ],\r\n    'budget': [\r\n      {\r\n        provider: 'ollama',\r\n        model: 'llama-2-7b',\r\n        reasoning: 'Free local model with no API costs',\r\n      },\r\n      {\r\n        provider: 'google',\r\n        model: 'gemini-pro',\r\n        reasoning: 'Very cost-effective cloud model',\r\n      },\r\n    ],\r\n  };\r\n  \r\n  return recommendations[useCase] || recommendations['chat'];\r\n}\r\n\r\n/**\r\n * Calculate estimated monthly cost based on usage\r\n */\r\nexport function estimateMonthlyCost(\r\n  provider: LLMProvider,\r\n  model: string,\r\n  estimatedRequests: number,\r\n  avgTokensPerRequest: number\r\n): {\r\n  promptCost: number;\r\n  completionCost: number;\r\n  totalCost: number;\r\n  currency: string;\r\n} {\r\n  // Get pricing from provider capabilities\r\n  // This is a simplified calculation\r\n  const pricing = getPricing(provider, model);\r\n  \r\n  if (!pricing) {\r\n    return {\r\n      promptCost: 0,\r\n      completionCost: 0,\r\n      totalCost: 0,\r\n      currency: 'USD',\r\n    };\r\n  }\r\n  \r\n  const promptTokens = avgTokensPerRequest * 0.7; // Assume 70% prompt\r\n  const completionTokens = avgTokensPerRequest * 0.3; // Assume 30% completion\r\n  \r\n  const promptCost = (promptTokens * estimatedRequests / 1000) * pricing.promptCostPer1k;\r\n  const completionCost = (completionTokens * estimatedRequests / 1000) * pricing.completionCostPer1k;\r\n  \r\n  return {\r\n    promptCost,\r\n    completionCost,\r\n    totalCost: promptCost + completionCost,\r\n    currency: pricing.currency,\r\n  };\r\n}\r\n\r\n/**\r\n * Get pricing for a specific provider and model\r\n */\r\nfunction getPricing(provider: LLMProvider, model: string): {\r\n  promptCostPer1k: number;\r\n  completionCostPer1k: number;\r\n  currency: string;\r\n} | null {\r\n  // This would typically come from provider capabilities\r\n  // Simplified for example\r\n  const pricingData: Record<string, any> = {\r\n    'anthropic:claude-3-opus-20240229': {\r\n      promptCostPer1k: 0.015,\r\n      completionCostPer1k: 0.075,\r\n      currency: 'USD',\r\n    },\r\n    'openai:gpt-4-turbo-preview': {\r\n      promptCostPer1k: 0.01,\r\n      completionCostPer1k: 0.03,\r\n      currency: 'USD',\r\n    },\r\n    'google:gemini-pro': {\r\n      promptCostPer1k: 0.00025,\r\n      completionCostPer1k: 0.0005,\r\n      currency: 'USD',\r\n    },\r\n    'ollama:llama-2-7b': {\r\n      promptCostPer1k: 0,\r\n      completionCostPer1k: 0,\r\n      currency: 'USD',\r\n    },\r\n  };\r\n  \r\n  return pricingData[`${provider}:${model}`] || null;\r\n}"],"names":["ProviderManager","createProviderManager","logger","configManager","customConfig","defaultConfig","getDefaultProviderConfig","config","providers","loadProviderConfigs","defaultProvider","process","env","DEFAULT_LLM_PROVIDER","anthropic","provider","apiKey","ANTHROPIC_API_KEY","model","temperature","maxTokens","enableStreaming","enableCaching","timeout","retryAttempts","openai","OPENAI_API_KEY","google","GOOGLE_AI_API_KEY","cohere","COHERE_API_KEY","ollama","apiUrl","OLLAMA_API_URL","fallbackStrategy","getDefaultFallbackStrategy","loadBalancing","enabled","strategy","costOptimization","maxCostPerRequest","preferredProviders","caching","ttl","maxSize","monitoring","metricsInterval","name","maxAttempts","rules","condition","fallbackProviders","retryOriginal","retryDelay","errorCodes","configs","loaded","Object","entries","envPrefix","toUpperCase","parseFloat","parseInt","validateProviderConfig","errors","push","undefined","topP","getModelRecommendations","useCase","recommendations","reasoning","estimateMonthlyCost","estimatedRequests","avgTokensPerRequest","pricing","getPricing","promptCost","completionCost","totalCost","currency","promptTokens","completionTokens","promptCostPer1k","completionCostPer1k","pricingData"],"mappings":"AAMA,SAASA,eAAe,QAA+B,wBAAwB;AAM/E,OAAO,SAASC,sBACdC,MAAe,EACfC,aAA4B,EAC5BC,YAA6C;IAE7C,MAAMC,gBAAgBC;IACtB,MAAMC,SAAS;QAAE,GAAGF,aAAa;QAAE,GAAGD,YAAY;IAAC;IAGnDG,OAAOC,SAAS,GAAGC,oBAAoBF,OAAOC,SAAS;IAEvD,OAAO,IAAIR,gBAAgBE,QAAQC,eAAeI;AACpD;AAKA,OAAO,SAASD;IACd,MAAMI,kBAAkB,AAACC,QAAQC,GAAG,CAACC,oBAAoB,IAAoB;IAE7E,OAAO;QACLH;QACAF,WAAW;YACTM,WAAW;gBACTC,UAAU;gBACVC,QAAQL,QAAQC,GAAG,CAACK,iBAAiB;gBACrCC,OAAO;gBACPC,aAAa;gBACbC,WAAW;gBACXC,iBAAiB;gBACjBC,eAAe;gBACfC,SAAS;gBACTC,eAAe;YACjB;YACAC,QAAQ;gBACNV,UAAU;gBACVC,QAAQL,QAAQC,GAAG,CAACc,cAAc;gBAClCR,OAAO;gBACPC,aAAa;gBACbC,WAAW;gBACXC,iBAAiB;gBACjBC,eAAe;gBACfC,SAAS;gBACTC,eAAe;YACjB;YACAG,QAAQ;gBACNZ,UAAU;gBACVC,QAAQL,QAAQC,GAAG,CAACgB,iBAAiB;gBACrCV,OAAO;gBACPC,aAAa;gBACbC,WAAW;gBACXC,iBAAiB;gBACjBC,eAAe;gBACfC,SAAS;gBACTC,eAAe;YACjB;YACAK,QAAQ;gBACNd,UAAU;gBACVC,QAAQL,QAAQC,GAAG,CAACkB,cAAc;gBAClCZ,OAAO;gBACPC,aAAa;gBACbC,WAAW;gBACXC,iBAAiB;gBACjBC,eAAe;gBACfC,SAAS;gBACTC,eAAe;YACjB;YACAO,QAAQ;gBACNhB,UAAU;gBACViB,QAAQrB,QAAQC,GAAG,CAACqB,cAAc,IAAI;gBACtCf,OAAO;gBACPC,aAAa;gBACbC,WAAW;gBACXC,iBAAiB;gBACjBC,eAAe;gBACfC,SAAS;gBACTC,eAAe;YACjB;QACF;QACAU,kBAAkBC;QAClBC,eAAe;YACbC,SAAS;YACTC,UAAU;QACZ;QACAC,kBAAkB;YAChBF,SAAS;YACTG,mBAAmB;YACnBC,oBAAoB;gBAAC;gBAAa;aAAS;QAC7C;QACAC,SAAS;YACPL,SAAS;YACTM,KAAK;YACLC,SAAS;YACTN,UAAU;QACZ;QACAO,YAAY;YACVR,SAAS;YACTS,iBAAiB;QACnB;IACF;AACF;AAKA,SAASX;IACP,OAAO;QACLY,MAAM;QACNV,SAAS;QACTW,aAAa;QACbC,OAAO;YACL;gBACEC,WAAW;gBACXC,mBAAmB;oBAAC;oBAAU;oBAAU;oBAAU;iBAAS;gBAC3DC,eAAe;gBACfC,YAAY;YACd;YACA;gBACEH,WAAW;gBACXC,mBAAmB;oBAAC;oBAAU;oBAAU;oBAAa;iBAAS;gBAC9DC,eAAe;gBACfC,YAAY;YACd;YACA;gBACEH,WAAW;gBACXC,mBAAmB;oBAAC;oBAAa;oBAAU;iBAAS;gBACpDC,eAAe;YACjB;YACA;gBACEF,WAAW;gBACXC,mBAAmB;oBAAC;oBAAU;oBAAU;iBAAS;gBACjDC,eAAe;YACjB;YACA;gBACEF,WAAW;gBACXI,YAAY;oBAAC;oBAAkB;iBAAkB;gBACjDH,mBAAmB,EAAE;gBACrBC,eAAe;YACjB;SACD;IACH;AACF;AAKA,SAAS3C,oBACP8C,OAA+C;IAE/C,MAAMC,SAAS;QAAE,GAAGD,OAAO;IAAC;IAG5B,KAAK,MAAM,CAACxC,UAAUR,OAAO,IAAIkD,OAAOC,OAAO,CAACF,QAAS;QACvD,MAAMG,YAAY,GAAG5C,SAAS6C,WAAW,GAAG,CAAC,CAAC;QAG9C,IAAIjD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,KAAK,CAAC,CAAC,EAAE;YACpCpD,OAAOW,KAAK,GAAGP,QAAQC,GAAG,CAAC,GAAG+C,UAAU,KAAK,CAAC,CAAC;QACjD;QACA,IAAIhD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,WAAW,CAAC,CAAC,EAAE;YAC1CpD,OAAOY,WAAW,GAAG0C,WAAWlD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,WAAW,CAAC,CAAC;QACxE;QACA,IAAIhD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,UAAU,CAAC,CAAC,EAAE;YACzCpD,OAAOa,SAAS,GAAG0C,SAASnD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,UAAU,CAAC,CAAC,EAAE;QACrE;QACA,IAAIhD,QAAQC,GAAG,CAAC,GAAG+C,UAAU,OAAO,CAAC,CAAC,EAAE;YACtCpD,OAAOyB,MAAM,GAAGrB,QAAQC,GAAG,CAAC,GAAG+C,UAAU,OAAO,CAAC,CAAC;QACpD;IACF;IAEA,OAAOH;AACT;AAKA,OAAO,SAASO,uBAAuBxD,MAAyB;IAC9D,MAAMyD,SAAmB,EAAE;IAE3B,IAAI,CAACzD,OAAOQ,QAAQ,EAAE;QACpBiD,OAAOC,IAAI,CAAC;IACd;IAEA,IAAI,CAAC1D,OAAOW,KAAK,EAAE;QACjB8C,OAAOC,IAAI,CAAC;IACd;IAEA,IAAI1D,OAAOY,WAAW,KAAK+C,WAAW;QACpC,IAAI3D,OAAOY,WAAW,GAAG,KAAKZ,OAAOY,WAAW,GAAG,GAAG;YACpD6C,OAAOC,IAAI,CAAC;QACd;IACF;IAEA,IAAI1D,OAAOa,SAAS,KAAK8C,WAAW;QAClC,IAAI3D,OAAOa,SAAS,GAAG,KAAKb,OAAOa,SAAS,GAAG,QAAQ;YACrD4C,OAAOC,IAAI,CAAC;QACd;IACF;IAEA,IAAI1D,OAAO4D,IAAI,KAAKD,WAAW;QAC7B,IAAI3D,OAAO4D,IAAI,GAAG,KAAK5D,OAAO4D,IAAI,GAAG,GAAG;YACtCH,OAAOC,IAAI,CAAC;QACd;IACF;IAEA,IAAI1D,OAAOgB,OAAO,KAAK2C,WAAW;QAChC,IAAI3D,OAAOgB,OAAO,GAAG,QAAQhB,OAAOgB,OAAO,GAAG,QAAQ;YACpDyC,OAAOC,IAAI,CAAC;QACd;IACF;IAEA,OAAOD;AACT;AAKA,OAAO,SAASI,wBAAwBC,OAAe;IAKrD,MAAMC,kBAAyC;QAC7C,mBAAmB;YACjB;gBACEvD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;YACA;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;SACD;QACD,QAAQ;YACN;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;YACA;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;SACD;QACD,YAAY;YACV;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;YACA;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;SACD;QACD,SAAS;YACP;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;YACA;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;SACD;QACD,UAAU;YACR;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;YACA;gBACExD,UAAU;gBACVG,OAAO;gBACPqD,WAAW;YACb;SACD;IACH;IAEA,OAAOD,eAAe,CAACD,QAAQ,IAAIC,eAAe,CAAC,OAAO;AAC5D;AAKA,OAAO,SAASE,oBACdzD,QAAqB,EACrBG,KAAa,EACbuD,iBAAyB,EACzBC,mBAA2B;IAS3B,MAAMC,UAAUC,WAAW7D,UAAUG;IAErC,IAAI,CAACyD,SAAS;QACZ,OAAO;YACLE,YAAY;YACZC,gBAAgB;YAChBC,WAAW;YACXC,UAAU;QACZ;IACF;IAEA,MAAMC,eAAeP,sBAAsB;IAC3C,MAAMQ,mBAAmBR,sBAAsB;IAE/C,MAAMG,aAAa,AAACI,eAAeR,oBAAoB,OAAQE,QAAQQ,eAAe;IACtF,MAAML,iBAAiB,AAACI,mBAAmBT,oBAAoB,OAAQE,QAAQS,mBAAmB;IAElG,OAAO;QACLP;QACAC;QACAC,WAAWF,aAAaC;QACxBE,UAAUL,QAAQK,QAAQ;IAC5B;AACF;AAKA,SAASJ,WAAW7D,QAAqB,EAAEG,KAAa;IAOtD,MAAMmE,cAAmC;QACvC,oCAAoC;YAClCF,iBAAiB;YACjBC,qBAAqB;YACrBJ,UAAU;QACZ;QACA,8BAA8B;YAC5BG,iBAAiB;YACjBC,qBAAqB;YACrBJ,UAAU;QACZ;QACA,qBAAqB;YACnBG,iBAAiB;YACjBC,qBAAqB;YACrBJ,UAAU;QACZ;QACA,qBAAqB;YACnBG,iBAAiB;YACjBC,qBAAqB;YACrBJ,UAAU;QACZ;IACF;IAEA,OAAOK,WAAW,CAAC,GAAGtE,SAAS,CAAC,EAAEG,OAAO,CAAC,IAAI;AAChD"}