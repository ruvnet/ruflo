{"version":3,"sources":["../../../../src/services/agentic-flow-hooks/llm-hooks.ts"],"sourcesContent":["/**\r\n * LLM-specific hooks for agentic-flow integration\r\n * \r\n * Provides pre/post operation hooks for all LLM calls with\r\n * memory persistence and performance optimization.\r\n */\r\n\r\nimport { agenticHookManager } from './hook-manager.js';\r\nimport type {\r\n  AgenticHookContext,\r\n  HookHandlerResult,\r\n  LLMHookPayload,\r\n  LLMMetrics,\r\n  Pattern,\r\n  SideEffect,\r\n} from './types.js';\r\n\r\n// ===== Pre-LLM Call Hook =====\r\n\r\nexport const preLLMCallHook = {\r\n  id: 'agentic-pre-llm-call',\r\n  type: 'pre-llm-call' as const,\r\n  priority: 100,\r\n  handler: async (\r\n    payload: LLMHookPayload,\r\n    context: AgenticHookContext\r\n  ): Promise<HookHandlerResult> => {\r\n    const { provider, model, operation, request } = payload;\r\n    \r\n    // Check memory for similar requests\r\n    const cacheKey = generateCacheKey(provider, model, request);\r\n    const cached = await checkMemoryCache(cacheKey, context);\r\n    \r\n    if (cached) {\r\n      return {\r\n        continue: false, // Skip LLM call\r\n        modified: true,\r\n        payload: {\r\n          ...payload,\r\n          response: cached.response,\r\n          metrics: {\r\n            ...cached.metrics,\r\n            cacheHit: true,\r\n          },\r\n        },\r\n        sideEffects: [\r\n          {\r\n            type: 'metric',\r\n            action: 'increment',\r\n            data: { name: 'llm.cache.hits' },\r\n          },\r\n        ],\r\n      };\r\n    }\r\n    \r\n    // Load provider-specific optimizations\r\n    const optimizations = await loadProviderOptimizations(provider, context);\r\n    \r\n    // Apply request optimizations\r\n    const optimizedRequest = applyRequestOptimizations(\r\n      request,\r\n      optimizations,\r\n      context\r\n    );\r\n    \r\n    // Track pre-call metrics\r\n    const sideEffects: SideEffect[] = [\r\n      {\r\n        type: 'metric',\r\n        action: 'increment',\r\n        data: { name: `llm.calls.${provider}.${model}` },\r\n      },\r\n      {\r\n        type: 'memory',\r\n        action: 'store',\r\n        data: {\r\n          key: `llm:request:${context.correlationId}`,\r\n          value: {\r\n            provider,\r\n            model,\r\n            operation,\r\n            request: optimizedRequest,\r\n            timestamp: Date.now(),\r\n          },\r\n          ttl: 3600, // 1 hour\r\n        },\r\n      },\r\n    ];\r\n    \r\n    return {\r\n      continue: true,\r\n      modified: true,\r\n      payload: {\r\n        ...payload,\r\n        request: optimizedRequest,\r\n      },\r\n      sideEffects,\r\n    };\r\n  },\r\n};\r\n\r\n// ===== Post-LLM Call Hook =====\r\n\r\nexport const postLLMCallHook = {\r\n  id: 'agentic-post-llm-call',\r\n  type: 'post-llm-call' as const,\r\n  priority: 100,\r\n  handler: async (\r\n    payload: LLMHookPayload,\r\n    context: AgenticHookContext\r\n  ): Promise<HookHandlerResult> => {\r\n    const { provider, model, request, response, metrics } = payload;\r\n    \r\n    if (!response || !metrics) {\r\n      return { continue: true };\r\n    }\r\n    \r\n    const sideEffects: SideEffect[] = [];\r\n    \r\n    // Store response in memory for caching\r\n    const cacheKey = generateCacheKey(provider, model, request);\r\n    sideEffects.push({\r\n      type: 'memory',\r\n      action: 'store',\r\n      data: {\r\n        key: `llm:cache:${cacheKey}`,\r\n        value: {\r\n          response,\r\n          metrics,\r\n          timestamp: Date.now(),\r\n        },\r\n        ttl: determineCacheTTL(operation, response),\r\n      },\r\n    });\r\n    \r\n    // Extract patterns for neural training\r\n    const patterns = extractResponsePatterns(request, response, metrics);\r\n    if (patterns.length > 0) {\r\n      sideEffects.push({\r\n        type: 'neural',\r\n        action: 'train',\r\n        data: {\r\n          patterns,\r\n          modelId: `llm-optimizer-${provider}`,\r\n        },\r\n      });\r\n    }\r\n    \r\n    // Update performance metrics\r\n    sideEffects.push(\r\n      {\r\n        type: 'metric',\r\n        action: 'update',\r\n        data: {\r\n          name: `llm.latency.${provider}.${model}`,\r\n          value: metrics.latency,\r\n        },\r\n      },\r\n      {\r\n        type: 'metric',\r\n        action: 'update',\r\n        data: {\r\n          name: `llm.tokens.${provider}.${model}`,\r\n          value: response.usage.totalTokens,\r\n        },\r\n      },\r\n      {\r\n        type: 'metric',\r\n        action: 'update',\r\n        data: {\r\n          name: `llm.cost.${provider}.${model}`,\r\n          value: metrics.costEstimate,\r\n        },\r\n      }\r\n    );\r\n    \r\n    // Check for performance issues\r\n    if (metrics.latency > getLatencyThreshold(provider, model)) {\r\n      sideEffects.push({\r\n        type: 'notification',\r\n        action: 'send',\r\n        data: {\r\n          level: 'warning',\r\n          message: `High latency detected for ${provider}/${model}: ${metrics.latency}ms`,\r\n        },\r\n      });\r\n    }\r\n    \r\n    // Store provider health score\r\n    await updateProviderHealth(provider, metrics.providerHealth, context);\r\n    \r\n    return {\r\n      continue: true,\r\n      sideEffects,\r\n    };\r\n  },\r\n};\r\n\r\n// ===== LLM Error Hook =====\r\n\r\nexport const llmErrorHook = {\r\n  id: 'agentic-llm-error',\r\n  type: 'llm-error' as const,\r\n  priority: 100,\r\n  handler: async (\r\n    payload: LLMHookPayload,\r\n    context: AgenticHookContext\r\n  ): Promise<HookHandlerResult> => {\r\n    const { provider, model, error } = payload;\r\n    \r\n    if (!error) {\r\n      return { continue: true };\r\n    }\r\n    \r\n    const sideEffects: SideEffect[] = [];\r\n    \r\n    // Log error details\r\n    sideEffects.push({\r\n      type: 'log',\r\n      action: 'write',\r\n      data: {\r\n        level: 'error',\r\n        message: `LLM error from ${provider}/${model}`,\r\n        data: {\r\n          error: error.message,\r\n          stack: error.stack,\r\n          request: payload.request,\r\n        },\r\n      },\r\n    });\r\n    \r\n    // Update error metrics\r\n    sideEffects.push({\r\n      type: 'metric',\r\n      action: 'increment',\r\n      data: { name: `llm.errors.${provider}.${model}` },\r\n    });\r\n    \r\n    // Check if we should fallback\r\n    const fallbackProvider = await selectFallbackProvider(\r\n      provider,\r\n      model,\r\n      error,\r\n      context\r\n    );\r\n    \r\n    if (fallbackProvider) {\r\n      return {\r\n        continue: false, // Don't propagate error\r\n        modified: true,\r\n        payload: {\r\n          ...payload,\r\n          provider: fallbackProvider.provider,\r\n          model: fallbackProvider.model,\r\n          error: undefined, // Clear error for retry\r\n        },\r\n        sideEffects: [\r\n          ...sideEffects,\r\n          {\r\n            type: 'notification',\r\n            action: 'send',\r\n            data: {\r\n              level: 'info',\r\n              message: `Falling back from ${provider}/${model} to ${fallbackProvider.provider}/${fallbackProvider.model}`,\r\n            },\r\n          },\r\n        ],\r\n      };\r\n    }\r\n    \r\n    return {\r\n      continue: true,\r\n      sideEffects,\r\n    };\r\n  },\r\n};\r\n\r\n// ===== LLM Retry Hook =====\r\n\r\nexport const llmRetryHook = {\r\n  id: 'agentic-llm-retry',\r\n  type: 'llm-retry' as const,\r\n  priority: 90,\r\n  handler: async (\r\n    payload: LLMHookPayload,\r\n    context: AgenticHookContext\r\n  ): Promise<HookHandlerResult> => {\r\n    const { provider, model, metrics } = payload;\r\n    const retryCount = metrics?.retryCount || 0;\r\n    \r\n    // Adjust request parameters for retry\r\n    const adjustedRequest = adjustRequestForRetry(\r\n      payload.request,\r\n      retryCount\r\n    );\r\n    \r\n    const sideEffects: SideEffect[] = [\r\n      {\r\n        type: 'metric',\r\n        action: 'increment',\r\n        data: { name: `llm.retries.${provider}.${model}` },\r\n      },\r\n    ];\r\n    \r\n    // Apply exponential backoff\r\n    const backoffMs = Math.min(1000 * Math.pow(2, retryCount), 10000);\r\n    await new Promise(resolve => setTimeout(resolve, backoffMs));\r\n    \r\n    return {\r\n      continue: true,\r\n      modified: true,\r\n      payload: {\r\n        ...payload,\r\n        request: adjustedRequest,\r\n        metrics: {\r\n          ...metrics,\r\n          retryCount: retryCount + 1,\r\n        },\r\n      },\r\n      sideEffects,\r\n    };\r\n  },\r\n};\r\n\r\n// ===== Helper Functions =====\r\n\r\nfunction generateCacheKey(\r\n  provider: string,\r\n  model: string,\r\n  request: LLMHookPayload['request']\r\n): string {\r\n  const normalized = {\r\n    provider,\r\n    model,\r\n    messages: request.messages?.map(m => ({\r\n      role: m.role,\r\n      content: m.content.substring(0, 100), // First 100 chars\r\n    })),\r\n    temperature: request.temperature,\r\n    maxTokens: request.maxTokens,\r\n  };\r\n  \r\n  return Buffer.from(JSON.stringify(normalized)).toString('base64');\r\n}\r\n\r\nasync function checkMemoryCache(\r\n  cacheKey: string,\r\n  context: AgenticHookContext\r\n): Promise<any | null> {\r\n  // Implementation would integrate with memory service\r\n  // This is a placeholder\r\n  return null;\r\n}\r\n\r\nasync function loadProviderOptimizations(\r\n  provider: string,\r\n  context: AgenticHookContext\r\n): Promise<any> {\r\n  // Load provider-specific optimizations from memory\r\n  // This is a placeholder\r\n  return {\r\n    maxRetries: 3,\r\n    timeout: 30000,\r\n    rateLimit: 100,\r\n  };\r\n}\r\n\r\nfunction applyRequestOptimizations(\r\n  request: LLMHookPayload['request'],\r\n  optimizations: any,\r\n  context: AgenticHookContext\r\n): LLMHookPayload['request'] {\r\n  // Apply various optimizations\r\n  const optimized = { ...request };\r\n  \r\n  // Optimize token usage\r\n  if (optimized.maxTokens && optimized.maxTokens > 4000) {\r\n    optimized.maxTokens = 4000; // Cap at reasonable limit\r\n  }\r\n  \r\n  // Optimize temperature for consistency\r\n  if (optimized.temperature === undefined) {\r\n    optimized.temperature = 0.7;\r\n  }\r\n  \r\n  // Add stop sequences if missing\r\n  if (!optimized.stopSequences && optimized.messages) {\r\n    optimized.stopSequences = ['\\n\\nHuman:', '\\n\\nAssistant:'];\r\n  }\r\n  \r\n  return optimized;\r\n}\r\n\r\nfunction determineCacheTTL(\r\n  operation: string,\r\n  response: LLMHookPayload['response']\r\n): number {\r\n  // Determine cache TTL based on operation and response\r\n  switch (operation) {\r\n    case 'embedding':\r\n      return 86400; // 24 hours for embeddings\r\n    case 'completion':\r\n      // Shorter TTL for completions\r\n      return response?.usage?.totalTokens && response.usage.totalTokens > 1000\r\n        ? 1800 // 30 minutes for long responses\r\n        : 3600; // 1 hour for short responses\r\n    default:\r\n      return 3600; // 1 hour default\r\n  }\r\n}\r\n\r\nfunction extractResponsePatterns(\r\n  request: LLMHookPayload['request'],\r\n  response: LLMHookPayload['response'],\r\n  metrics: LLMMetrics\r\n): Pattern[] {\r\n  const patterns: Pattern[] = [];\r\n  \r\n  // Extract performance patterns\r\n  if (metrics.latency > 1000) {\r\n    patterns.push({\r\n      id: `perf_${Date.now()}`,\r\n      type: 'optimization',\r\n      confidence: 0.8,\r\n      occurrences: 1,\r\n      context: {\r\n        provider: metrics.providerHealth < 0.8 ? 'unhealthy' : 'healthy',\r\n        requestSize: JSON.stringify(request).length,\r\n        responseTokens: response?.usage?.totalTokens || 0,\r\n        latency: metrics.latency,\r\n      },\r\n    });\r\n  }\r\n  \r\n  // Extract success patterns\r\n  if (response?.choices?.[0]?.finishReason === 'stop') {\r\n    patterns.push({\r\n      id: `success_${Date.now()}`,\r\n      type: 'success',\r\n      confidence: 0.9,\r\n      occurrences: 1,\r\n      context: {\r\n        temperature: request.temperature,\r\n        maxTokens: request.maxTokens,\r\n        actualTokens: response.usage?.totalTokens || 0,\r\n      },\r\n    });\r\n  }\r\n  \r\n  return patterns;\r\n}\r\n\r\nfunction getLatencyThreshold(provider: string, model: string): number {\r\n  // Provider/model specific thresholds\r\n  const thresholds: Record<string, number> = {\r\n    'openai:gpt-4': 5000,\r\n    'openai:gpt-3.5-turbo': 2000,\r\n    'anthropic:claude-3': 4000,\r\n    'anthropic:claude-instant': 1500,\r\n  };\r\n  \r\n  return thresholds[`${provider}:${model}`] || 3000;\r\n}\r\n\r\nasync function updateProviderHealth(\r\n  provider: string,\r\n  health: number,\r\n  context: AgenticHookContext\r\n): Promise<void> {\r\n  // Update provider health in memory\r\n  const healthKey = `provider:health:${provider}`;\r\n  const currentHealth = await context.memory.cache.get(healthKey) || [];\r\n  \r\n  currentHealth.push({\r\n    timestamp: Date.now(),\r\n    health,\r\n  });\r\n  \r\n  // Keep last 100 health checks\r\n  if (currentHealth.length > 100) {\r\n    currentHealth.shift();\r\n  }\r\n  \r\n  await context.memory.cache.set(healthKey, currentHealth);\r\n}\r\n\r\nasync function selectFallbackProvider(\r\n  provider: string,\r\n  model: string,\r\n  error: Error,\r\n  context: AgenticHookContext\r\n): Promise<{ provider: string; model: string } | null> {\r\n  // Implement intelligent fallback selection\r\n  const fallbacks: Record<string, { provider: string; model: string }[]> = {\r\n    'openai': [\r\n      { provider: 'anthropic', model: 'claude-3' },\r\n      { provider: 'cohere', model: 'command' },\r\n    ],\r\n    'anthropic': [\r\n      { provider: 'openai', model: 'gpt-4' },\r\n      { provider: 'cohere', model: 'command' },\r\n    ],\r\n  };\r\n  \r\n  const candidates = fallbacks[provider] || [];\r\n  \r\n  // Select based on health scores\r\n  for (const candidate of candidates) {\r\n    const healthKey = `provider:health:${candidate.provider}`;\r\n    const healthData = await context.memory.cache.get(healthKey) || [];\r\n    \r\n    if (healthData.length > 0) {\r\n      const avgHealth = healthData.reduce((sum: number, h: any) => \r\n        sum + h.health, 0\r\n      ) / healthData.length;\r\n      \r\n      if (avgHealth > 0.7) {\r\n        return candidate;\r\n      }\r\n    }\r\n  }\r\n  \r\n  return null;\r\n}\r\n\r\nfunction adjustRequestForRetry(\r\n  request: LLMHookPayload['request'],\r\n  retryCount: number\r\n): LLMHookPayload['request'] {\r\n  const adjusted = { ...request };\r\n  \r\n  // Increase temperature slightly for variety\r\n  if (adjusted.temperature !== undefined) {\r\n    adjusted.temperature = Math.min(\r\n      adjusted.temperature + (0.1 * retryCount),\r\n      1.0\r\n    );\r\n  }\r\n  \r\n  // Reduce max tokens to improve success rate\r\n  if (adjusted.maxTokens !== undefined) {\r\n    adjusted.maxTokens = Math.floor(\r\n      adjusted.maxTokens * Math.pow(0.9, retryCount)\r\n    );\r\n  }\r\n  \r\n  return adjusted;\r\n}\r\n\r\n// ===== Register Hooks =====\r\n\r\nexport function registerLLMHooks(): void {\r\n  agenticHookManager.register(preLLMCallHook);\r\n  agenticHookManager.register(postLLMCallHook);\r\n  agenticHookManager.register(llmErrorHook);\r\n  agenticHookManager.register(llmRetryHook);\r\n}"],"names":["agenticHookManager","preLLMCallHook","id","type","priority","handler","payload","context","provider","model","operation","request","cacheKey","generateCacheKey","cached","checkMemoryCache","continue","modified","response","metrics","cacheHit","sideEffects","action","data","name","optimizations","loadProviderOptimizations","optimizedRequest","applyRequestOptimizations","key","correlationId","value","timestamp","Date","now","ttl","postLLMCallHook","push","determineCacheTTL","patterns","extractResponsePatterns","length","modelId","latency","usage","totalTokens","costEstimate","getLatencyThreshold","level","message","updateProviderHealth","providerHealth","llmErrorHook","error","stack","fallbackProvider","selectFallbackProvider","undefined","llmRetryHook","retryCount","adjustedRequest","adjustRequestForRetry","backoffMs","Math","min","pow","Promise","resolve","setTimeout","normalized","messages","map","m","role","content","substring","temperature","maxTokens","Buffer","from","JSON","stringify","toString","maxRetries","timeout","rateLimit","optimized","stopSequences","confidence","occurrences","requestSize","responseTokens","choices","finishReason","actualTokens","thresholds","health","healthKey","currentHealth","memory","cache","get","shift","set","fallbacks","candidates","candidate","healthData","avgHealth","reduce","sum","h","adjusted","floor","registerLLMHooks","register"],"mappings":"AAOA,SAASA,kBAAkB,QAAQ,oBAAoB;AAYvD,OAAO,MAAMC,iBAAiB;IAC5BC,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEC,WAAAA,UAAS,EAAEC,OAAO,EAAE,GAAGL;QAGhD,MAAMM,WAAWC,iBAAiBL,UAAUC,OAAOE;QACnD,MAAMG,SAAS,MAAMC,iBAAiBH,UAAUL;QAEhD,IAAIO,QAAQ;YACV,OAAO;gBACLE,UAAU;gBACVC,UAAU;gBACVX,SAAS;oBACP,GAAGA,OAAO;oBACVY,UAAUJ,OAAOI,QAAQ;oBACzBC,SAAS;wBACP,GAAGL,OAAOK,OAAO;wBACjBC,UAAU;oBACZ;gBACF;gBACAC,aAAa;oBACX;wBACElB,MAAM;wBACNmB,QAAQ;wBACRC,MAAM;4BAAEC,MAAM;wBAAiB;oBACjC;iBACD;YACH;QACF;QAGA,MAAMC,gBAAgB,MAAMC,0BAA0BlB,UAAUD;QAGhE,MAAMoB,mBAAmBC,0BACvBjB,SACAc,eACAlB;QAIF,MAAMc,cAA4B;YAChC;gBACElB,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBAAEC,MAAM,CAAC,UAAU,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBAAC;YACjD;YACA;gBACEN,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJM,KAAK,CAAC,YAAY,EAAEtB,QAAQuB,aAAa,EAAE;oBAC3CC,OAAO;wBACLvB;wBACAC;wBACAC,WAAAA;wBACAC,SAASgB;wBACTK,WAAWC,KAAKC,GAAG;oBACrB;oBACAC,KAAK;gBACP;YACF;SACD;QAED,OAAO;YACLnB,UAAU;YACVC,UAAU;YACVX,SAAS;gBACP,GAAGA,OAAO;gBACVK,SAASgB;YACX;YACAN;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAMe,kBAAkB;IAC7BlC,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEE,OAAO,EAAEO,QAAQ,EAAEC,OAAO,EAAE,GAAGb;QAExD,IAAI,CAACY,YAAY,CAACC,SAAS;YACzB,OAAO;gBAAEH,UAAU;YAAK;QAC1B;QAEA,MAAMK,cAA4B,EAAE;QAGpC,MAAMT,WAAWC,iBAAiBL,UAAUC,OAAOE;QACnDU,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJM,KAAK,CAAC,UAAU,EAAEjB,UAAU;gBAC5BmB,OAAO;oBACLb;oBACAC;oBACAa,WAAWC,KAAKC,GAAG;gBACrB;gBACAC,KAAKG,kBAAkB5B,WAAWQ;YACpC;QACF;QAGA,MAAMqB,WAAWC,wBAAwB7B,SAASO,UAAUC;QAC5D,IAAIoB,SAASE,MAAM,GAAG,GAAG;YACvBpB,YAAYgB,IAAI,CAAC;gBACflC,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJgB;oBACAG,SAAS,CAAC,cAAc,EAAElC,UAAU;gBACtC;YACF;QACF;QAGAa,YAAYgB,IAAI,CACd;YACElC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,YAAY,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACxCsB,OAAOZ,QAAQwB,OAAO;YACxB;QACF,GACA;YACExC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,WAAW,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACvCsB,OAAOb,SAAS0B,KAAK,CAACC,WAAW;YACnC;QACF,GACA;YACE1C,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJC,MAAM,CAAC,SAAS,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBACrCsB,OAAOZ,QAAQ2B,YAAY;YAC7B;QACF;QAIF,IAAI3B,QAAQwB,OAAO,GAAGI,oBAAoBvC,UAAUC,QAAQ;YAC1DY,YAAYgB,IAAI,CAAC;gBACflC,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBACJyB,OAAO;oBACPC,SAAS,CAAC,0BAA0B,EAAEzC,SAAS,CAAC,EAAEC,MAAM,EAAE,EAAEU,QAAQwB,OAAO,CAAC,EAAE,CAAC;gBACjF;YACF;QACF;QAGA,MAAMO,qBAAqB1C,UAAUW,QAAQgC,cAAc,EAAE5C;QAE7D,OAAO;YACLS,UAAU;YACVK;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAM+B,eAAe;IAC1BlD,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAE4C,KAAK,EAAE,GAAG/C;QAEnC,IAAI,CAAC+C,OAAO;YACV,OAAO;gBAAErC,UAAU;YAAK;QAC1B;QAEA,MAAMK,cAA4B,EAAE;QAGpCA,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBACJyB,OAAO;gBACPC,SAAS,CAAC,eAAe,EAAEzC,SAAS,CAAC,EAAEC,OAAO;gBAC9Cc,MAAM;oBACJ8B,OAAOA,MAAMJ,OAAO;oBACpBK,OAAOD,MAAMC,KAAK;oBAClB3C,SAASL,QAAQK,OAAO;gBAC1B;YACF;QACF;QAGAU,YAAYgB,IAAI,CAAC;YACflC,MAAM;YACNmB,QAAQ;YACRC,MAAM;gBAAEC,MAAM,CAAC,WAAW,EAAEhB,SAAS,CAAC,EAAEC,OAAO;YAAC;QAClD;QAGA,MAAM8C,mBAAmB,MAAMC,uBAC7BhD,UACAC,OACA4C,OACA9C;QAGF,IAAIgD,kBAAkB;YACpB,OAAO;gBACLvC,UAAU;gBACVC,UAAU;gBACVX,SAAS;oBACP,GAAGA,OAAO;oBACVE,UAAU+C,iBAAiB/C,QAAQ;oBACnCC,OAAO8C,iBAAiB9C,KAAK;oBAC7B4C,OAAOI;gBACT;gBACApC,aAAa;uBACRA;oBACH;wBACElB,MAAM;wBACNmB,QAAQ;wBACRC,MAAM;4BACJyB,OAAO;4BACPC,SAAS,CAAC,kBAAkB,EAAEzC,SAAS,CAAC,EAAEC,MAAM,IAAI,EAAE8C,iBAAiB/C,QAAQ,CAAC,CAAC,EAAE+C,iBAAiB9C,KAAK,EAAE;wBAC7G;oBACF;iBACD;YACH;QACF;QAEA,OAAO;YACLO,UAAU;YACVK;QACF;IACF;AACF,EAAE;AAIF,OAAO,MAAMqC,eAAe;IAC1BxD,IAAI;IACJC,MAAM;IACNC,UAAU;IACVC,SAAS,OACPC,SACAC;QAEA,MAAM,EAAEC,QAAQ,EAAEC,KAAK,EAAEU,OAAO,EAAE,GAAGb;QACrC,MAAMqD,aAAaxC,SAASwC,cAAc;QAG1C,MAAMC,kBAAkBC,sBACtBvD,QAAQK,OAAO,EACfgD;QAGF,MAAMtC,cAA4B;YAChC;gBACElB,MAAM;gBACNmB,QAAQ;gBACRC,MAAM;oBAAEC,MAAM,CAAC,YAAY,EAAEhB,SAAS,CAAC,EAAEC,OAAO;gBAAC;YACnD;SACD;QAGD,MAAMqD,YAAYC,KAAKC,GAAG,CAAC,OAAOD,KAAKE,GAAG,CAAC,GAAGN,aAAa;QAC3D,MAAM,IAAIO,QAAQC,CAAAA,UAAWC,WAAWD,SAASL;QAEjD,OAAO;YACL9C,UAAU;YACVC,UAAU;YACVX,SAAS;gBACP,GAAGA,OAAO;gBACVK,SAASiD;gBACTzC,SAAS;oBACP,GAAGA,OAAO;oBACVwC,YAAYA,aAAa;gBAC3B;YACF;YACAtC;QACF;IACF;AACF,EAAE;AAIF,SAASR,iBACPL,QAAgB,EAChBC,KAAa,EACbE,OAAkC;IAElC,MAAM0D,aAAa;QACjB7D;QACAC;QACA6D,UAAU3D,QAAQ2D,QAAQ,EAAEC,IAAIC,CAAAA,IAAM,CAAA;gBACpCC,MAAMD,EAAEC,IAAI;gBACZC,SAASF,EAAEE,OAAO,CAACC,SAAS,CAAC,GAAG;YAClC,CAAA;QACAC,aAAajE,QAAQiE,WAAW;QAChCC,WAAWlE,QAAQkE,SAAS;IAC9B;IAEA,OAAOC,OAAOC,IAAI,CAACC,KAAKC,SAAS,CAACZ,aAAaa,QAAQ,CAAC;AAC1D;AAEA,eAAenE,iBACbH,QAAgB,EAChBL,OAA2B;IAI3B,OAAO;AACT;AAEA,eAAemB,0BACblB,QAAgB,EAChBD,OAA2B;IAI3B,OAAO;QACL4E,YAAY;QACZC,SAAS;QACTC,WAAW;IACb;AACF;AAEA,SAASzD,0BACPjB,OAAkC,EAClCc,aAAkB,EAClBlB,OAA2B;IAG3B,MAAM+E,YAAY;QAAE,GAAG3E,OAAO;IAAC;IAG/B,IAAI2E,UAAUT,SAAS,IAAIS,UAAUT,SAAS,GAAG,MAAM;QACrDS,UAAUT,SAAS,GAAG;IACxB;IAGA,IAAIS,UAAUV,WAAW,KAAKnB,WAAW;QACvC6B,UAAUV,WAAW,GAAG;IAC1B;IAGA,IAAI,CAACU,UAAUC,aAAa,IAAID,UAAUhB,QAAQ,EAAE;QAClDgB,UAAUC,aAAa,GAAG;YAAC;YAAc;SAAiB;IAC5D;IAEA,OAAOD;AACT;AAEA,SAAShD,kBACP5B,UAAiB,EACjBQ,QAAoC;IAGpC,OAAQR;QACN,KAAK;YACH,OAAO;QACT,KAAK;YAEH,OAAOQ,UAAU0B,OAAOC,eAAe3B,SAAS0B,KAAK,CAACC,WAAW,GAAG,OAChE,OACA;QACN;YACE,OAAO;IACX;AACF;AAEA,SAASL,wBACP7B,OAAkC,EAClCO,QAAoC,EACpCC,OAAmB;IAEnB,MAAMoB,WAAsB,EAAE;IAG9B,IAAIpB,QAAQwB,OAAO,GAAG,MAAM;QAC1BJ,SAASF,IAAI,CAAC;YACZnC,IAAI,CAAC,KAAK,EAAE+B,KAAKC,GAAG,IAAI;YACxB/B,MAAM;YACNqF,YAAY;YACZC,aAAa;YACblF,SAAS;gBACPC,UAAUW,QAAQgC,cAAc,GAAG,MAAM,cAAc;gBACvDuC,aAAaV,KAAKC,SAAS,CAACtE,SAAS8B,MAAM;gBAC3CkD,gBAAgBzE,UAAU0B,OAAOC,eAAe;gBAChDF,SAASxB,QAAQwB,OAAO;YAC1B;QACF;IACF;IAGA,IAAIzB,UAAU0E,SAAS,CAAC,EAAE,EAAEC,iBAAiB,QAAQ;QACnDtD,SAASF,IAAI,CAAC;YACZnC,IAAI,CAAC,QAAQ,EAAE+B,KAAKC,GAAG,IAAI;YAC3B/B,MAAM;YACNqF,YAAY;YACZC,aAAa;YACblF,SAAS;gBACPqE,aAAajE,QAAQiE,WAAW;gBAChCC,WAAWlE,QAAQkE,SAAS;gBAC5BiB,cAAc5E,SAAS0B,KAAK,EAAEC,eAAe;YAC/C;QACF;IACF;IAEA,OAAON;AACT;AAEA,SAASQ,oBAAoBvC,QAAgB,EAAEC,KAAa;IAE1D,MAAMsF,aAAqC;QACzC,gBAAgB;QAChB,wBAAwB;QACxB,sBAAsB;QACtB,4BAA4B;IAC9B;IAEA,OAAOA,UAAU,CAAC,GAAGvF,SAAS,CAAC,EAAEC,OAAO,CAAC,IAAI;AAC/C;AAEA,eAAeyC,qBACb1C,QAAgB,EAChBwF,MAAc,EACdzF,OAA2B;IAG3B,MAAM0F,YAAY,CAAC,gBAAgB,EAAEzF,UAAU;IAC/C,MAAM0F,gBAAgB,MAAM3F,QAAQ4F,MAAM,CAACC,KAAK,CAACC,GAAG,CAACJ,cAAc,EAAE;IAErEC,cAAc7D,IAAI,CAAC;QACjBL,WAAWC,KAAKC,GAAG;QACnB8D;IACF;IAGA,IAAIE,cAAczD,MAAM,GAAG,KAAK;QAC9ByD,cAAcI,KAAK;IACrB;IAEA,MAAM/F,QAAQ4F,MAAM,CAACC,KAAK,CAACG,GAAG,CAACN,WAAWC;AAC5C;AAEA,eAAe1C,uBACbhD,QAAgB,EAChBC,KAAa,EACb4C,KAAY,EACZ9C,OAA2B;IAG3B,MAAMiG,YAAmE;QACvE,UAAU;YACR;gBAAEhG,UAAU;gBAAaC,OAAO;YAAW;YAC3C;gBAAED,UAAU;gBAAUC,OAAO;YAAU;SACxC;QACD,aAAa;YACX;gBAAED,UAAU;gBAAUC,OAAO;YAAQ;YACrC;gBAAED,UAAU;gBAAUC,OAAO;YAAU;SACxC;IACH;IAEA,MAAMgG,aAAaD,SAAS,CAAChG,SAAS,IAAI,EAAE;IAG5C,KAAK,MAAMkG,aAAaD,WAAY;QAClC,MAAMR,YAAY,CAAC,gBAAgB,EAAES,UAAUlG,QAAQ,EAAE;QACzD,MAAMmG,aAAa,MAAMpG,QAAQ4F,MAAM,CAACC,KAAK,CAACC,GAAG,CAACJ,cAAc,EAAE;QAElE,IAAIU,WAAWlE,MAAM,GAAG,GAAG;YACzB,MAAMmE,YAAYD,WAAWE,MAAM,CAAC,CAACC,KAAaC,IAChDD,MAAMC,EAAEf,MAAM,EAAE,KACdW,WAAWlE,MAAM;YAErB,IAAImE,YAAY,KAAK;gBACnB,OAAOF;YACT;QACF;IACF;IAEA,OAAO;AACT;AAEA,SAAS7C,sBACPlD,OAAkC,EAClCgD,UAAkB;IAElB,MAAMqD,WAAW;QAAE,GAAGrG,OAAO;IAAC;IAG9B,IAAIqG,SAASpC,WAAW,KAAKnB,WAAW;QACtCuD,SAASpC,WAAW,GAAGb,KAAKC,GAAG,CAC7BgD,SAASpC,WAAW,GAAI,MAAMjB,YAC9B;IAEJ;IAGA,IAAIqD,SAASnC,SAAS,KAAKpB,WAAW;QACpCuD,SAASnC,SAAS,GAAGd,KAAKkD,KAAK,CAC7BD,SAASnC,SAAS,GAAGd,KAAKE,GAAG,CAAC,KAAKN;IAEvC;IAEA,OAAOqD;AACT;AAIA,OAAO,SAASE;IACdlH,mBAAmBmH,QAAQ,CAAClH;IAC5BD,mBAAmBmH,QAAQ,CAAC/E;IAC5BpC,mBAAmBmH,QAAQ,CAAC/D;IAC5BpD,mBAAmBmH,QAAQ,CAACzD;AAC9B"}