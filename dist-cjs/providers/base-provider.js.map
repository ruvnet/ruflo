{
  "version": 3,
  "sources": ["../../src/providers/base-provider.ts"],
  "sourcesContent": ["/**\n * Abstract Base Provider for LLM integrations\n * Provides common functionality for all LLM providers\n */\n\nimport { EventEmitter } from 'events';\nimport { ILogger } from '../core/logger.js';\nimport { circuitBreaker, CircuitBreaker } from '../utils/helpers.js';\nimport {\n  ILLMProvider,\n  LLMProvider,\n  LLMProviderConfig,\n  LLMRequest,\n  LLMResponse,\n  LLMStreamEvent,\n  LLMModel,\n  ModelInfo,\n  ProviderCapabilities,\n  HealthCheckResult,\n  ProviderStatus,\n  CostEstimate,\n  UsageStats,\n  UsagePeriod,\n  LLMProviderError,\n  RateLimitError,\n  ProviderUnavailableError,\n} from './types.js';\n\nexport interface BaseProviderOptions {\n  logger: ILogger;\n  config: LLMProviderConfig;\n  cacheTTL?: number;\n  circuitBreakerOptions?: {\n    threshold?: number;\n    timeout?: number;\n    resetTimeout?: number;\n  };\n}\n\nexport abstract class BaseProvider extends EventEmitter implements ILLMProvider {\n  abstract readonly name: LLMProvider;\n  abstract readonly capabilities: ProviderCapabilities;\n  \n  protected logger: ILogger;\n  protected circuitBreaker: CircuitBreaker;\n  protected healthCheckInterval?: NodeJS.Timeout;\n  protected lastHealthCheck?: HealthCheckResult;\n  protected requestCount = 0;\n  protected errorCount = 0;\n  protected totalTokens = 0;\n  protected totalCost = 0;\n  protected requestMetrics: Map<string, any> = new Map();\n  \n  public config: LLMProviderConfig;\n\n  constructor(options: BaseProviderOptions) {\n    super();\n    this.logger = options.logger;\n    this.config = options.config;\n    \n    // Initialize circuit breaker\n    this.circuitBreaker = circuitBreaker(`llm-${this.name}`, {\n      threshold: options.circuitBreakerOptions?.threshold || 5,\n      timeout: options.circuitBreakerOptions?.timeout || 60000,\n      resetTimeout: options.circuitBreakerOptions?.resetTimeout || 300000,\n    });\n    \n    // Start health checks if enabled\n    if (this.config.enableCaching) {\n      this.startHealthChecks();\n    }\n  }\n\n  /**\n   * Initialize the provider\n   */\n  async initialize(): Promise<void> {\n    this.logger.info(`Initializing ${this.name} provider`, {\n      model: this.config.model,\n      temperature: this.config.temperature,\n      maxTokens: this.config.maxTokens,\n    });\n    \n    // Validate configuration\n    this.validateConfig();\n    \n    // Provider-specific initialization\n    await this.doInitialize();\n    \n    // Perform initial health check\n    await this.healthCheck();\n  }\n\n  /**\n   * Provider-specific initialization\n   */\n  protected abstract doInitialize(): Promise<void>;\n\n  /**\n   * Validate provider configuration\n   */\n  protected validateConfig(): void {\n    if (!this.config.model) {\n      throw new Error(`Model is required for ${this.name} provider`);\n    }\n    \n    if (!this.validateModel(this.config.model)) {\n      throw new Error(`Model ${this.config.model} is not supported by ${this.name} provider`);\n    }\n    \n    if (this.config.temperature !== undefined) {\n      if (this.config.temperature < 0 || this.config.temperature > 2) {\n        throw new Error('Temperature must be between 0 and 2');\n      }\n    }\n    \n    if (this.config.maxTokens !== undefined) {\n      const maxAllowed = this.capabilities.maxOutputTokens[this.config.model] || 4096;\n      if (this.config.maxTokens > maxAllowed) {\n        throw new Error(`Max tokens exceeds limit of ${maxAllowed} for model ${this.config.model}`);\n      }\n    }\n  }\n\n  /**\n   * Complete a request\n   */\n  async complete(request: LLMRequest): Promise<LLMResponse> {\n    const startTime = Date.now();\n    \n    try {\n      // Use circuit breaker\n      const response = await this.circuitBreaker.execute(async () => {\n        return await this.doComplete(request);\n      });\n      \n      // Track metrics\n      const latency = Date.now() - startTime;\n      this.trackRequest(request, response, latency);\n      \n      // Emit events\n      this.emit('response', {\n        provider: this.name,\n        model: response.model,\n        latency,\n        tokens: response.usage.totalTokens,\n        cost: response.cost?.totalCost,\n      });\n      \n      return response;\n    } catch (error) {\n      this.errorCount++;\n      \n      // Transform to provider error\n      const providerError = this.transformError(error);\n      \n      // Track error\n      this.emit('error', {\n        provider: this.name,\n        error: providerError,\n        request,\n      });\n      \n      throw providerError;\n    }\n  }\n\n  /**\n   * Provider-specific completion implementation\n   */\n  protected abstract doComplete(request: LLMRequest): Promise<LLMResponse>;\n\n  /**\n   * Stream complete a request\n   */\n  async *streamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\n    const startTime = Date.now();\n    let totalTokens = 0;\n    let totalCost = 0;\n    \n    try {\n      // Check if streaming is supported\n      if (!this.capabilities.supportsStreaming) {\n        throw new LLMProviderError(\n          'Streaming not supported',\n          'STREAMING_NOT_SUPPORTED',\n          this.name,\n          undefined,\n          false\n        );\n      }\n      \n      // Use circuit breaker\n      const stream = await this.circuitBreaker.execute(async () => {\n        return this.doStreamComplete(request);\n      });\n      \n      // Process stream\n      for await (const event of stream) {\n        if (event.usage) {\n          totalTokens = event.usage.totalTokens;\n        }\n        if (event.cost) {\n          totalCost = event.cost.totalCost;\n        }\n        \n        yield event;\n      }\n      \n      // Track metrics\n      const latency = Date.now() - startTime;\n      this.trackStreamRequest(request, totalTokens, totalCost, latency);\n      \n    } catch (error) {\n      this.errorCount++;\n      \n      // Transform to provider error\n      const providerError = this.transformError(error);\n      \n      // Yield error event\n      yield {\n        type: 'error',\n        error: providerError,\n      };\n      \n      throw providerError;\n    }\n  }\n\n  /**\n   * Provider-specific stream completion implementation\n   */\n  protected abstract doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent>;\n\n  /**\n   * List available models\n   */\n  abstract listModels(): Promise<LLMModel[]>;\n\n  /**\n   * Get model information\n   */\n  abstract getModelInfo(model: LLMModel): Promise<ModelInfo>;\n\n  /**\n   * Validate if a model is supported\n   */\n  validateModel(model: LLMModel): boolean {\n    return this.capabilities.supportedModels.includes(model);\n  }\n\n  /**\n   * Perform health check\n   */\n  async healthCheck(): Promise<HealthCheckResult> {\n    const startTime = Date.now();\n    \n    try {\n      // Provider-specific health check\n      const result = await this.doHealthCheck();\n      \n      this.lastHealthCheck = {\n        ...result,\n        latency: Date.now() - startTime,\n        timestamp: new Date(),\n      };\n      \n      this.emit('health_check', this.lastHealthCheck);\n      return this.lastHealthCheck;\n      \n    } catch (error) {\n      this.lastHealthCheck = {\n        healthy: false,\n        error: error instanceof Error ? error.message : 'Unknown error',\n        latency: Date.now() - startTime,\n        timestamp: new Date(),\n      };\n      \n      this.emit('health_check', this.lastHealthCheck);\n      return this.lastHealthCheck;\n    }\n  }\n\n  /**\n   * Provider-specific health check implementation\n   */\n  protected abstract doHealthCheck(): Promise<HealthCheckResult>;\n\n  /**\n   * Get provider status\n   */\n  getStatus(): ProviderStatus {\n    const queueLength = this.requestMetrics.size;\n    const errorRate = this.requestCount > 0 ? this.errorCount / this.requestCount : 0;\n    \n    return {\n      available: this.lastHealthCheck?.healthy ?? false,\n      currentLoad: queueLength / 100, // Normalize to 0-1\n      queueLength,\n      activeRequests: queueLength,\n      rateLimitRemaining: this.getRateLimitRemaining(),\n      rateLimitReset: this.getRateLimitReset(),\n    };\n  }\n\n  /**\n   * Get remaining rate limit (override in provider)\n   */\n  protected getRateLimitRemaining(): number | undefined {\n    return undefined;\n  }\n\n  /**\n   * Get rate limit reset time (override in provider)\n   */\n  protected getRateLimitReset(): Date | undefined {\n    return undefined;\n  }\n\n  /**\n   * Estimate cost for a request\n   */\n  async estimateCost(request: LLMRequest): Promise<CostEstimate> {\n    const model = request.model || this.config.model;\n    const pricing = this.capabilities.pricing?.[model];\n    \n    if (!pricing) {\n      return {\n        estimatedPromptTokens: 0,\n        estimatedCompletionTokens: 0,\n        estimatedTotalTokens: 0,\n        estimatedCost: {\n          prompt: 0,\n          completion: 0,\n          total: 0,\n          currency: 'USD',\n        },\n        confidence: 0,\n      };\n    }\n    \n    // Estimate tokens (simple approximation, providers should override)\n    const promptTokens = this.estimateTokens(JSON.stringify(request.messages));\n    const completionTokens = request.maxTokens || this.config.maxTokens || 1000;\n    \n    const promptCost = (promptTokens / 1000) * pricing.promptCostPer1k;\n    const completionCost = (completionTokens / 1000) * pricing.completionCostPer1k;\n    \n    return {\n      estimatedPromptTokens: promptTokens,\n      estimatedCompletionTokens: completionTokens,\n      estimatedTotalTokens: promptTokens + completionTokens,\n      estimatedCost: {\n        prompt: promptCost,\n        completion: completionCost,\n        total: promptCost + completionCost,\n        currency: pricing.currency,\n      },\n      confidence: 0.7, // 70% confidence in estimation\n    };\n  }\n\n  /**\n   * Simple token estimation (4 chars = 1 token approximation)\n   */\n  protected estimateTokens(text: string): number {\n    return Math.ceil(text.length / 4);\n  }\n\n  /**\n   * Get usage statistics\n   */\n  async getUsage(period: UsagePeriod = 'day'): Promise<UsageStats> {\n    const now = new Date();\n    const start = this.getStartDate(now, period);\n    \n    // In a real implementation, this would query a database\n    // For now, return current session stats\n    return {\n      period: { start, end: now },\n      requests: this.requestCount,\n      tokens: {\n        prompt: Math.floor(this.totalTokens * 0.7), // Estimate\n        completion: Math.floor(this.totalTokens * 0.3),\n        total: this.totalTokens,\n      },\n      cost: {\n        prompt: this.totalCost * 0.7,\n        completion: this.totalCost * 0.3,\n        total: this.totalCost,\n        currency: 'USD',\n      },\n      errors: this.errorCount,\n      averageLatency: this.calculateAverageLatency(),\n      modelBreakdown: {}, // Would need to track per model\n    };\n  }\n\n  /**\n   * Get start date for period\n   */\n  private getStartDate(end: Date, period: UsagePeriod): Date {\n    const start = new Date(end);\n    switch (period) {\n      case 'hour':\n        start.setHours(start.getHours() - 1);\n        break;\n      case 'day':\n        start.setDate(start.getDate() - 1);\n        break;\n      case 'week':\n        start.setDate(start.getDate() - 7);\n        break;\n      case 'month':\n        start.setMonth(start.getMonth() - 1);\n        break;\n      case 'all':\n        start.setFullYear(2020); // Arbitrary old date\n        break;\n    }\n    return start;\n  }\n\n  /**\n   * Calculate average latency\n   */\n  private calculateAverageLatency(): number {\n    if (this.requestMetrics.size === 0) return 0;\n    \n    let totalLatency = 0;\n    let count = 0;\n    \n    this.requestMetrics.forEach((metrics) => {\n      if (metrics.latency) {\n        totalLatency += metrics.latency;\n        count++;\n      }\n    });\n    \n    return count > 0 ? totalLatency / count : 0;\n  }\n\n  /**\n   * Track successful request\n   */\n  protected trackRequest(request: LLMRequest, response: LLMResponse, latency: number): void {\n    this.requestCount++;\n    this.totalTokens += response.usage.totalTokens;\n    \n    if (response.cost) {\n      this.totalCost += response.cost.totalCost;\n    }\n    \n    // Store metrics (in memory for now)\n    const requestId = response.id;\n    this.requestMetrics.set(requestId, {\n      timestamp: new Date(),\n      model: response.model,\n      tokens: response.usage.totalTokens,\n      cost: response.cost?.totalCost,\n      latency,\n    });\n    \n    // Clean up old metrics (keep last 1000)\n    if (this.requestMetrics.size > 1000) {\n      const oldestKey = this.requestMetrics.keys().next().value;\n      this.requestMetrics.delete(oldestKey);\n    }\n  }\n\n  /**\n   * Track streaming request\n   */\n  protected trackStreamRequest(\n    request: LLMRequest,\n    totalTokens: number,\n    totalCost: number,\n    latency: number\n  ): void {\n    this.requestCount++;\n    this.totalTokens += totalTokens;\n    this.totalCost += totalCost;\n    \n    // Store metrics\n    const requestId = `stream-${Date.now()}`;\n    this.requestMetrics.set(requestId, {\n      timestamp: new Date(),\n      model: request.model || this.config.model,\n      tokens: totalTokens,\n      cost: totalCost,\n      latency,\n      stream: true,\n    });\n  }\n\n  /**\n   * Transform errors to provider errors\n   */\n  protected transformError(error: unknown): LLMProviderError {\n    if (error instanceof LLMProviderError) {\n      return error;\n    }\n    \n    if (error instanceof Error) {\n      // Check for common error patterns\n      if (error.message.includes('rate limit')) {\n        return new RateLimitError(error.message, this.name);\n      }\n      \n      if (error.message.includes('timeout') || error.message.includes('ETIMEDOUT')) {\n        return new LLMProviderError(\n          'Request timed out',\n          'TIMEOUT',\n          this.name,\n          undefined,\n          true\n        );\n      }\n      \n      if (error.message.includes('ECONNREFUSED') || error.message.includes('fetch failed')) {\n        return new ProviderUnavailableError(this.name, { originalError: error.message });\n      }\n    }\n    \n    return new LLMProviderError(\n      error instanceof Error ? error.message : String(error),\n      'UNKNOWN',\n      this.name,\n      undefined,\n      true\n    );\n  }\n\n  /**\n   * Start periodic health checks\n   */\n  protected startHealthChecks(): void {\n    const interval = this.config.cacheTimeout || 300000; // 5 minutes default\n    \n    this.healthCheckInterval = setInterval(() => {\n      this.healthCheck().catch((error) => {\n        this.logger.error(`Health check failed for ${this.name}`, error);\n      });\n    }, interval);\n  }\n\n  /**\n   * Clean up resources\n   */\n  destroy(): void {\n    if (this.healthCheckInterval) {\n      clearInterval(this.healthCheckInterval);\n    }\n    \n    this.requestMetrics.clear();\n    this.removeAllListeners();\n    \n    this.logger.info(`${this.name} provider destroyed`);\n  }\n}"],
  "mappings": ";;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA,oBAA6B;AAE7B,qBAA+C;AAC/C,mBAkBO;AAaA,MAAe,qBAAqB,2BAAqC;AAAA,EAvChF,OAuCgF;AAAA;AAAA;AAAA,EAIpE;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,eAAe;AAAA,EACf,aAAa;AAAA,EACb,cAAc;AAAA,EACd,YAAY;AAAA,EACZ,iBAAmC,oBAAI,IAAI;AAAA,EAE9C;AAAA,EAEP,YAAY,SAA8B;AACxC,UAAM;AACN,SAAK,SAAS,QAAQ;AACtB,SAAK,SAAS,QAAQ;AAGtB,SAAK,qBAAiB,+BAAe,OAAO,KAAK,IAAI,IAAI;AAAA,MACvD,WAAW,QAAQ,uBAAuB,aAAa;AAAA,MACvD,SAAS,QAAQ,uBAAuB,WAAW;AAAA,MACnD,cAAc,QAAQ,uBAAuB,gBAAgB;AAAA,IAC/D,CAAC;AAGD,QAAI,KAAK,OAAO,eAAe;AAC7B,WAAK,kBAAkB;AAAA,IACzB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAA4B;AAChC,SAAK,OAAO,KAAK,gBAAgB,KAAK,IAAI,aAAa;AAAA,MACrD,OAAO,KAAK,OAAO;AAAA,MACnB,aAAa,KAAK,OAAO;AAAA,MACzB,WAAW,KAAK,OAAO;AAAA,IACzB,CAAC;AAGD,SAAK,eAAe;AAGpB,UAAM,KAAK,aAAa;AAGxB,UAAM,KAAK,YAAY;AAAA,EACzB;AAAA;AAAA;AAAA;AAAA,EAUU,iBAAuB;AAC/B,QAAI,CAAC,KAAK,OAAO,OAAO;AACtB,YAAM,IAAI,MAAM,yBAAyB,KAAK,IAAI,WAAW;AAAA,IAC/D;AAEA,QAAI,CAAC,KAAK,cAAc,KAAK,OAAO,KAAK,GAAG;AAC1C,YAAM,IAAI,MAAM,SAAS,KAAK,OAAO,KAAK,wBAAwB,KAAK,IAAI,WAAW;AAAA,IACxF;AAEA,QAAI,KAAK,OAAO,gBAAgB,QAAW;AACzC,UAAI,KAAK,OAAO,cAAc,KAAK,KAAK,OAAO,cAAc,GAAG;AAC9D,cAAM,IAAI,MAAM,qCAAqC;AAAA,MACvD;AAAA,IACF;AAEA,QAAI,KAAK,OAAO,cAAc,QAAW;AACvC,YAAM,aAAa,KAAK,aAAa,gBAAgB,KAAK,OAAO,KAAK,KAAK;AAC3E,UAAI,KAAK,OAAO,YAAY,YAAY;AACtC,cAAM,IAAI,MAAM,+BAA+B,UAAU,cAAc,KAAK,OAAO,KAAK,EAAE;AAAA,MAC5F;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,SAAS,SAA2C;AACxD,UAAM,YAAY,KAAK,IAAI;AAE3B,QAAI;AAEF,YAAM,WAAW,MAAM,KAAK,eAAe,QAAQ,YAAY;AAC7D,eAAO,MAAM,KAAK,WAAW,OAAO;AAAA,MACtC,CAAC;AAGD,YAAM,UAAU,KAAK,IAAI,IAAI;AAC7B,WAAK,aAAa,SAAS,UAAU,OAAO;AAG5C,WAAK,KAAK,YAAY;AAAA,QACpB,UAAU,KAAK;AAAA,QACf,OAAO,SAAS;AAAA,QAChB;AAAA,QACA,QAAQ,SAAS,MAAM;AAAA,QACvB,MAAM,SAAS,MAAM;AAAA,MACvB,CAAC;AAED,aAAO;AAAA,IACT,SAAS,OAAO;AACd,WAAK;AAGL,YAAM,gBAAgB,KAAK,eAAe,KAAK;AAG/C,WAAK,KAAK,SAAS;AAAA,QACjB,UAAU,KAAK;AAAA,QACf,OAAO;AAAA,QACP;AAAA,MACF,CAAC;AAED,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAUA,OAAO,eAAe,SAAoD;AACxE,UAAM,YAAY,KAAK,IAAI;AAC3B,QAAI,cAAc;AAClB,QAAI,YAAY;AAEhB,QAAI;AAEF,UAAI,CAAC,KAAK,aAAa,mBAAmB;AACxC,cAAM,IAAI;AAAA,UACR;AAAA,UACA;AAAA,UACA,KAAK;AAAA,UACL;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAGA,YAAM,SAAS,MAAM,KAAK,eAAe,QAAQ,YAAY;AAC3D,eAAO,KAAK,iBAAiB,OAAO;AAAA,MACtC,CAAC;AAGD,uBAAiB,SAAS,QAAQ;AAChC,YAAI,MAAM,OAAO;AACf,wBAAc,MAAM,MAAM;AAAA,QAC5B;AACA,YAAI,MAAM,MAAM;AACd,sBAAY,MAAM,KAAK;AAAA,QACzB;AAEA,cAAM;AAAA,MACR;AAGA,YAAM,UAAU,KAAK,IAAI,IAAI;AAC7B,WAAK,mBAAmB,SAAS,aAAa,WAAW,OAAO;AAAA,IAElE,SAAS,OAAO;AACd,WAAK;AAGL,YAAM,gBAAgB,KAAK,eAAe,KAAK;AAG/C,YAAM;AAAA,QACJ,MAAM;AAAA,QACN,OAAO;AAAA,MACT;AAEA,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAoBA,cAAc,OAA0B;AACtC,WAAO,KAAK,aAAa,gBAAgB,SAAS,KAAK;AAAA,EACzD;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAA0C;AAC9C,UAAM,YAAY,KAAK,IAAI;AAE3B,QAAI;AAEF,YAAM,SAAS,MAAM,KAAK,cAAc;AAExC,WAAK,kBAAkB;AAAA,QACrB,GAAG;AAAA,QACH,SAAS,KAAK,IAAI,IAAI;AAAA,QACtB,WAAW,oBAAI,KAAK;AAAA,MACtB;AAEA,WAAK,KAAK,gBAAgB,KAAK,eAAe;AAC9C,aAAO,KAAK;AAAA,IAEd,SAAS,OAAO;AACd,WAAK,kBAAkB;AAAA,QACrB,SAAS;AAAA,QACT,OAAO,iBAAiB,QAAQ,MAAM,UAAU;AAAA,QAChD,SAAS,KAAK,IAAI,IAAI;AAAA,QACtB,WAAW,oBAAI,KAAK;AAAA,MACtB;AAEA,WAAK,KAAK,gBAAgB,KAAK,eAAe;AAC9C,aAAO,KAAK;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAUA,YAA4B;AAC1B,UAAM,cAAc,KAAK,eAAe;AACxC,UAAM,YAAY,KAAK,eAAe,IAAI,KAAK,aAAa,KAAK,eAAe;AAEhF,WAAO;AAAA,MACL,WAAW,KAAK,iBAAiB,WAAW;AAAA,MAC5C,aAAa,cAAc;AAAA;AAAA,MAC3B;AAAA,MACA,gBAAgB;AAAA,MAChB,oBAAoB,KAAK,sBAAsB;AAAA,MAC/C,gBAAgB,KAAK,kBAAkB;AAAA,IACzC;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKU,wBAA4C;AACpD,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKU,oBAAsC;AAC9C,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,SAA4C;AAC7D,UAAM,QAAQ,QAAQ,SAAS,KAAK,OAAO;AAC3C,UAAM,UAAU,KAAK,aAAa,UAAU,KAAK;AAEjD,QAAI,CAAC,SAAS;AACZ,aAAO;AAAA,QACL,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,QAC3B,sBAAsB;AAAA,QACtB,eAAe;AAAA,UACb,QAAQ;AAAA,UACR,YAAY;AAAA,UACZ,OAAO;AAAA,UACP,UAAU;AAAA,QACZ;AAAA,QACA,YAAY;AAAA,MACd;AAAA,IACF;AAGA,UAAM,eAAe,KAAK,eAAe,KAAK,UAAU,QAAQ,QAAQ,CAAC;AACzE,UAAM,mBAAmB,QAAQ,aAAa,KAAK,OAAO,aAAa;AAEvE,UAAM,aAAc,eAAe,MAAQ,QAAQ;AACnD,UAAM,iBAAkB,mBAAmB,MAAQ,QAAQ;AAE3D,WAAO;AAAA,MACL,uBAAuB;AAAA,MACvB,2BAA2B;AAAA,MAC3B,sBAAsB,eAAe;AAAA,MACrC,eAAe;AAAA,QACb,QAAQ;AAAA,QACR,YAAY;AAAA,QACZ,OAAO,aAAa;AAAA,QACpB,UAAU,QAAQ;AAAA,MACpB;AAAA,MACA,YAAY;AAAA;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKU,eAAe,MAAsB;AAC7C,WAAO,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,SAAS,SAAsB,OAA4B;AAC/D,UAAM,MAAM,oBAAI,KAAK;AACrB,UAAM,QAAQ,KAAK,aAAa,KAAK,MAAM;AAI3C,WAAO;AAAA,MACL,QAAQ,EAAE,OAAO,KAAK,IAAI;AAAA,MAC1B,UAAU,KAAK;AAAA,MACf,QAAQ;AAAA,QACN,QAAQ,KAAK,MAAM,KAAK,cAAc,GAAG;AAAA;AAAA,QACzC,YAAY,KAAK,MAAM,KAAK,cAAc,GAAG;AAAA,QAC7C,OAAO,KAAK;AAAA,MACd;AAAA,MACA,MAAM;AAAA,QACJ,QAAQ,KAAK,YAAY;AAAA,QACzB,YAAY,KAAK,YAAY;AAAA,QAC7B,OAAO,KAAK;AAAA,QACZ,UAAU;AAAA,MACZ;AAAA,MACA,QAAQ,KAAK;AAAA,MACb,gBAAgB,KAAK,wBAAwB;AAAA,MAC7C,gBAAgB,CAAC;AAAA;AAAA,IACnB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,aAAa,KAAW,QAA2B;AACzD,UAAM,QAAQ,IAAI,KAAK,GAAG;AAC1B,YAAQ,QAAQ;AAAA,MACd,KAAK;AACH,cAAM,SAAS,MAAM,SAAS,IAAI,CAAC;AACnC;AAAA,MACF,KAAK;AACH,cAAM,QAAQ,MAAM,QAAQ,IAAI,CAAC;AACjC;AAAA,MACF,KAAK;AACH,cAAM,QAAQ,MAAM,QAAQ,IAAI,CAAC;AACjC;AAAA,MACF,KAAK;AACH,cAAM,SAAS,MAAM,SAAS,IAAI,CAAC;AACnC;AAAA,MACF,KAAK;AACH,cAAM,YAAY,IAAI;AACtB;AAAA,IACJ;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,0BAAkC;AACxC,QAAI,KAAK,eAAe,SAAS;AAAG,aAAO;AAE3C,QAAI,eAAe;AACnB,QAAI,QAAQ;AAEZ,SAAK,eAAe,QAAQ,CAAC,YAAY;AACvC,UAAI,QAAQ,SAAS;AACnB,wBAAgB,QAAQ;AACxB;AAAA,MACF;AAAA,IACF,CAAC;AAED,WAAO,QAAQ,IAAI,eAAe,QAAQ;AAAA,EAC5C;AAAA;AAAA;AAAA;AAAA,EAKU,aAAa,SAAqB,UAAuB,SAAuB;AACxF,SAAK;AACL,SAAK,eAAe,SAAS,MAAM;AAEnC,QAAI,SAAS,MAAM;AACjB,WAAK,aAAa,SAAS,KAAK;AAAA,IAClC;AAGA,UAAM,YAAY,SAAS;AAC3B,SAAK,eAAe,IAAI,WAAW;AAAA,MACjC,WAAW,oBAAI,KAAK;AAAA,MACpB,OAAO,SAAS;AAAA,MAChB,QAAQ,SAAS,MAAM;AAAA,MACvB,MAAM,SAAS,MAAM;AAAA,MACrB;AAAA,IACF,CAAC;AAGD,QAAI,KAAK,eAAe,OAAO,KAAM;AACnC,YAAM,YAAY,KAAK,eAAe,KAAK,EAAE,KAAK,EAAE;AACpD,WAAK,eAAe,OAAO,SAAS;AAAA,IACtC;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKU,mBACR,SACA,aACA,WACA,SACM;AACN,SAAK;AACL,SAAK,eAAe;AACpB,SAAK,aAAa;AAGlB,UAAM,YAAY,UAAU,KAAK,IAAI,CAAC;AACtC,SAAK,eAAe,IAAI,WAAW;AAAA,MACjC,WAAW,oBAAI,KAAK;AAAA,MACpB,OAAO,QAAQ,SAAS,KAAK,OAAO;AAAA,MACpC,QAAQ;AAAA,MACR,MAAM;AAAA,MACN;AAAA,MACA,QAAQ;AAAA,IACV,CAAC;AAAA,EACH;AAAA;AAAA;AAAA;AAAA,EAKU,eAAe,OAAkC;AACzD,QAAI,iBAAiB,+BAAkB;AACrC,aAAO;AAAA,IACT;AAEA,QAAI,iBAAiB,OAAO;AAE1B,UAAI,MAAM,QAAQ,SAAS,YAAY,GAAG;AACxC,eAAO,IAAI,4BAAe,MAAM,SAAS,KAAK,IAAI;AAAA,MACpD;AAEA,UAAI,MAAM,QAAQ,SAAS,SAAS,KAAK,MAAM,QAAQ,SAAS,WAAW,GAAG;AAC5E,eAAO,IAAI;AAAA,UACT;AAAA,UACA;AAAA,UACA,KAAK;AAAA,UACL;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAEA,UAAI,MAAM,QAAQ,SAAS,cAAc,KAAK,MAAM,QAAQ,SAAS,cAAc,GAAG;AACpF,eAAO,IAAI,sCAAyB,KAAK,MAAM,EAAE,eAAe,MAAM,QAAQ,CAAC;AAAA,MACjF;AAAA,IACF;AAEA,WAAO,IAAI;AAAA,MACT,iBAAiB,QAAQ,MAAM,UAAU,OAAO,KAAK;AAAA,MACrD;AAAA,MACA,KAAK;AAAA,MACL;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKU,oBAA0B;AAClC,UAAM,WAAW,KAAK,OAAO,gBAAgB;AAE7C,SAAK,sBAAsB,YAAY,MAAM;AAC3C,WAAK,YAAY,EAAE,MAAM,CAAC,UAAU;AAClC,aAAK,OAAO,MAAM,2BAA2B,KAAK,IAAI,IAAI,KAAK;AAAA,MACjE,CAAC;AAAA,IACH,GAAG,QAAQ;AAAA,EACb;AAAA;AAAA;AAAA;AAAA,EAKA,UAAgB;AACd,QAAI,KAAK,qBAAqB;AAC5B,oBAAc,KAAK,mBAAmB;AAAA,IACxC;AAEA,SAAK,eAAe,MAAM;AAC1B,SAAK,mBAAmB;AAExB,SAAK,OAAO,KAAK,GAAG,KAAK,IAAI,qBAAqB;AAAA,EACpD;AACF;",
  "names": []
}
