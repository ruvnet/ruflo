{
  "version": 3,
  "sources": ["../../src/providers/ollama-provider.ts"],
  "sourcesContent": ["/**\n * Ollama Provider Implementation\n * Supports local models running via Ollama\n */\n\nimport { BaseProvider } from './base-provider.js';\nimport {\n  LLMProvider,\n  LLMModel,\n  LLMRequest,\n  LLMResponse,\n  LLMStreamEvent,\n  ModelInfo,\n  ProviderCapabilities,\n  HealthCheckResult,\n  LLMProviderError,\n  ProviderUnavailableError,\n} from './types.js';\n\ninterface OllamaGenerateRequest {\n  model: string;\n  prompt: string;\n  system?: string;\n  template?: string;\n  context?: number[];\n  stream?: boolean;\n  raw?: boolean;\n  format?: 'json';\n  options?: {\n    temperature?: number;\n    top_k?: number;\n    top_p?: number;\n    num_predict?: number;\n    stop?: string[];\n    seed?: number;\n    num_ctx?: number;\n  };\n}\n\ninterface OllamaChatRequest {\n  model: string;\n  messages: Array<{\n    role: 'system' | 'user' | 'assistant';\n    content: string;\n  }>;\n  stream?: boolean;\n  format?: 'json';\n  options?: OllamaGenerateRequest['options'];\n}\n\ninterface OllamaResponse {\n  model: string;\n  created_at: string;\n  response?: string;\n  message?: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  context?: number[];\n  total_duration?: number;\n  load_duration?: number;\n  prompt_eval_count?: number;\n  prompt_eval_duration?: number;\n  eval_count?: number;\n  eval_duration?: number;\n}\n\ninterface OllamaModelInfo {\n  name: string;\n  modified_at: string;\n  size: number;\n  digest: string;\n  details: {\n    format: string;\n    family: string;\n    families: string[] | null;\n    parameter_size: string;\n    quantization_level: string;\n  };\n}\n\nexport class OllamaProvider extends BaseProvider {\n  readonly name: LLMProvider = 'ollama';\n  readonly capabilities: ProviderCapabilities = {\n    supportedModels: [\n      'llama-2-7b',\n      'llama-2-13b',\n      'llama-2-70b',\n      'mistral-7b',\n      'mixtral-8x7b',\n      'custom-model',\n    ],\n    maxContextLength: {\n      'llama-2-7b': 4096,\n      'llama-2-13b': 4096,\n      'llama-2-70b': 4096,\n      'mistral-7b': 8192,\n      'mixtral-8x7b': 32768,\n      'custom-model': 4096,\n    } as Record<LLMModel, number>,\n    maxOutputTokens: {\n      'llama-2-7b': 2048,\n      'llama-2-13b': 2048,\n      'llama-2-70b': 2048,\n      'mistral-7b': 4096,\n      'mixtral-8x7b': 4096,\n      'custom-model': 2048,\n    } as Record<LLMModel, number>,\n    supportsStreaming: true,\n    supportsFunctionCalling: false,\n    supportsSystemMessages: true,\n    supportsVision: false,\n    supportsAudio: false,\n    supportsTools: false,\n    supportsFineTuning: false,\n    supportsEmbeddings: true,\n    supportsLogprobs: false,\n    supportsBatching: false,\n    pricing: {\n      // Local models have no API cost\n      'llama-2-7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n      'llama-2-13b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n      'llama-2-70b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n      'mistral-7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n      'mixtral-8x7b': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n      'custom-model': { promptCostPer1k: 0, completionCostPer1k: 0, currency: 'USD' },\n    },\n  };\n\n  private baseUrl: string;\n  private availableModels: Set<string> = new Set();\n\n  protected async doInitialize(): Promise<void> {\n    this.baseUrl = this.config.apiUrl || 'http://localhost:11434';\n    \n    // Check if Ollama is running and get available models\n    try {\n      await this.fetchAvailableModels();\n    } catch (error) {\n      this.logger.warn('Failed to fetch Ollama models, will retry on first request', error);\n    }\n  }\n\n  private async fetchAvailableModels(): Promise<void> {\n    try {\n      const response = await fetch(`${this.baseUrl}/api/tags`);\n      if (!response.ok) {\n        throw new Error(`Failed to fetch models: ${response.status}`);\n      }\n      \n      const data = await response.json();\n      this.availableModels.clear();\n      \n      if (data.models && Array.isArray(data.models)) {\n        data.models.forEach((model: OllamaModelInfo) => {\n          this.availableModels.add(model.name);\n          // Map common model names\n          if (model.name.includes('llama2:7b')) {\n            this.availableModels.add('llama-2-7b');\n          } else if (model.name.includes('llama2:13b')) {\n            this.availableModels.add('llama-2-13b');\n          } else if (model.name.includes('llama2:70b')) {\n            this.availableModels.add('llama-2-70b');\n          } else if (model.name.includes('mistral')) {\n            this.availableModels.add('mistral-7b');\n          } else if (model.name.includes('mixtral')) {\n            this.availableModels.add('mixtral-8x7b');\n          }\n        });\n      }\n    } catch (error) {\n      throw new ProviderUnavailableError('ollama', {\n        message: 'Ollama service is not available',\n        details: error,\n      });\n    }\n  }\n\n  protected async doComplete(request: LLMRequest): Promise<LLMResponse> {\n    // Use chat endpoint for multi-turn conversations\n    const ollamaRequest: OllamaChatRequest = {\n      model: this.mapToOllamaModel(request.model || this.config.model),\n      messages: request.messages.map(msg => ({\n        role: msg.role === 'system' ? 'system' : msg.role === 'assistant' ? 'assistant' : 'user',\n        content: msg.content,\n      })),\n      stream: false,\n      options: {\n        temperature: request.temperature ?? this.config.temperature,\n        top_k: request.topK ?? this.config.topK,\n        top_p: request.topP ?? this.config.topP,\n        num_predict: request.maxTokens ?? this.config.maxTokens,\n        stop: request.stopSequences ?? this.config.stopSequences,\n      },\n    };\n\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), this.config.timeout || 120000); // Longer timeout for local models\n\n    try {\n      const response = await fetch(`${this.baseUrl}/api/chat`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(ollamaRequest),\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeout);\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const data: OllamaResponse = await response.json();\n      \n      // Calculate metrics\n      const promptTokens = data.prompt_eval_count || this.estimateTokens(JSON.stringify(request.messages));\n      const completionTokens = data.eval_count || this.estimateTokens(data.message?.content || '');\n      const totalDuration = data.total_duration ? data.total_duration / 1000000 : 0; // Convert nanoseconds to milliseconds\n\n      return {\n        id: `ollama-${Date.now()}`,\n        model: request.model || this.config.model,\n        provider: 'ollama',\n        content: data.message?.content || '',\n        usage: {\n          promptTokens,\n          completionTokens,\n          totalTokens: promptTokens + completionTokens,\n        },\n        cost: {\n          promptCost: 0,\n          completionCost: 0,\n          totalCost: 0,\n          currency: 'USD',\n        },\n        latency: totalDuration,\n        finishReason: data.done ? 'stop' : 'length',\n        metadata: {\n          loadDuration: data.load_duration,\n          promptEvalDuration: data.prompt_eval_duration,\n          evalDuration: data.eval_duration,\n        },\n      };\n    } catch (error) {\n      clearTimeout(timeout);\n      \n      // Check if Ollama is running\n      if (error instanceof TypeError && error.message.includes('fetch')) {\n        throw new ProviderUnavailableError('ollama', {\n          message: 'Cannot connect to Ollama. Make sure Ollama is running on ' + this.baseUrl,\n        });\n      }\n      \n      throw this.transformError(error);\n    }\n  }\n\n  protected async *doStreamComplete(request: LLMRequest): AsyncIterable<LLMStreamEvent> {\n    const ollamaRequest: OllamaChatRequest = {\n      model: this.mapToOllamaModel(request.model || this.config.model),\n      messages: request.messages.map(msg => ({\n        role: msg.role === 'system' ? 'system' : msg.role === 'assistant' ? 'assistant' : 'user',\n        content: msg.content,\n      })),\n      stream: true,\n      options: {\n        temperature: request.temperature ?? this.config.temperature,\n        top_k: request.topK ?? this.config.topK,\n        top_p: request.topP ?? this.config.topP,\n        num_predict: request.maxTokens ?? this.config.maxTokens,\n        stop: request.stopSequences ?? this.config.stopSequences,\n      },\n    };\n\n    const controller = new AbortController();\n    const timeout = setTimeout(() => controller.abort(), (this.config.timeout || 120000) * 2);\n\n    try {\n      const response = await fetch(`${this.baseUrl}/api/chat`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(ollamaRequest),\n        signal: controller.signal,\n      });\n\n      if (!response.ok) {\n        await this.handleErrorResponse(response);\n      }\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n      let buffer = '';\n      let totalContent = '';\n      let promptTokens = 0;\n      let completionTokens = 0;\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.trim() === '') continue;\n          \n          try {\n            const data: OllamaResponse = JSON.parse(line);\n            \n            if (data.message?.content) {\n              totalContent += data.message.content;\n              yield {\n                type: 'content',\n                delta: { content: data.message.content },\n              };\n            }\n            \n            if (data.done) {\n              promptTokens = data.prompt_eval_count || this.estimateTokens(JSON.stringify(request.messages));\n              completionTokens = data.eval_count || this.estimateTokens(totalContent);\n              \n              yield {\n                type: 'done',\n                usage: {\n                  promptTokens,\n                  completionTokens,\n                  totalTokens: promptTokens + completionTokens,\n                },\n                cost: {\n                  promptCost: 0,\n                  completionCost: 0,\n                  totalCost: 0,\n                  currency: 'USD',\n                },\n              };\n            }\n          } catch (e) {\n            this.logger.warn('Failed to parse Ollama stream chunk', { line, error: e });\n          }\n        }\n      }\n    } catch (error) {\n      clearTimeout(timeout);\n      \n      if (error instanceof TypeError && error.message.includes('fetch')) {\n        throw new ProviderUnavailableError('ollama', {\n          message: 'Cannot connect to Ollama. Make sure Ollama is running on ' + this.baseUrl,\n        });\n      }\n      \n      throw this.transformError(error);\n    } finally {\n      clearTimeout(timeout);\n    }\n  }\n\n  async listModels(): Promise<LLMModel[]> {\n    // Refresh available models\n    await this.fetchAvailableModels();\n    \n    // Return intersection of supported models and available models\n    return this.capabilities.supportedModels.filter(model => \n      this.availableModels.has(this.mapToOllamaModel(model))\n    );\n  }\n\n  async getModelInfo(model: LLMModel): Promise<ModelInfo> {\n    const ollamaModel = this.mapToOllamaModel(model);\n    \n    try {\n      const response = await fetch(`${this.baseUrl}/api/show`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ name: ollamaModel }),\n      });\n\n      if (!response.ok) {\n        throw new Error('Model not found');\n      }\n\n      const data = await response.json();\n      \n      return {\n        model,\n        name: data.name || model,\n        description: data.description || this.getModelDescription(model),\n        contextLength: this.capabilities.maxContextLength[model] || 4096,\n        maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\n        supportedFeatures: ['chat', 'completion'],\n        pricing: this.capabilities.pricing![model],\n        metadata: {\n          parameterSize: data.details?.parameter_size,\n          quantization: data.details?.quantization_level,\n          format: data.details?.format,\n        },\n      };\n    } catch (error) {\n      // Fallback to default info\n      return {\n        model,\n        name: model,\n        description: this.getModelDescription(model),\n        contextLength: this.capabilities.maxContextLength[model] || 4096,\n        maxOutputTokens: this.capabilities.maxOutputTokens[model] || 2048,\n        supportedFeatures: ['chat', 'completion'],\n        pricing: this.capabilities.pricing![model],\n      };\n    }\n  }\n\n  protected async doHealthCheck(): Promise<HealthCheckResult> {\n    try {\n      const response = await fetch(`${this.baseUrl}/api/tags`);\n      \n      if (!response.ok) {\n        throw new Error(`Health check failed: ${response.status}`);\n      }\n\n      return {\n        healthy: true,\n        timestamp: new Date(),\n        details: {\n          modelsAvailable: this.availableModels.size,\n        },\n      };\n    } catch (error) {\n      return {\n        healthy: false,\n        error: error instanceof Error ? error.message : 'Cannot connect to Ollama',\n        timestamp: new Date(),\n      };\n    }\n  }\n\n  private mapToOllamaModel(model: LLMModel): string {\n    const modelMap: Record<string, string> = {\n      'llama-2-7b': 'llama2:7b',\n      'llama-2-13b': 'llama2:13b',\n      'llama-2-70b': 'llama2:70b',\n      'mistral-7b': 'mistral:7b',\n      'mixtral-8x7b': 'mixtral:8x7b',\n      'custom-model': this.config.providerOptions?.customModel || 'llama2:latest',\n    };\n    return modelMap[model] || model;\n  }\n\n  private getModelDescription(model: LLMModel): string {\n    const descriptions: Record<string, string> = {\n      'llama-2-7b': 'Llama 2 7B - Efficient open-source model',\n      'llama-2-13b': 'Llama 2 13B - Balanced performance model',\n      'llama-2-70b': 'Llama 2 70B - Large open-source model',\n      'mistral-7b': 'Mistral 7B - Fast and efficient model',\n      'mixtral-8x7b': 'Mixtral 8x7B - Mixture of experts model',\n      'custom-model': 'Custom local model',\n    };\n    return descriptions[model] || 'Local language model via Ollama';\n  }\n\n  private async handleErrorResponse(response: Response): Promise<void> {\n    const errorText = await response.text();\n    let errorData: any;\n\n    try {\n      errorData = JSON.parse(errorText);\n    } catch {\n      errorData = { error: errorText };\n    }\n\n    const message = errorData.error || 'Unknown error';\n\n    throw new LLMProviderError(\n      message,\n      `OLLAMA_${response.status}`,\n      'ollama',\n      response.status,\n      response.status >= 500,\n      errorData\n    );\n  }\n}"],
  "mappings": ";;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAKA,2BAA6B;AAC7B,mBAWO;AAiEA,MAAM,uBAAuB,kCAAa;AAAA,EAlFjD,OAkFiD;AAAA;AAAA;AAAA,EACtC,OAAoB;AAAA,EACpB,eAAqC;AAAA,IAC5C,iBAAiB;AAAA,MACf;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,IACA,kBAAkB;AAAA,MAChB,cAAc;AAAA,MACd,eAAe;AAAA,MACf,eAAe;AAAA,MACf,cAAc;AAAA,MACd,gBAAgB;AAAA,MAChB,gBAAgB;AAAA,IAClB;AAAA,IACA,iBAAiB;AAAA,MACf,cAAc;AAAA,MACd,eAAe;AAAA,MACf,eAAe;AAAA,MACf,cAAc;AAAA,MACd,gBAAgB;AAAA,MAChB,gBAAgB;AAAA,IAClB;AAAA,IACA,mBAAmB;AAAA,IACnB,yBAAyB;AAAA,IACzB,wBAAwB;AAAA,IACxB,gBAAgB;AAAA,IAChB,eAAe;AAAA,IACf,eAAe;AAAA,IACf,oBAAoB;AAAA,IACpB,oBAAoB;AAAA,IACpB,kBAAkB;AAAA,IAClB,kBAAkB;AAAA,IAClB,SAAS;AAAA;AAAA,MAEP,cAAc,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,MAC5E,eAAe,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,MAC7E,eAAe,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,MAC7E,cAAc,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,MAC5E,gBAAgB,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,MAC9E,gBAAgB,EAAE,iBAAiB,GAAG,qBAAqB,GAAG,UAAU,MAAM;AAAA,IAChF;AAAA,EACF;AAAA,EAEQ;AAAA,EACA,kBAA+B,oBAAI,IAAI;AAAA,EAE/C,MAAgB,eAA8B;AAC5C,SAAK,UAAU,KAAK,OAAO,UAAU;AAGrC,QAAI;AACF,YAAM,KAAK,qBAAqB;AAAA,IAClC,SAAS,OAAO;AACd,WAAK,OAAO,KAAK,8DAA8D,KAAK;AAAA,IACtF;AAAA,EACF;AAAA,EAEA,MAAc,uBAAsC;AAClD,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,WAAW;AACvD,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI,MAAM,2BAA2B,SAAS,MAAM,EAAE;AAAA,MAC9D;AAEA,YAAM,OAAO,MAAM,SAAS,KAAK;AACjC,WAAK,gBAAgB,MAAM;AAE3B,UAAI,KAAK,UAAU,MAAM,QAAQ,KAAK,MAAM,GAAG;AAC7C,aAAK,OAAO,QAAQ,CAAC,UAA2B;AAC9C,eAAK,gBAAgB,IAAI,MAAM,IAAI;AAEnC,cAAI,MAAM,KAAK,SAAS,WAAW,GAAG;AACpC,iBAAK,gBAAgB,IAAI,YAAY;AAAA,UACvC,WAAW,MAAM,KAAK,SAAS,YAAY,GAAG;AAC5C,iBAAK,gBAAgB,IAAI,aAAa;AAAA,UACxC,WAAW,MAAM,KAAK,SAAS,YAAY,GAAG;AAC5C,iBAAK,gBAAgB,IAAI,aAAa;AAAA,UACxC,WAAW,MAAM,KAAK,SAAS,SAAS,GAAG;AACzC,iBAAK,gBAAgB,IAAI,YAAY;AAAA,UACvC,WAAW,MAAM,KAAK,SAAS,SAAS,GAAG;AACzC,iBAAK,gBAAgB,IAAI,cAAc;AAAA,UACzC;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,SAAS,OAAO;AACd,YAAM,IAAI,sCAAyB,UAAU;AAAA,QAC3C,SAAS;AAAA,QACT,SAAS;AAAA,MACX,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,MAAgB,WAAW,SAA2C;AAEpE,UAAM,gBAAmC;AAAA,MACvC,OAAO,KAAK,iBAAiB,QAAQ,SAAS,KAAK,OAAO,KAAK;AAAA,MAC/D,UAAU,QAAQ,SAAS,IAAI,UAAQ;AAAA,QACrC,MAAM,IAAI,SAAS,WAAW,WAAW,IAAI,SAAS,cAAc,cAAc;AAAA,QAClF,SAAS,IAAI;AAAA,MACf,EAAE;AAAA,MACF,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,aAAa,QAAQ,eAAe,KAAK,OAAO;AAAA,QAChD,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,QACnC,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,QACnC,aAAa,QAAQ,aAAa,KAAK,OAAO;AAAA,QAC9C,MAAM,QAAQ,iBAAiB,KAAK,OAAO;AAAA,MAC7C;AAAA,IACF;AAEA,UAAM,aAAa,IAAI,gBAAgB;AACvC,UAAM,UAAU,WAAW,MAAM,WAAW,MAAM,GAAG,KAAK,OAAO,WAAW,IAAM;AAElF,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,aAAa;AAAA,QACvD,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,gBAAgB;AAAA,QAClB;AAAA,QACA,MAAM,KAAK,UAAU,aAAa;AAAA,QAClC,QAAQ,WAAW;AAAA,MACrB,CAAC;AAED,mBAAa,OAAO;AAEpB,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,KAAK,oBAAoB,QAAQ;AAAA,MACzC;AAEA,YAAM,OAAuB,MAAM,SAAS,KAAK;AAGjD,YAAM,eAAe,KAAK,qBAAqB,KAAK,eAAe,KAAK,UAAU,QAAQ,QAAQ,CAAC;AACnG,YAAM,mBAAmB,KAAK,cAAc,KAAK,eAAe,KAAK,SAAS,WAAW,EAAE;AAC3F,YAAM,gBAAgB,KAAK,iBAAiB,KAAK,iBAAiB,MAAU;AAE5E,aAAO;AAAA,QACL,IAAI,UAAU,KAAK,IAAI,CAAC;AAAA,QACxB,OAAO,QAAQ,SAAS,KAAK,OAAO;AAAA,QACpC,UAAU;AAAA,QACV,SAAS,KAAK,SAAS,WAAW;AAAA,QAClC,OAAO;AAAA,UACL;AAAA,UACA;AAAA,UACA,aAAa,eAAe;AAAA,QAC9B;AAAA,QACA,MAAM;AAAA,UACJ,YAAY;AAAA,UACZ,gBAAgB;AAAA,UAChB,WAAW;AAAA,UACX,UAAU;AAAA,QACZ;AAAA,QACA,SAAS;AAAA,QACT,cAAc,KAAK,OAAO,SAAS;AAAA,QACnC,UAAU;AAAA,UACR,cAAc,KAAK;AAAA,UACnB,oBAAoB,KAAK;AAAA,UACzB,cAAc,KAAK;AAAA,QACrB;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,mBAAa,OAAO;AAGpB,UAAI,iBAAiB,aAAa,MAAM,QAAQ,SAAS,OAAO,GAAG;AACjE,cAAM,IAAI,sCAAyB,UAAU;AAAA,UAC3C,SAAS,8DAA8D,KAAK;AAAA,QAC9E,CAAC;AAAA,MACH;AAEA,YAAM,KAAK,eAAe,KAAK;AAAA,IACjC;AAAA,EACF;AAAA,EAEA,OAAiB,iBAAiB,SAAoD;AACpF,UAAM,gBAAmC;AAAA,MACvC,OAAO,KAAK,iBAAiB,QAAQ,SAAS,KAAK,OAAO,KAAK;AAAA,MAC/D,UAAU,QAAQ,SAAS,IAAI,UAAQ;AAAA,QACrC,MAAM,IAAI,SAAS,WAAW,WAAW,IAAI,SAAS,cAAc,cAAc;AAAA,QAClF,SAAS,IAAI;AAAA,MACf,EAAE;AAAA,MACF,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,aAAa,QAAQ,eAAe,KAAK,OAAO;AAAA,QAChD,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,QACnC,OAAO,QAAQ,QAAQ,KAAK,OAAO;AAAA,QACnC,aAAa,QAAQ,aAAa,KAAK,OAAO;AAAA,QAC9C,MAAM,QAAQ,iBAAiB,KAAK,OAAO;AAAA,MAC7C;AAAA,IACF;AAEA,UAAM,aAAa,IAAI,gBAAgB;AACvC,UAAM,UAAU,WAAW,MAAM,WAAW,MAAM,IAAI,KAAK,OAAO,WAAW,QAAU,CAAC;AAExF,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,aAAa;AAAA,QACvD,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,gBAAgB;AAAA,QAClB;AAAA,QACA,MAAM,KAAK,UAAU,aAAa;AAAA,QAClC,QAAQ,WAAW;AAAA,MACrB,CAAC;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,KAAK,oBAAoB,QAAQ;AAAA,MACzC;AAEA,YAAM,SAAS,SAAS,KAAM,UAAU;AACxC,YAAM,UAAU,IAAI,YAAY;AAChC,UAAI,SAAS;AACb,UAAI,eAAe;AACnB,UAAI,eAAe;AACnB,UAAI,mBAAmB;AAEvB,aAAO,MAAM;AACX,cAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,YAAI;AAAM;AAEV,kBAAU,QAAQ,OAAO,OAAO,EAAE,QAAQ,KAAK,CAAC;AAChD,cAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,iBAAS,MAAM,IAAI,KAAK;AAExB,mBAAW,QAAQ,OAAO;AACxB,cAAI,KAAK,KAAK,MAAM;AAAI;AAExB,cAAI;AACF,kBAAM,OAAuB,KAAK,MAAM,IAAI;AAE5C,gBAAI,KAAK,SAAS,SAAS;AACzB,8BAAgB,KAAK,QAAQ;AAC7B,oBAAM;AAAA,gBACJ,MAAM;AAAA,gBACN,OAAO,EAAE,SAAS,KAAK,QAAQ,QAAQ;AAAA,cACzC;AAAA,YACF;AAEA,gBAAI,KAAK,MAAM;AACb,6BAAe,KAAK,qBAAqB,KAAK,eAAe,KAAK,UAAU,QAAQ,QAAQ,CAAC;AAC7F,iCAAmB,KAAK,cAAc,KAAK,eAAe,YAAY;AAEtE,oBAAM;AAAA,gBACJ,MAAM;AAAA,gBACN,OAAO;AAAA,kBACL;AAAA,kBACA;AAAA,kBACA,aAAa,eAAe;AAAA,gBAC9B;AAAA,gBACA,MAAM;AAAA,kBACJ,YAAY;AAAA,kBACZ,gBAAgB;AAAA,kBAChB,WAAW;AAAA,kBACX,UAAU;AAAA,gBACZ;AAAA,cACF;AAAA,YACF;AAAA,UACF,SAAS,GAAG;AACV,iBAAK,OAAO,KAAK,uCAAuC,EAAE,MAAM,OAAO,EAAE,CAAC;AAAA,UAC5E;AAAA,QACF;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,mBAAa,OAAO;AAEpB,UAAI,iBAAiB,aAAa,MAAM,QAAQ,SAAS,OAAO,GAAG;AACjE,cAAM,IAAI,sCAAyB,UAAU;AAAA,UAC3C,SAAS,8DAA8D,KAAK;AAAA,QAC9E,CAAC;AAAA,MACH;AAEA,YAAM,KAAK,eAAe,KAAK;AAAA,IACjC,UAAE;AACA,mBAAa,OAAO;AAAA,IACtB;AAAA,EACF;AAAA,EAEA,MAAM,aAAkC;AAEtC,UAAM,KAAK,qBAAqB;AAGhC,WAAO,KAAK,aAAa,gBAAgB;AAAA,MAAO,WAC9C,KAAK,gBAAgB,IAAI,KAAK,iBAAiB,KAAK,CAAC;AAAA,IACvD;AAAA,EACF;AAAA,EAEA,MAAM,aAAa,OAAqC;AACtD,UAAM,cAAc,KAAK,iBAAiB,KAAK;AAE/C,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,aAAa;AAAA,QACvD,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,gBAAgB;AAAA,QAClB;AAAA,QACA,MAAM,KAAK,UAAU,EAAE,MAAM,YAAY,CAAC;AAAA,MAC5C,CAAC;AAED,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI,MAAM,iBAAiB;AAAA,MACnC;AAEA,YAAM,OAAO,MAAM,SAAS,KAAK;AAEjC,aAAO;AAAA,QACL;AAAA,QACA,MAAM,KAAK,QAAQ;AAAA,QACnB,aAAa,KAAK,eAAe,KAAK,oBAAoB,KAAK;AAAA,QAC/D,eAAe,KAAK,aAAa,iBAAiB,KAAK,KAAK;AAAA,QAC5D,iBAAiB,KAAK,aAAa,gBAAgB,KAAK,KAAK;AAAA,QAC7D,mBAAmB,CAAC,QAAQ,YAAY;AAAA,QACxC,SAAS,KAAK,aAAa,QAAS,KAAK;AAAA,QACzC,UAAU;AAAA,UACR,eAAe,KAAK,SAAS;AAAA,UAC7B,cAAc,KAAK,SAAS;AAAA,UAC5B,QAAQ,KAAK,SAAS;AAAA,QACxB;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AAEd,aAAO;AAAA,QACL;AAAA,QACA,MAAM;AAAA,QACN,aAAa,KAAK,oBAAoB,KAAK;AAAA,QAC3C,eAAe,KAAK,aAAa,iBAAiB,KAAK,KAAK;AAAA,QAC5D,iBAAiB,KAAK,aAAa,gBAAgB,KAAK,KAAK;AAAA,QAC7D,mBAAmB,CAAC,QAAQ,YAAY;AAAA,QACxC,SAAS,KAAK,aAAa,QAAS,KAAK;AAAA,MAC3C;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAgB,gBAA4C;AAC1D,QAAI;AACF,YAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,WAAW;AAEvD,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI,MAAM,wBAAwB,SAAS,MAAM,EAAE;AAAA,MAC3D;AAEA,aAAO;AAAA,QACL,SAAS;AAAA,QACT,WAAW,oBAAI,KAAK;AAAA,QACpB,SAAS;AAAA,UACP,iBAAiB,KAAK,gBAAgB;AAAA,QACxC;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,aAAO;AAAA,QACL,SAAS;AAAA,QACT,OAAO,iBAAiB,QAAQ,MAAM,UAAU;AAAA,QAChD,WAAW,oBAAI,KAAK;AAAA,MACtB;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,iBAAiB,OAAyB;AAChD,UAAM,WAAmC;AAAA,MACvC,cAAc;AAAA,MACd,eAAe;AAAA,MACf,eAAe;AAAA,MACf,cAAc;AAAA,MACd,gBAAgB;AAAA,MAChB,gBAAgB,KAAK,OAAO,iBAAiB,eAAe;AAAA,IAC9D;AACA,WAAO,SAAS,KAAK,KAAK;AAAA,EAC5B;AAAA,EAEQ,oBAAoB,OAAyB;AACnD,UAAM,eAAuC;AAAA,MAC3C,cAAc;AAAA,MACd,eAAe;AAAA,MACf,eAAe;AAAA,MACf,cAAc;AAAA,MACd,gBAAgB;AAAA,MAChB,gBAAgB;AAAA,IAClB;AACA,WAAO,aAAa,KAAK,KAAK;AAAA,EAChC;AAAA,EAEA,MAAc,oBAAoB,UAAmC;AACnE,UAAM,YAAY,MAAM,SAAS,KAAK;AACtC,QAAI;AAEJ,QAAI;AACF,kBAAY,KAAK,MAAM,SAAS;AAAA,IAClC,QAAQ;AACN,kBAAY,EAAE,OAAO,UAAU;AAAA,IACjC;AAEA,UAAM,UAAU,UAAU,SAAS;AAEnC,UAAM,IAAI;AAAA,MACR;AAAA,MACA,UAAU,SAAS,MAAM;AAAA,MACzB;AAAA,MACA,SAAS;AAAA,MACT,SAAS,UAAU;AAAA,MACnB;AAAA,IACF;AAAA,EACF;AACF;",
  "names": []
}
